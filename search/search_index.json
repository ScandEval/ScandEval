{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"A Robust Multilingual Evaluation Framework for Language Models <p>EuroEval is a language model benchmarking framework that supports evaluating all types of language models out there: encoders, decoders, encoder-decoders, base models, and instruction tuned models. EuroEval has been battle-tested for more than three years and are the standard evaluation benchmark for many companies, universities and organisations around Europe.</p> <p>Check out the leaderboards to see how different language models perform on a wide range of tasks in various European languages. The leaderboards are updated regularly with new models and new results. All benchmark results have been computed using the associated EuroEval Python package, which you can use to replicate all the results. It supports all models on the Hugging Face Hub, as well as models accessible through 100+ different APIs, including models you are hosting yourself via, e.g., Ollama or LM Studio.</p> <p>The idea of EuroEval grew out of the development of Danish language model R\u00f8B\u00c6RTa in 2021, when we realised that there was no standard way to evaluate Danish language models. It started as a hobby project including Danish, Swedish and Norwegian, but has since grown to include 8+ European languages.</p> <p>EuroEval is maintained by Dan Saattrup Nielsen from the Alexandra Institute and Kenneth Enevoldsen from Aarhus University, and is funded by the EU project TrustLLM.</p> <p>The image used in the logo has been created by the amazing Scandinavia and the World team. Go check them out!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-do-you-determine-if-a-model-is-commercial-or-not","title":"How do you determine if a model is \"Commercial\" or not?","text":"<p>We generally determine this based on whether a model's license allows commercial use of the model. However if we are aware that a model is trained on data, that does not allow for commercial use, we will specify it as non-commercial model, despite the stated license. If you find an issue with any of models feel free to open an issue.</p>"},{"location":"faq/#not-finding-the-answer-that-you-are-looking-for","title":"Not finding the answer that you are looking for?","text":"<p>If don't find the answer that you are looking for feel free to ask your question in the forum.</p>"},{"location":"methodology/","title":"Evaluation Methodology","text":"<p>The evaluation methodology is different depending on the architecture of the model. For encoder models, we use a finetuning approach, where we finetune the model on the training data of the task, and evaluate it on the test data. For decoder models, we use either a few-shot or zero-shot approach, where we evaluate the model on the test data without any finetuning, but where the few-shot examples come from the training data of the task. It has been shown that the few-shot approach corresponds to finetuning in the sense of being equivalent to gradient updates on the training data, making the two evaluation methodologies comparable.</p>"},{"location":"methodology/#robust-evaluation","title":"Robust Evaluation","text":"<p>For each model and dataset, we evaluate the model as described above 10 times, each time on a bootstrapped (i.e., sampling with replacement) version of the training and test set. The evaluation score is then the mean of these scores, along with a 95% confidence interval, computed as the mean \u00b1 1.96 x standard error of the mean, where the standard error of the mean is the sample standard deviation divided by the square root of the number of samples.</p> <p>The bootstrap theorem means that this mean and associated confidence interval will be asymptotically correct, giving us a more reliable estimate of the true performance of the model, rather than just the performance on a single test set, which can be noisy.</p>"},{"location":"methodology/#formulating-nlu-tasks-as-generative-tasks","title":"Formulating NLU Tasks as Generative Tasks","text":"<p>In this section we describe how we rephrase the NLU tasks as text-to-text tasks, which makes it possible to evaluate generative models on the tasks. We set up the prompts differently depending on whether the model is instruction tuned or not, as the instruction tuned models require a different prompt structure to ensure that they generate the correct output.</p> <p>For the base (i.e., non-instruction tuned) models, we use the following prompt structure:</p> <pre><code>[prefix prompt]\n\n{% for each few-shot example %}\n  [document prefix]: [few-shot example document]\n\n  [label prefix]: [few-shot example label]\n{% end for %}\n\n[document prefix]: [new document]\n\n[label prefix]:\n</code></pre> <p>For the instruction tuned models, we use the following prompt structure:</p> <pre><code>{% for each few-shot example %}\n  USER: [instruction with few-shot example]\n  ASSISTANT: [label]\n{% end for %}\nUSER: [instruction with new example]\nASSISTANT:\n</code></pre> <p>Here we would use the model's chat template to set up the <code>USER</code> and <code>ASSISTANT</code> parts of the prompt. See all the specific prompts used for each dataset in the dataset configs module.</p> <p>For the sentiment classification task, we simply have the models generate translations of the three labels (positive, negative and neutral). For the linguistic acceptability task, also a text classification task, we use the translations of \"yes\" and \"no\" as the two labels, corresponding to whether the document is grammatically correct or not. For the extractive question answering task, we have the model output the answer directly. For this task we found that changing the label prefix from \"Answer\" to \"Answer in max 3 words\" resulted in a drastic improvement, due to many of the answers of instruction tuned models starting with unnecessary text akin to \"The answer is\". Lastly, for the named entity recognition task, we require the output to be a JSON dictionary, with keys being the translated named entity tags, and values being lists of named entities of that category. To ensure that we are not biasing the evaluation toward models knowing the JSON format, we employ structured generation using the outlines package, which modifies the logits outputted by the model to ensure that the output is always a valid JSON dictionary in the aforementioned format.</p>"},{"location":"methodology/#score-aggregation","title":"Score Aggregation","text":"<p>From the raw scores of the 10 evaluations per dataset, we need to aggregate the model scores into a single score. We want an aggregation method that satisfies the following criteria:</p> <ul> <li>Task Fairness: Each task should be weighted equally.</li> <li>Comparison: If we evaluate models in multiple languages, then it should be   possible to meaningfully compare the language scores of these models with each other.</li> <li>Robustness: If two models do not have a significantly different score on a   dataset, then the aggregated score should reflect this.</li> <li>Magnitude Preservation: The magnitude of the difference between the dataset score   of two models should be reflected in the aggregated score.</li> <li>Minimal Change: Adding a new model should minimally affect the aggregated scores   of the other models.</li> </ul> <p>Before we introduce our chosen aggregation method, we will briefly discuss some common aggregation methods and how they do not satisfy the criteria.</p> <p>The mean score is the most common aggregation method, which would simply be the mean of the 10 scores for each dataset, and then the mean of the dataset scores for each task. This method does not satisfy the Task Fairness criterion, as it does not take into account that metrics have different ranges and variances. The Comparison criterion is also not satisfied, as datasets vary from language to language, with some datasets being more difficult than others. It does, however, satisfy the Robustness, Magnitude Preservation and Minimal Change criteria.</p> <p>The mean rank is another common aggregation method, where we compute the rank of each model on each dataset, and then take the mean of the ranks. This method satisfies the Task Fairness criterion, as it re-casts the scores into a common comparable framework, which therefore weights each task equally. For the same reason, it also satisfies the Comparison criterion (it is important here that we evaluate all the models on all the languages for this to be satisfied). It does not satisfy the Robustness and Magnitude Preservation criteria, by definition of rank. It partially satisfies the Minimal Change criterion, since it only affects the scores of the models which are worse than the new model.</p> <p>We thus see that the mean score and mean rank methods satisfy a disjoint set of the criteria, but that they together satisfy all the criteria. Based on this observation, we introduce the mean rank score method, defined as follows. For each dataset, we start by sorting the models by their mean score on the dataset. As with a rank, we assign the best model with rank score 1. For the next best model, we conduct a one-tailed Welch's t-test to see if the next best model is significantly worse than the first model (p &lt; 0.05). If so, we compute the absolute difference between the mean score of the two models, and divide that by the standard deviation of all the mean scores of the models on the dataset.</p> <p>We then add this to the rank score of the first model. We continue this process for all the models to get the rank scores for the dataset, and to compute the overall score for the model, we take the mean of the rank scores for the datasets. We note that the mean rank score has an intuitive interpretation: it is the average number of standard deviations from the best scoring model (+1).</p> <p>This metric satisfies Task Fairness since we normalise all the scores by dividing by the standard deviation of the dataset scores. The Robustness criterion is satisfied due to our use of a one-tailed Welch's t-test. The Magnitude Preservation criterion is also satisfied, as the magnitude of the difference between the dataset score of two models is reflected in the rank score. It also satisfies Comparison, as we compare the models on a common scale (same argument as the mean rank method). Finally, the Minimal Change criterion is partially satisfied, as adding new models only minimally changes the score of existing models. Concretely, adding new scores will affect the standard deviation normalising factor (this effect tends to zero as the number of models grows, however), and if the model beats all the other models then all the scores will be affected, due to the relative nature of the metric.</p>"},{"location":"methodology/#papers","title":"Papers","text":"<p>Check out more in-depth descriptions of the methodology in the associated research papers:</p> <ul> <li>Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on   Multilingual NLU Tasks</li> <li>ScandEval: A Benchmark for Scandinavian Natural Language   Processing</li> </ul>"},{"location":"python-package/","title":"The <code>euroeval</code> Python Package","text":"<p>The <code>euroeval</code> Python package is the Python package used to evaluate language models in EuroEval. This page will give you a brief overview of the package and how to use it. You can also check out the full API reference for more details.</p>"},{"location":"python-package/#installation","title":"Installation","text":"<p>To install the package simply write the following command in your favorite terminal:</p> <pre><code>$ pip install euroeval[all]\n</code></pre> <p>This will install the EuroEval package with all extras. You can also install the minimal version by leaving out the <code>[all]</code>, in which case the package will let you know when an evaluation requires a certain extra dependency, and how you install it.</p>"},{"location":"python-package/#quickstart","title":"Quickstart","text":""},{"location":"python-package/#benchmarking-from-the-command-line","title":"Benchmarking from the Command Line","text":"<p>The easiest way to benchmark pretrained models is via the command line interface. After having installed the package, you can benchmark your favorite model like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt;\n</code></pre> <p>Here <code>model</code> is the HuggingFace model ID, which can be found on the HuggingFace Hub. By default this will benchmark the model on all the tasks available. If you want to benchmark on a particular task, then use the <code>--task</code> argument:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre> <p>We can also narrow down which languages we would like to benchmark on. This can be done by setting the <code>--language</code> argument. Here we thus benchmark the model on the Danish sentiment classification task:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification --language da\n</code></pre> <p>Multiple models, datasets and/or languages can be specified by just attaching multiple arguments. Here is an example with two models:</p> <pre><code>$ euroeval --model &lt;model-id1&gt; --model &lt;model-id2&gt;\n</code></pre> <p>The specific model version/revision to use can also be added after the suffix '@':</p> <pre><code>$ euroeval --model &lt;model-id&gt;@&lt;commit&gt;\n</code></pre> <p>This can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.</p> <p>See all the arguments and options available for the <code>euroeval</code> command by typing</p> <pre><code>$ euroeval --help\n</code></pre>"},{"location":"python-package/#benchmarking-from-a-script","title":"Benchmarking from a Script","text":"<p>In a script, the syntax is similar to the command line interface. You simply initialise an object of the <code>Benchmarker</code> class, and call this benchmark object with your favorite model:</p> <pre><code>&gt;&gt;&gt; from euroeval import Benchmarker\n&gt;&gt;&gt; benchmark = Benchmarker()\n&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\")\n</code></pre> <p>To benchmark on a specific task and/or language, you simply specify the <code>task</code> or <code>language</code> arguments, shown here with same example as above:</p> <pre><code>&gt;&gt;&gt; benchmark(model=\"&lt;model&gt;\", task=\"sentiment-classification\", language=\"da\")\n</code></pre> <p>If you want to benchmark a subset of all the models on the Hugging Face Hub, you can simply leave out the <code>model</code> argument. In this example, we're benchmarking all Danish models on the Danish sentiment classification task:</p> <pre><code>&gt;&gt;&gt; benchmark(task=\"sentiment-classification\", language=\"da\")\n</code></pre>"},{"location":"python-package/#benchmarking-from-docker","title":"Benchmarking from Docker","text":"<p>A Dockerfile is provided in the repo, which can be downloaded and run, without needing to clone the repo and installing from source. This can be fetched programmatically by running the following:</p> <pre><code>$ wget https://raw.githubusercontent.com/EuroEval/EuroEval/main/Dockerfile.cuda\n</code></pre> <p>Next, to be able to build the Docker image, first ensure that the NVIDIA Container Toolkit is installed and configured. Ensure that the the CUDA version stated at the top of the Dockerfile matches the CUDA version installed (which you can check using <code>nvidia-smi</code>). After that, we build the image as follows:</p> <pre><code>$ docker build --pull -t euroeval -f Dockerfile.cuda .\n</code></pre> <p>With the Docker image built, we can now evaluate any model as follows:</p> <pre><code>$ docker run -e args=\"&lt;euroeval-arguments&gt;\" --gpus 1 --name euroeval --rm euroeval\n</code></pre> <p>Here <code>&lt;euroeval-arguments&gt;</code> consists of the arguments added to the <code>euroeval</code> CLI argument. This could for instance be <code>--model &lt;model-id&gt; --task sentiment-classification</code>.</p>"},{"location":"datasets/","title":"Datasets","text":"<p>\ud83d\udc48 Choose a language on the left to see all the evaluation datasets available for that language.</p>"},{"location":"datasets/danish/","title":"\ud83c\udde9\ud83c\uddf0 Danish","text":"<p>This is an overview of all the datasets used in the Danish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/danish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/danish/#angry-tweeets","title":"Angry Tweeets","text":"<p>This dataset was published in this paper and was a crowd-sourcing effort to annotate sentiment of Danish tweets.</p> <p>The original full dataset consists of 3,458 samples, and we are using a split of 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). All the samples in the original test set are included in our test set, but our test set is furthermore using a subset of the original training set as test samples as well. The original dataset did not have a validation split, so we have created one by sampling from the training set.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jeg tror, det der var kampen. Goff virker lost\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@USER @USER Vi bruger ogs\u00e5 snildt 1-2 timer (nogle gange flere timer end det) p\u00e5 at putte den yngste. Det er oftest Tommi, som g\u00f8r det, for jeg g\u00e5r helt amok i processen. S\u00e5 sm\u00f8rer jeg madpakker og rydder op i stedet.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Er du nysgerrig p\u00e5, hvordan du diskvalificerer dig selv fra at blive taget seri\u00f8st i den offentlige debat? Naser har svaret. #dkpol #dkmedier [LINK]\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nKlassificer sentimentet i tweetet. Svar kun med 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset angry-tweeets\n</code></pre>"},{"location":"datasets/danish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/danish/#dansk","title":"DANSK","text":"<p>This dataset was published in this paper and is a manually annotated subset of Danish Gigaword with the 18 different named entities, following the OntoNotes 5.0 scheme. It was annotated by 10 different annotators.</p> <p>The original full dataset consists of 15,062 samples, and we are using a split of 1,024 / 256 / 1,024 samples for training, validation and testing, respectively (so 2,304 samples used in total). All samples in the validation and test sets of our version also belong to the original validation and test set, respectively.</p> <p>We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003 labelling scheme, which is more common in the NER literature. The mapping is as follows:</p> <ul> <li><code>PERSON</code> \u27a1\ufe0f <code>PER</code></li> <li><code>LOCATION</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>FACILITY</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>ORGANIZATION</code> \u27a1\ufe0f <code>PER</code></li> <li><code>EVENT</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>LANGUAGE</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>PRODUCT</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>WORK OF ART</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>NORP</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>CARDINAL</code> \u27a1\ufe0f <code>O</code></li> <li><code>DATE</code> \u27a1\ufe0f <code>O</code></li> <li><code>LAW</code> \u27a1\ufe0f <code>O</code></li> <li><code>MONEY</code> \u27a1\ufe0f <code>O</code></li> <li><code>ORDINAL</code> \u27a1\ufe0f <code>O</code></li> <li><code>PERCENT</code> \u27a1\ufe0f <code>O</code></li> <li><code>QUANTITY</code> \u27a1\ufe0f <code>O</code></li> <li><code>TIME</code> \u27a1\ufe0f <code>O</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['I', 'dette', 'efter\u00e5r', 'har', 'Gr\u00f8nland', 'taget', 'en', 'stor', 'beslutning', 'ved', 'folkeafstemningen', 'den', '25.', 'november', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['\u00c5h', ',', 'Petra', ',', 'vis', 'mig', 'din', 'krop', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Fravalget', 'af', 'revision', 'registreres', 'automatisk', 'ved', 'anmeldelse', 'af', 'stiftelse', 'af', 'selskabet', 'hos', 'Erhvervs-styrelsen', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, som forekommer i den givne s\u00e6tning.\n</code></pre></li> <li>Base prompt template:   <pre><code>S\u00e6tning: {text}\nNavngivne enheder: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>S\u00e6tning: {text}\n\nIdentific\u00e9r de navngivne enheder i s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', 'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset dansk\n</code></pre>"},{"location":"datasets/danish/#unofficial-dane","title":"Unofficial: DaNE","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Danish Universal Dependencies treebank. The NER labels follow the CoNLL-2003 labelling scheme.</p> <p>The original full dataset consists of 4,383 / 564 / 565 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'var', 'det', '\u00e5r', ',', 'hans', 'f\u00f8rste', 'LP', ',', '\"', 'With', 'A', 'Little', 'Help', 'From', 'My', 'Friends', '\"', ',', 'udkom', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Eddie', 'Carbone', ',', 'italiensk-amerikansk', 'havnearbejder', 'i', 'New', 'York', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'I-PER', 'O', 'B-MISC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['\"', 'Jeg', 'er', 'mig', '!', '\"', 'insisterer', 'han', 'under', 'det', 'flere', 'hundrede', '\u00e5r', 'gamle', 'egetr\u00e6', ',', 'liggende', ',', 'som', 'den', 'popflab', 'han', 'er', ',', 'p\u00e5', 'ryggen', 'i', 'sine', 'orange', 'jeans', ',', 't-shirt', '-', 'som', 'naturligvis', 'stiller', 'et', 'solbrunt', 'beh\u00e5ret', 'bryst', 'til', 'skue', '-', 'et', 'par', '68er', '\"', 'make', 'love', 'not', 'war', '\"', 'solbriller', 'han', 'netop', 'har', 'k\u00f8bt', 'i', 'Paris', ',', 'og', 'en', 'Kings', 'i', 'k\u00e6ften', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p>"},{"location":"datasets/danish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/danish/#scala-da","title":"ScaLA-da","text":"<p>This dataset was published in this paper and was automatically created from the Danish Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 5,512 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Samme dame dukkede netop nu op sammen med Odd-Catla's erkl\u00e6rede yndling, v\u00e6bneren Aikin af Cantir.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Gebyrets st\u00f8rrelse afh\u00e6nger nemlig af helt, i hvilken kategori den p\u00e5g\u00e6ldende \\\"levnedsmiddelvirksomhed\\\" placeres.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Den statsansatte dyrl\u00e6ge Kronf\u00e5gels p\u00e5 slagteri i Kristiansstad, Karl Erik Bj\u00f8rkman, understreger, bel\u00e6gningen hos producenten betyder meget for dyrenes trivsel:\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>S\u00e6tning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>S\u00e6tning: {text}\n\nBestem om s\u00e6tningen er grammatisk korrekt eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nej</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-da\n</code></pre>"},{"location":"datasets/danish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/danish/#scandiqa-da","title":"ScandiQA-da","text":"<p>This dataset was published in this paper and was automatically created from the Danish part of the MKQA dataset. The MKQA dataset is based on the English Natural Questions dataset, based on search queries from the Google search engine. The questions and answers were manually translated to Danish (and other languages) as part of MKQA, and the contexts were in ScandiQA-da machine translated using the DeepL translation API. A rule-based approach was used to ensure that the translated contexts still contained the answer to the question, potentially by changing the answers slightly.</p> <p>The original full dataset consists of 6,810 / 500 / 500 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All validation samples in our version also belong to the original validation set, and all original test samples are included in our test set. The remaining 1,548 test samples in our version was sampled from the original training set.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": '\"(Sittin\\' On) The Dock of the Bay\" er en sang, der er skrevet af soul-sangeren Otis Redding og guitaristen Steve Cropper sammen. Den blev indspillet af Redding to gange i 1967, herunder en gang f\u00e5 dage f\u00f8r hans d\u00f8d i et flystyrt. Sangen blev udgivet p\u00e5 Stax Records\\' Volt-label i 1968 og blev den f\u00f8rste posthume single, der l\u00e5 \u00f8verst p\u00e5 hitlisterne i USA. Den n\u00e5ede op som nummer 3 p\u00e5 den britiske single-liste.',\n  \"question\": 'Hvem sang sitting on the dock of the bay?',\n  \"answers\": {\n    \"answer_start\": array([79]),\n    \"text\": array(['Otis Redding'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": \"The Cat in the Hat Knows a Lot About That!\\nKatten i hatten ved meget om det!\\n\\n\\n\\nKatten i hatten pilot\\n\\n\\n\\nGenre\\nB\u00f8rne-tv/undervisning/komedie\\n\\n\\nInstrueret af\\nTony Collingwood\\n\\n\\nStemmer fra\\nMartin Short\\nJacob Ewaniuk\\nAlexa Torrington\\nRob Tinkler\\n\\n\\nKomponist af temamusik\\nDavid Schweitzer\\n\\n\\nKomponist(er)\\nDavid Schweitzer\\n\\n\\nOprindelsesland\\nCanada\\nDet Forenede Kongerige\\nUSA\\n\\n\\nOprindelige sprog\\nEngelsk\\n\\n\\nAntal s\u00e6soner\\n2\\n\\n\\nAntal episoder\\n60 (liste over episoder)\\n\\n\\nProduktion\\n\\n\\nL\u00f8betid\\n30 minutter\\n\\n\\nProduktionsselskab(er)\\nCollingwood O'Hare Productions\\nPortfolio Entertainment\\nRandom House Children's Entertainment\\nTreehouse TV\\n\\n\\nDistribut\u00f8r\\nTreehouse TV\\n\\n\\nUdgivelse\\n\\n\\nOprindelige netv\u00e6rk\\nTreehouse TV (Canada)\\nPBS Kids (USA)\\nCITV og Tiny Pop (UK)\\n\\n\\nBilledformat\\n480i (SDTV)\\n1080i (HDTV)\\n\\n\\nOriginaludgivelse\\n7. august 2010 (2010-08-07) - nu\\n\\n\\nEksterne links\\n\\n\\nWebsted\\npbskids.org/catinthehat/\",\n  \"question\": 'Hvem synger titelmelodien til the cat in the hat?',\n  \"answers\": {\n    \"answer_start\": array([269]),\n    \"text\": array(['David Schweitzer'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Modern Slavery Act 2015\\nLoven om moderne slaveri fra 2015 er en lov fra Det Forenede Kongeriges parlament. Den har til form\u00e5l at bek\u00e6mpe slaveri i Det Forenede Kongerige og konsoliderer tidligere lovovertr\u00e6delser vedr\u00f8rende menneskehandel og slaveri. Loven g\u00e6lder for England og Wales. Lovforslaget blev forelagt underhuset i udkast i oktober 2013 af James Brokenshire, parlamentarisk undersekret\u00e6r for kriminalitet og sikkerhed, i oktober 2013. Lovforslagets sponsorer i indenrigsministeriet var Theresa May og Lord Bates. Det fik kongelig samstemmende udtalelse og blev lov den 26. marts 2015.',\n  \"question\": 'Hvorn\u00e5r tr\u00e5dte den moderne slaveri i kraft?',\n  \"answers\": {\n    \"answer_start\": array([580]),\n    \"text\": array(['26. marts 2015'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rgsm\u00e5l: {question}\nSvar med maks. 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor med maks. 3 ord.\n\nSp\u00f8rgsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scandiqa-da\n</code></pre>"},{"location":"datasets/danish/#knowledge","title":"Knowledge","text":""},{"location":"datasets/danish/#danske-talemader","title":"Danske Talem\u00e5der","text":"<p>This dataset was created by The Danish Language and Literature Society, published here. The dataset features Danish idioms along with their official meaning. For each idiom, three negative samples were created: (a) a random idiom, (b) a concrete made-up idiom, and (c) an abstract made-up idiom. The dataset was created to evaluate the ability of language models to understand Danish idioms.</p> <p>The original full dataset consists of 1,000 samples. We use a 128 / 64 / 808 split for training, validation and testing, respectively (so 1,000 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'tale nogen efter munden'?\\nSvarmuligheder:\\na. v\u00e6re f\u00f8jelig og give nogen ret selvom man ikke n\u00f8dvendigvis er enig\\nb. erkl\u00e6re sig helt enig med en anden person\\nc. sige det pr\u00e6cis samme som en anden; efterabe\\nd. v\u00e6re egoistisk og sn\u00e6versynet; kun t\u00e6nke p\u00e5 sig selv\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'der falder en sten fra \u00e9ns hjerte'?\\nSvarmuligheder:\\na. en bestemt (kriminel, efters\u00f8gt) person er forsvundet\\nb. man bliver fri for en sorg eller bekymring; man bliver lettet\\nc. man mister \u00e9n man har k\u00e6r\\nd. en sten forlader et hjerte man er i besiddelse af\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad betyder udtrykket 'have spidse albuer'?\\nSvarmuligheder:\\na. person der har det meget d\u00e5rligt fysisk og psykisk\\nb. have ophobet vrede over l\u00e6ngere tid\\nc. h\u00e6vde sig p\u00e5 andres bekostning\\nd. have knogler der tr\u00e6der tydeligt frem p\u00e5 ens albuer\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Hvad er betydningen af f\u00f8lgende talem\u00e5de: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset danske-talemaader\n</code></pre>"},{"location":"datasets/danish/#danish-citizen-tests","title":"Danish Citizen Tests","text":"<p>This dataset was created by scraping the Danish citizenship tests (indf\u00f8dsretspr\u00f8ven) and permanent residency tests (medborgerskabspr\u00f8ven) from 2016 to 2023. These are available on the official website of the Danish Ministry of International Recruitment and Integration.</p> <p>The original full dataset consists of 720 samples. We use an 80 / 128 / 512 split for training, validation and testing, respectively (so 720 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvilke lande er med i rigsf\u00e6llesskab et?\\nSvarmuligheder:\\na. Danmark, Gr\u00f8nland og F\u00e6r\u00f8erne\\nb. Danmark, Island og Norge\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvor mange medlemmer har Folketinget?\\nSvarmuligheder:\\na. 87\\nb. 179\\nc. 265\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvem kan blive biskop i den danske folkekirke?\\nSvarmuligheder:\\na. Kun m\u00e6nd\\nb. Kun kvinder\\nc. B\u00e5de m \u00e6nd og kvinder\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset danish-citizen-tests\n</code></pre>"},{"location":"datasets/danish/#unofficial-mmlu-da","title":"Unofficial: MMLU-da","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Danish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvilket af f\u00f8lgende coronavirusser har for\u00e5rsaget tusindvis af d\u00f8dsfald over hele verden som en 'opst\u00e5et' virus?\\nSvarmuligheder:\\na. MERS\\nb. SARS\\nc. OC43\\nd. HKU1\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilken orbitale v\u00e6g er mest sandsynligt at kollapse i en 'blow out' fraktur?\\nSvarmuligheder:\\na. Taget\\nb. Gulvet\\nc. Den laterale v\u00e6g\\nd. Den mediale v\u00e6g\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvad er navnet p\u00e5 den st\u00f8rste struktur i Teotihuac\u00e1n, og hvor mange platforme og pyramider blev bygget der?\\nSvarmuligheder:\\na. M\u00e5nepyramiden; 250\\nb. Templet for den fjerkr\u00e6kl\u00e6dte slange; 400\\nc. Solpyramiden; 600\\nd. Inskriptionstemplen; 700\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul>"},{"location":"datasets/danish/#unofficial-arc-da","title":"Unofficial: ARC-da","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Danish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Et farmaceutisk firma har offentliggjort resultaterne af et begr\u00e6nset eksperiment, der unders\u00f8ger den beskyttende virkning af en kemisk forbindelse mod h\u00f8je doser af UV-str\u00e5ler p\u00e5 hudceller. Senere blev det opdaget, at resultaterne ikke var reproducerbare. Hvilken handling kunne forskere fra firmaet have foretaget for at undg\u00e5 at offentligg\u00f8re fejlagtige resultater?\\nSvarmuligheder:\\na. Udf\u00f8r flere fors\u00f8g.\\nb. Brug kun lave niveauer af str\u00e5ling.\\nc. Brug forskellige b\u00f8lgel\u00e6ngder af str\u00e5ling.\\nd. Unders\u00f8g resultaterne af lignende eksperimenter, f\u00f8r man dannede en hypotese.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En ingeni\u00f8r skal beregne den potentielle energi af en rutschebanekabine \u00f8verst p\u00e5 en skr\u00e5ning. Hvilken information ville bedst hj\u00e6lpe ingeni\u00f8ren med at bestemme den potentielle energi af kabine?\\nSvarmuligheder:\\na. den afstand, som rutschebanekabinen skal rejse\\nb. massen af rutschebanekabinen ved fuld kapacitet\\nc. den gennemsnitlige v\u00e6gt af en tom rutschebanekabine\\nd. retningen, som rutschebanekabinen bev\u00e6ger sig i\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En studerende h\u00e6ldte vand i en plastbakke. Studerende satte derefter bakken i fryseren. Hvilken egenskab ved vand \u00e6ndrede sig, da vandet fryser?\\nSvarmuligheder:\\na. Vandet blev til en gas.\\nb. Massen af vandet steg.\\nc. Vandet tog en bestemt form.\\nd. Smagen af vandet \u00e6ndrede sig ikke.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_c}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul>"},{"location":"datasets/danish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/danish/#hellaswag-da","title":"HellaSwag-da","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Disse mennesker tr\u00e6der pedalerne med kun det ene ben og st\u00e5r midt p\u00e5 cyklen med det andet ben, der holder deres h\u00e6nder oppe. n\u00e6ste g\u00f8r de\\nSvarmuligheder:\\na. en anden \u00f8velse, hvor de s\u00e6tter det ene ben p\u00e5 pedalen, mens de har det andet ben ude og hopper op og ned.\\nb. tager hinandens h\u00e6nder og udf\u00f8rer en eller anden dansebev\u00e6gelse p\u00e5 b\u00f8rsterne, som de bruger til at snurre rundt med deres kroppe og hoppe med h\u00e6nderne oppe.\\nc. drejer med deres forstenede h\u00e6nder, laver en U-vending og starter derefter deres handlinger igen og igen.\\nd. skifter til at st\u00e5 ved hj\u00e6lp af to arme for at balancere sig selv.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] S\u00e5dan dr\u00e6ber du frugtfluer [title] Brug r\u00e5dden frugt. [step] Dit problem med frugtfluer begyndte sandsynligvis f\u00f8rst, da du opdagede, at du havde efterladt nogle frugter, der til sidst blev r\u00e5dne. Brug den metode, der samlede fluene f\u00f8rste gang til at fange dem igen, men denne gang f\u00f8r dem til en mere morbide slutning.\\nSvarmuligheder:\\na. Dr\u00e6b fluene ved at tr\u00e6kke dem fra deres rede eller ved at bruge tunge k\u00e6der med t\u00e6nger til at fange dem og placere dem i en spand eller stuen. Du kan ogs\u00e5 bruge dyreaff\u00f8ring s\u00e5som fiske- og ande-urin.\\nb. Placer et stykke r\u00e5dden frugt i en sk\u00e5l og str\u00e6k klart plastik over toppen. Sk\u00e6r flere sm\u00e5 huller i plastikken med en tandstik og lad det st\u00e5 t\u00e6t p\u00e5 stedet med fluene.\\nc. Efter at have fors\u00f8gt at fange dobbelt s\u00e5 mange fluer, som du kan, skal du fjerne de ubehagelige frugtstykker fra pakken og bage dem i 2-3 minutter. Fluene vil flyde \u00f8verst p\u00e5 den s\u00f8de marmelade, n\u00e5r du fjerner frugten fra marmeladen.\\nd. [substeps] Tjek d\u00e5ser for knotten, melbiller og fluer. K\u00f8b blomster fra havecentret, hvis du ikke har al produktion i n\u00e6rheden.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En mand st\u00e5r indend\u00f8rs p\u00e5 en platform foran tre tilskuere og l\u00f8fter en tung v\u00e6gtstang. En mand n\u00e6rmer sig en v\u00e6gtstang p\u00e5 gulvet og st\u00e5r foran den og forbereder sig p\u00e5 at l\u00f8fte den. manden\\nSvarmuligheder:\\na. l\u00f8fter v\u00e6gtstangen, der h\u00e6nger i luften p\u00e5 platformen, og vender sig mod tilskuerne.\\nb. l\u00f8fter v\u00e6gtstangen og viser, hvordan han udf\u00f8rer det, idet han pauser p\u00e5 hver stang for at m\u00e5le v\u00e6gten.\\nc. b\u00f8jer sig derefter i kn\u00e6ene og l\u00e6gger h\u00e6nderne p\u00e5 v\u00e6gtens stangdel.\\nd. l\u00f8fter derefter klokken p\u00e5 sine skuldre, l\u00e6ner sig tilbage, s\u00e6tter armene bag hovedet og l\u00f8fter den let.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_c}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rgsm\u00e5l: {text}\nSvarmuligheder:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at svare med 'a', 'b', 'c' eller 'd', og intet andet.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-da\n</code></pre>"},{"location":"datasets/danish/#summarization","title":"Summarization","text":""},{"location":"datasets/danish/#nordjylland-news","title":"Nordjylland News","text":"<p>This dataset is based on news articles from the Danish news site TV2 Nord, where the summaries are taken as the introductory paragraphs of the articles.</p> <p>The original full dataset consists of 75,200 samples. We use an 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jacob Emil Andersen viste s\u00f8ndag rundt p\u00e5 Halvorsminde Efterskole ved Hj\u00f8rring. Skolen har ligget p\u00e5 samme sted siden 1903. Han er selv elev, da en IT-linje p\u00e5 skolen fangede hans interesse. -\u00a0Det betyder meget for mig, jeg ville ikke have v\u00e6ret lige s\u00e5 interesseret\u00a0i den her skole, hvis der ikke havde v\u00e6ret IT, fort\u00e6ller Jacob Emil Andersen, der oprindeligt stammer fra Aalborg, til TV2 Nord. En af dem, han viser rundt til Efterskolernes dag, er Isabella Kristensen, der g\u00e5r i skole i Hune. Hun er p\u00e5 jagt efter noget helt specielt. -\u00a0Helt sikkert dans, springgymnastik og fitness med noget puls, forklarer Isabella Kristensen til TV2 Nord. Netop efterskolernes specialisering er en af grundene til, at rekordmange v\u00e6lger at bruge et \u00e5r v\u00e6k fra familien i 8.-, 9.- eller 10.-klasse. De s\u00e6rlige linjefag har man flere af p\u00e5 Halvorsminde Efterskole. Jern og metal, arbejde med tr\u00e6 og vinterbadning er blot nogle af de aktiviteter, eleverne kan st\u00f8de ind i p\u00e5 de forskellige linjefag, som skolen tilbyder. Men efterskolerne skal ogs\u00e5 huske at have fokus p\u00e5 den\u00a0faglighe kvalitet,\u00a0lyder det fra forstanderen. -\u00a0Vi skal v\u00e6re skarpe p\u00e5 nogle nicheprodukter og nogle linjer med noget god kvalitet. S\u00e5 skal vi ogs\u00e5 lave god skole, fort\u00e6ller\u00a0forstander p\u00e5 Halvorsminde Efterskole, Jens Beermann, til TV2 Nord. Han bliver bakket op af sin kollega fra H\u00f8rby Efterskole ved S\u00e6by omkring 30 kilometer fra Halvorsminde. - N\u00e5r man laver sit valgfagsudbud, skal det ikke v\u00e6re tilf\u00e6ldigt. Man skal ikke t\u00e6nke, at \u2019det er smart! Det m\u00e5 tr\u00e6kke elever, det her!\u2019 Der skal v\u00e6re en velovervejet refleksion i forhold til, om det passer ind i det, vi gerne vil som skole,, siger forstander p\u00e5 H\u00f8rby Efterskole, Mogens Vesterg\u00e5rd, til TV2 Nord. Alene i Nordjylland gik mere end 2.000 elever p\u00e5 efterskole i skole\u00e5ret 2018-2019. B\u00e5de Halvorsminde Efterskole og H\u00f8rby Skole har plads til 130 elever. Og noget tyder p\u00e5, at der i hvert fald er sikret en ny\u00a0elev til n\u00e6ste skole\u00e5r efter dagens \u00e5bent hus. -\u00a0Jeg synes at det ser sp\u00e6ndende ud, og jeg har endnu mere lyst til at g\u00e5 her nu, siger Isabella Kristensen.\",\n  \"target_text\": \"S\u00f8ndag inviterede efterskoler landet over potentielle nye elever inden for. Efterskolerne specialiserer sig for at tiltr\u00e6kke elever, men den gode faglighed m\u00e5 ikke blive glemt, lyder det fra nordjyske forstandere.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Efter en nat med spejl glatte veje i Nordjylland melder Nordjyllands Politi om en helt problemfri morgen.\u00a0Selvom politikredse i TV2 Nords sendeomr\u00e5de melder om en rolig nat uden st\u00f8rre uheld, s\u00e5\u00a0kan de bilister, der skal af sted l\u00f8rdag morgen godt forvente\u00a0lidt l\u00e6ngere rejsetid. Der er nemlig stadig glatte veje, og der er faldet en del sne i Nordjylland.\u00a0Saltvogne og sneplove har allerede v\u00e6ret p\u00e5 vejene, og Politiet opfordre forsat bilisterne til at k\u00f8re forsigtigt ude p\u00e5 de snefyldte veje.\",\n  \"target_text\": \"Nordjyllands Politi melder om en stille morgen trods glatte veje og stort snefald i nat.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det var meget t\u00e6t p\u00e5 at g\u00e5 galt for en 10-\u00e5rig tysk dreng onsdag eftermiddag. Klokken 15:55 modtog alarmcentralen et opkald om en drengen, der var begravet i sand ved Vorup\u00f8r Strand. - Nogle b\u00f8rn legede p\u00e5 stranden, og her har de s\u00e5 gravet et hul ind i klitten. Det er s\u00e5 det, der er kollapset omkring drengen, fort\u00e6ller vagtchef Carsten Henriksen ved Midt- og Vestjyllands Politi. Det vides ikke pr\u00e6cist, hvor meget sand der v\u00e6ltede ned over barnet, men det var nok til, at drengen ikke selv kunne komme fri. De tilstedev\u00e6rende p\u00e5 stranden m\u00e5tte grave ham fri. Han var\u00a0helt begravet i sand i omkring fem minutter. - Der var en tysk l\u00e6ge p\u00e5 stranden, der kunne give f\u00f8rstehj\u00e6lp, indtil ambulancen kunne komme frem, fort\u00e6ller vagtchefen. Drengen kom sig hurtigt og har det godt, men blev alligevel k\u00f8rt til tjek p\u00e5 Aalborg Sygehus.\",\n  \"target_text\": \"B\u00f8rn p\u00e5 Vorup\u00f8r Strand havde gravet et hul ind i klitterne, som kollapsede omkring en 10-\u00e5rig dreng.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhedsartikel: {text}\nResum\u00e9: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhedsartikel: {text}\n\nSkriv et resum\u00e9 af ovenst\u00e5ende artikel.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nordjylland-news\n</code></pre>"},{"location":"datasets/dutch/","title":"\ud83c\uddf3\ud83c\uddf1 Dutch","text":"<p>This is an overview of all the datasets used in the Dutch part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/dutch/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/dutch/#dutch-social","title":"Dutch Social","text":"<p>This dataset consists of Dutch tweets annotated with sentiment labels. It is not sure how the sentiment labels were assigned, this information is pending from the authors.</p> <p>The original full dataset consists of 162,805 / 54,269 / 54,268 samples for training, validation and testing, respectively (so 271,342 samples used in total). We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": 'Novak Djokovic positief getest op coronavirus na eigen tennistoernooi\\n\\nhttps://t.co/U7VOcjANh9',\n  \"label\": 'positive'\n}\n</code></pre> <pre><code>{\n  \"text\": \"via @NYTimes  https://t.co/IjbCWIwYvR\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@backinflow 30 min Corona tijd....\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hieronder staan tweets en hun sentiment, dat 'positief', 'neutraal' of 'negatief' kan zijn.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nClassificeer het sentiment in de tweet. Antwoord met 'positief', 'neutraal' of 'negatief'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positief</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutraal</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negatief</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset dutch-social\n</code></pre>"},{"location":"datasets/dutch/#unofficial-dbrd","title":"Unofficial: DBRD","text":"<p>This dataset was published in this paper and features Dutch book reviews from Hebban.nl, annotated with sentiment labels, written by the users of the website.</p> <p>The original full dataset consists of 20,000 / 2,200 samples for training and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). The training and testing splits are subsets of the original splits, and the validation split is a disjoint subset of the original training split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Het boek geeft uitleg in de basis technieken en heeft handige tips, hoe je de klassieke recepten ook gewoon zelf kan maken, ze zijn geschreven in een soort leermodus, dit alles ondersteunt door stap voor stap foto\u2019s.\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Dit boek is het debuut van de Zuid-Afrikaanse schrijver S J Naud\u00e9 , het heeft diverse prijzen gewonnen waaronder de UJ Debutprys 2012.\\nHet is een verhalenbundel, met verhalen over personages, die metaforisch rondtrekkende vogels genoemd worden. Ze vliegen letterlijk rusteloos over de wereld. De een is een muzikante die drie continenten over reist om haar broers en zussen te ontmoeten, een man volgt zijn minnaar via Londen en Berlijn naar een kasteel , in Milaan is een futuristisch lawaaimachine te zien en een andere vrouw wil er voor zorgen dat er geen hiv meer voorkomt in Afrika. Zo zijn er nog een paar verhalen. Het ene verhaal heeft me meer geraakt dan het andere, het beste verhaal vind ik het verhaal waarin een man voor zijn doodzieke moeder zorgt, samen met een Japanse man.\\nDe thema\u2019s die in dit boek voorkomen zijn liefde, troost, acceptatie en succes. Leven en dood, reizen, gevoel en verstand komen steeds weer aan bod in de verhalen. Iedereen zoekt naar antwoorden die niet gegeven worden.\\nHet is een boek dat je niet even snel leest, het zijn allemaal op zich zelf staande verhalen, hoewel sommige personen in andere verhalen weer naar voren komen. Wat precies het verband daar tussen is, heb ik niet kunnen ontdekken.\\nHet is een boek dat niet echt vrolijk is, veel verhalen zijn somber. Doordat er veel Afrikaanse namen in voorkomen raak je af en toe de draad kwijt.\\nIk ben niet erg gecharmeerd van dit boek en geef het 2 sterren .\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Voor mij het zwakste boek van Coben tot nu toe.\\nHet was alsof ik naar een slechte B-film aan het kijken was. Bordkartonnen personages die me totaal onverschillig lieten. Deus ex machina's die de plot ongeloofwaardig maken.\\nVerloren is als een slecht, onevenwichtig James Bond verhaal. Veel actie zonder context, background en motivatie.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hieronder staan tweets en hun sentiment, dat 'positief', 'neutraal' of 'negatief' kan zijn.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nClassificeer het sentiment in de tweet. Antwoord met 'positief', 'neutraal' of 'negatief'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positief</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negatief</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset dbrd\n</code></pre>"},{"location":"datasets/dutch/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/dutch/#conll-2002-nl","title":"CoNLL-2002-nl","text":"<p>This dataset was published in this paper and consists of named entity recognition annotations of the Belgian newspaper \"De Morgen\" of 2000.</p> <p>The original full dataset consists of 8,324 / 1,916 / 1,518 samples for training, validation and testing, respectively (so 11,758 samples used in total). We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Puitstraat', '6', ',', '8890', 'Moorslede', '.'], dtype=object),\n  \"labels\": array(['B-LOC', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Monami-Van', 'Roost', 'had', 'nochtans', 'verloren', '.'], dtype=object),\n  \"labels\": array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O'], dtype=object),\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Het', 'overwicht', 'lag', 'op', 'nieuw', 'nummers', 'als', \"'\", 'Maria', 'Maria', \"'\", ',', \"'\", 'Put', 'Your', 'Lights', 'On', \"'\", 'en', \"'\", 'Smooth', \"'\", ',', 'stuk', 'voor', 'stuk', 'knappe', 'songs', 'die', 'zich', 'op', 'de', 'koop', 'toe', 'in', 'korte', ',', 'krachtige', 'versies', 'lieten', 'bewonderen', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object),\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Hieronder staan zinnen en JSON woordenboeken met de genoemde entiteiten die voorkomen in de gegeven zin.\n</code></pre></li> <li>Base prompt template:   <pre><code>Zin: {text}\nGenoemde entiteiten: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Zin: {text}\n\nIdentificeer de genoemde entiteiten in de zin. Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', 'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>persoon</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>persoon</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>locatie</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>locatie</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisatie</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisatie</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diversen</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diversen</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset conll-nl\n</code></pre>"},{"location":"datasets/dutch/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/dutch/#scala-nl","title":"ScaLA-nl","text":"<p>This dataset was published in this paper and was automatically created from the Dutch Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 13,603 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Met het toepassen van zelfbestuur wordt ook al op de lagere school begonnen.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vragen, die door een leek niet zo eenvoudig te zijn.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"U ziet een soort eng nachtclubomgeving, waar een groepje schertsaristocraten glazig zit te lachen om haar zouteloze tussenteksten, waarin ze wanhopig probeert een intelligent ondertoontje te leggen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hieronder staan zinnen en of ze grammaticaal correct zijn.\n</code></pre></li> <li>Base prompt template:   <pre><code>Zin: {text}\nGrammaticaal correct: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Zin: {text}\n\nBepaal of de zin grammaticaal correct is of niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nee</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nl\n</code></pre>"},{"location":"datasets/dutch/#unofficial-dutch-cola","title":"Unofficial: Dutch CoLA","text":"<p>This dataset is published here and is a manually annotated linguistic acceptability dataset, with documents coming from descriptions of Dutch syntax.</p> <p>The original full dataset consists of 19,900 / 2,400 / 2,400 samples for training, validation and testing, respectively (so 24,700 samples used in total). We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. The original splits were imbalanced, so we ensure a 50/50 split of correct/incorrect samples in the new splits. All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Tasman heeft geen Maori gezien.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Jan is vrij bang voor honden en ik ben het zeer erg voor spinnen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wat is het duidelijk dat Jan zal krijgen?\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p>"},{"location":"datasets/dutch/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/dutch/#squad-nl","title":"SQuAD-nl","text":"<p>This dataset is published here and is a machine translated dataset of the English SQuAD and XQuAD datasets. Google Translate was used to translate the original datasets to Dutch.</p> <p>These are based on English Wikipedia articles and the questions and answers are written by crowdworkers. It is not clear how the translations were done, this information is pending from the authors.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": \"Windows 8 bevat ook verbeterde ondersteuning voor mobiel breedband; het besturingssysteem kan nu de plaatsing van een simkaart detecteren en automatisch verbindingsinstellingen configureren (inclusief APN's en carrier-branding), en het internetgebruik verminderen om bandbreedte op gemeten netwerken te besparen. Windows 8 voegt ook een ge\u00efntegreerde instelling voor vliegtuigmodus toe om ook alle draadloze connectiviteit wereldwijd uit te schakelen. Vervoerders kunnen ook accountbeheersystemen aanbieden via Windows Store-apps, die automatisch kunnen worden ge\u00efnstalleerd als onderdeel van het verbindingsproces en gebruiksstatistieken bieden op hun respectievelijke tegel.\",\n  \"question\": 'Wat registreert het plaatsen van een simkaart?',\n  \"answers\": {\n    \"answer_start\": array([68]),\n    \"text\": array(['het besturingssysteem'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Het Duitse systeem van hoger onderwijs omvat twee vormen van academische instellingen: universiteiten en hogescholen (Fachhochschule). De universiteit van Jena is de grootste van de vier universiteiten van Th\u00fcringen en biedt bijna elke discipline. Het werd opgericht in 1558 en heeft vandaag 21.000 studenten. De op een na grootste is de Technische Universit\u00e4t Ilmenau met 7.000 studenten, opgericht in 1894, die veel technische disciplines biedt, zoals techniek en wiskunde. De universiteit van Erfurt, gesticht in 1392, heeft tegenwoordig 5.000 studenten en legt de nadruk op geesteswetenschappen en lerarenopleiding. De Bauhaus-universiteit Weimar is met 4.000 studenten de kleinste universiteit van Th\u00fcringen en is gespecialiseerd in creatieve vakken zoals architectuur en kunst. Het werd opgericht in 1860 en kreeg tijdens het interbellum bekendheid als de belangrijkste kunstacademie van Duitsland, het Bauhaus.',\n  \"question\": 'Wat is de grootste school in Th\u00fcringen?',\n  \"answers\": {\n    \"answer_start\": array([135]),\n    \"text\": array(['De universiteit van Jena'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Door di\u00ebten in westerse landen te vergelijken, hebben onderzoekers ontdekt dat hoewel de Fransen meer dierlijk vet eten, de incidentie van hartaandoeningen in Frankrijk laag blijft. Dit fenomeen wordt de Franse paradox genoemd en wordt verondersteld te ontstaan door de beschermende voordelen van het regelmatig consumeren van rode wijn. Afgezien van de mogelijke voordelen van alcohol zelf, waaronder verminderde aggregatie van bloedplaatjes en vasodilatatie, bieden polyfenolen (bijv. Resveratrol), voornamelijk in de druivenschil, andere vermoedelijke gezondheidsvoordelen, zoals:',\n  \"question\": 'Wat eten mensen in Frankrijk meer van dat in de meeste westerse landen?',\n  \"answers\": {\n    \"answer_start\": array([102]),\n    \"text\": array(['dierlijk vet'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Hieronder volgen teksten met bijbehorende vragen en antwoorden.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nVraag: {question}\nAntwoord in max 3 woorden: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBeantwoord de volgende vraag over de bovenstaande tekst in maximaal 3 woorden.\n\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor med maks. 3 ord.\n\nVraag: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset squad-nl\n</code></pre>"},{"location":"datasets/dutch/#knowledge","title":"Knowledge","text":""},{"location":"datasets/dutch/#mmlu-nl","title":"MMLU-nl","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Dutch was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Polarisatie is een eigenschap van\\nAntwoordopties:\\na. transversale golven.\\nb. longitudinale golven.\\nc. alle golven.\\nd. Geen van deze.\",\n  \"label\": \"a\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Welk internetbedrijf gaat onder de afkorting AOL?\\nAntwoordopties:\\na. Amerika Over Lijnen\\nb. Amerika Online\\nc. Amerikanen op Links\\nd. Amerikanen op LOR\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"Deze vraag verwijst naar de volgende informatie. Lees het volgende fragment. Nooit waren talenten van het hoogste genie van de meest verheven soort overvloediger geschonken aan een mens. Het genie van Napoleon is verbazingwekkend. Alle takken van menselijke kennis leken even vertrouwd voor zijn gigantische geest. Zijn conversaties op St. Helena, verspreid over de talloze en omvangrijke herdenkingsstukken van degenen die ze verzamelden, zijn gevuld met de grootste interesse. Tijdens de lange doodsstrijd van zijn gevangenschap en zijn dood, sprak hij met volledige vrijheid over de gebeurtenissen van zijn wonderbaarlijke carri\\u00e8re, en over al die onderwerpen van moralen, politiek en religie, die het meest diep de welvaart van ons ras betreffen. Er is geen geest die niet zal worden versterkt door bekendheid met deze diepzinnige gedachten, uitgedrukt met zoveel gloed van gevoel en energie van dictie. \\u2014 John S. C. Abbott, historicus, Napoleon op St. Helena, 1855 Napoleon hielp de Franse Revolutie tot een internationale beweging te maken in de gebieden die hij veroverde.\\nAntwoordopties:\\na. Door een universele valuta op basis van de Franse frank op te leggen\\nb. Door de brute onderdrukking van guerrilla-verzet\\nc. Door het afschaffen van feodalisme en herenboerderijen\\nd. Door het aanmoedigen van het gebruik van Frans als universele taal\",\n  \"label\": \"c\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-nl\n</code></pre>"},{"location":"datasets/dutch/#unofficial-arc-nl","title":"Unofficial: ARC-nl","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Dutch was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"In een graslandecosysteem, als de populatie van adelaars plotseling afneemt, wat zal waarschijnlijk het effect zijn op de rest van het ecosysteem?\\nAntwoordopties:\\na. Het ecosysteem zal overbevolkt worden met slangen.\\nb. Er zal een afname zijn in de populatie van slangen in het ecosysteem.\\nc. De voedingswaarde van de bodem zal afnemen in het ecosysteem.\\nd. Er zullen meer soorten planten beginnen te groeien in het ecosysteem.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ptolemeus was een oude astronoom die dacht dat de Aarde het centrum van het universum was. Toen hij observaties deed die hiermee niet overeenkwamen, stelde hij een verschijnsel genaamd \\\"epicycli\\\" voor om de observaties te verklaren. Hoe was Ptolemeus' proces vergelijkbaar met het moderne wetenschappelijke proces?\\nAntwoordopties:\\na. Ptolemeus baseerde zijn model deels op een geloofssysteem.\\nb. Observaties inspireerden Ptolemeus om zijn verklaringen aan te passen.\\nc. Ptolemeus probeerde het universum te beschrijven in plaats van het te verklaren.\\nd. Experimenten vormden de basis van Ptolemeus' model van het universum.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wat onderscheidt de organismen in het rijk Fungi van andere eukaryotische organismen?\\nAntwoordopties:\\na. Fungi zijn eencellig.\\nb. Fungi reproduceren seksueel.\\nc. Fungi verkrijgen voedingsstoffen door middel van absorptie.\\nd. Fungi maken voedsel door middel van fotosynthese.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-nl\n</code></pre>"},{"location":"datasets/dutch/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/dutch/#hellaswag-nl","title":"HellaSwag-nl","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use an 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Hoe maak je organische babydoekjes? [title] Kies een rol organische papieren handdoeken. [step] Deze dienen als de eigenlijke doekjes. Experimenteer met verschillende merken en texturen totdat je degene vindt die het beste werkt voor de huid van je baby.\\nAntwoordopties:\\na. Het is belangrijk om organische papieren handdoeken te gebruiken, omdat niet-organische papieren handdoeken bleekmiddel, verf en andere chemicali\\u00ebn kunnen bevatten die vaak worden gebruikt bij de productie van papierproducten. [substeps] Over het algemeen maken bekende merken van papieren handdoeken betere doekjes dan de goedkopere, generieke versies.\\nb. Je kunt een papieren handdoek gebruiken die gebruikt wordt voor luierdoekjes, maar je kunt dezelfde ook gebruiken voor andere doekjes. [substeps] Je kunt drie- of vierzijdige doekjes gebruiken om je te helpen bij het mengen van alle melk, yoghurt en water die je in \\u00e9\\u00e9n container hebt gemengd.\\nc. Als je zelfgemaakte lotion gebruikt, gebruik dan geen papieren handdoeken; deze moeten ook van niet-papier zijn. Rol een grote rol kleine papieren handdoeken uit en houd rekening met de algehele geur van de pad.\\nd. [substeps] Spreid het droge doekje uit over het hele oppervlak van de huid van je baby en vermijd contact met het droge doekje (tondeuse, kam of puimsteen). [title] Plaats de fles boven een kom met warm water gedurende 10 minuten.\",\n  \"label\": \"a\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hoe maak je een jurk zonder patroon [title] Koop een jurkmodel. [step] Je hebt een verstelbaar jurkmodel nodig om ervoor te zorgen dat je jurkontwerpen op exact de maat worden gemaakt die je nodig hebt. Verstelbare jurkmodellen zijn verkrijgbaar voor ongeveer $ 250 nieuw.\\nAntwoordopties:\\na. [substeps] Je kunt een schoenmakersstof, bedrukte binnenbekleding of bedrukt behang gebruiken om je jurkmodel te maken. Kies het patroon en knip het patroon zelf uit.\\nb. [title] Stel je jurkmodel af op de hoogte-, taille- en torso-maten die je gaat gebruiken voor je prototypejurk. [title] Maak een schets van de jurk die je wilt maken.\\nc. Als je van plan bent om strapless jurken te dragen, wil je misschien een jurkmodel kopen met een grotere voor-achter-maat. [title] Plaats je jurkmodel op de tafel.\\nd. Je kunt ook een jurkmodel in de supermarkt kopen. [substeps] Als je een strapless jurk wilt, kies dan voor een mouwloze jurk.\",\n  \"label\": \"b\",\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hoe citrusvruchten te raspen [title] Was de citrusvrucht. [step] Voordat je begint, spoel de vrucht af onder stromend koel water en wrijf het vervolgens zachtjes schoon met een schone doek of papieren handdoek. Een lichte spoeling helpt bij het verwijderen van het natuurlijke wasachtige residu aan de buitenkant van de vrucht.\\nAntwoordopties:\\na. [substeps] Zorg ervoor dat de vrucht volledig is afgespoeld voordat je doorgaat naar de volgende stap. De meeste citrusvruchten hebben het beschadigde deel verwijderd, maar met het middenstuk kun je afwisselen tussen het opfrissen van de schil met water en het verwijderen van de schil.\\nb. [substeps] Het werk kan het beste ook laat in de avond worden gedaan, nadat de suiker is verdampt. [title] Maak een zure citrus door een kom met zout in het water te dompelen.\\nc. Je kunt de citrusvrucht ook kort laten weken in een ondiepe kom met water. [substeps] Het is belangrijk om citrusvruchten altijd te wassen wanneer je ze raspt, omdat de buitenkant het deel is dat daadwerkelijk in je voedsel terechtkomt.\\nd. [title] Doe het mengsel van rasp in een druppelaar. [step] Commercieel verkrijgbare rasp komt van de schil van de citrusboom.\",\n  \"label\": \"c\",\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Hieronder staan meerkeuzevragen (met antwoorden).\n</code></pre></li> <li>Base prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAntwoord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Vraag: {text}\nAntwoordopties:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantwoord de bovenstaande vraag met 'a', 'b', 'c' of 'd', en niets anders.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-nl\n</code></pre>"},{"location":"datasets/dutch/#summarization","title":"Summarization","text":""},{"location":"datasets/dutch/#wikilingua-nl","title":"WikiLingua-nl","text":"<p>This dataset was published here and consists of Dutch WikiHow articles and their summaries, where a summary consists of the first sentence of each \"how-to\" step in the article (and this first sentence is not included in the article text).</p> <p>The original full dataset consists of 21,345 / 3,058 / 6,105 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Je gaat de ham ongeveer 15 tot 20 minuten glaceren voordat hij klaar is met koken. Om het glazuur op tijd klaar te hebben, begin je met de bereiding ervan ongeveer 45 tot 60 minuten voordat je verwacht dat de ham klaar zal zijn. Snelle glazuren zijn in een paar minuten klaar, en zelfs de glazuren die op het fornuis moeten sudderen, nemen minder dan 15 minuten in beslag. Voor de eenvoudigste optie zonder te koken, klop je gewoon 270 g donkerbruine suiker met 60 ml sinaasappelsap, rode wijn of cognac. Meng de ingredi\\u00ebnten in een kleine kom totdat de suiker volledig is opgelost. Als alternatief, combineer je 270 g lichtbruine suiker, 160 ml sojasaus, en twee gehakte knoflookteentjes in een kleine steelpan -- breng dan de ingredi\\u00ebnten aan de kook op gemiddeld vuur. Zet  de temperatuur lager zodra het mengsel aan de kook is. Roer het af en toe door en laat het 3-5 minuten sudderen, of tot het iets is ingedikt. Zet dan het vuur uit en laat het glazuur minstens 10 tot 15 minuten afkoelen alvorens het over de ham te strijken. Klop 320 ml melasse, 160 ml bourbon en \\u00bd theelepel (1 g) gemalen kruidnagel in een kleine steelpan. Breng de ingredi\\u00ebnten aan de kook op middelmatig vuur, zet het vuur dan laag en laat het onder af en toe roeren, sudderen gedurende 3-5 minuten. Op het moment dat het mengsel iets verdikt is, zet je het vuur uit en laat je het 10 tot 15 minuten afkoelen. Combineer 180 ml ahornsiroop, 120 ml sinaasappelmarmelade, 2 eetlepels (30 g) ongezouten boter, 1 eetlepel (16 g) Dijon-mosterd, 1 theelepel (2 g) gemalen zwarte peper, en \\u00bc theelepel gemalen kaneel in een kleine steelpan. Laat het mengsel op matig vuur sudderen, onder af en toe roeren, gedurende 5-10 minuten, of totdat het stroperig is en is ingedikt tot 240 ml. Laat het glazuur minstens 10 tot 15 minuten afkoelen alvorens het over de ham te strijken. Er zijn talloze recepten voor glazuren te vinden, maar het bedenken van een eigen glazuur is eenvoudig. Experimenteer met ingredi\\u00ebnten tot je de zoete, zure en hartige smaken in balans hebt gebracht. Streef naar ongeveer 240 tot 500 ml glazuur, en reserveer ongeveer een derde ervan voor op de eettafel. De basisingredi\\u00ebnten van een glazuur zijn een zoetstof (zoals bruine suiker of melasse), een zuur (zoals azijn of sinaasappelsap), en kruiden of specerijen (zoals tijm of kruidnagel).\",\n  \"target_text\": \"Bereid het glazuur voor nadat je de ham in de oven hebt gezet. Klop een glazuur van bruine suiker voor een eenvoudige klassieker. Sudder een sojasausglazuur voor een hartige smaak. Combineer bourbon, melasse en kruidnagel voor een diep, warm glazuur. Maak een esdoorn-sinaasappelglazuur voor een pittige, opvallende smaakcombinatie. Bedenk je eigen aangepaste glazuur.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Je koplampen zijn je meest belangrijke levenslijn tijdens het rijden in het donker. Als ze niet in goede conditie zijn, vergroot je onnodig het risico op een ongeval. Houd je koplampen schoon door ze om de paar weken te wassen -- dit houdt de helderheid en scherpte van de lichtbundel hoog. Als een koplamp opbrandt, vervang deze dan zo snel mogelijk en rijd niet in het donker totdat de lamp hersteld is. Het is daarnaast overigens ook verboden om auto te rijden zonder goed werkende koplampen. Bovendien moet je voor de meeste zichtbaarheid je voorruit, ramen en spiegels zo helder en schoon maken als je kunt. Veeg deze belangrijke onderdelen van je auto niet schoon met je hand -- de natuurlijke olie van je huid kan vlekken op de spiegel achterlaten. Gebruik in plaats daarvan een krant of microvezeldoekje. De verstralerlichten van je auto kunnen je veiligheid significant vergroten wanneer je 's nachts rijdt, maar alleen als je ze correct gebruikt. Verstralers gebruik je bij het rijden door zeer donkere gebieden met weinig zicht, waar er niet veel verkeer is. In deze gevallen kunnen verstralers je gezichtsbereik veel breder en langer maken, dus gebruik ze waar nodig.  Zorg dat je verstralers uitschakelt wanneer je achter een andere auto rijdt of als er tegenliggers zijn. In deze gevallen kan het heldere licht van de verstralers andere automobilisten verblinden, waardoor het moeilijker voor hen wordt om veilig te rijden. Als je afslaat bij een bocht of over een heuveltop gaat en de zwakke gloed ziet van de koplampen van een andere auto, zet je verstralers dan voor alle zekerheid uit, zodat de andere bestuurder niet plotseling wordt verblind. Soms, zijn de koplampen van een auto schuiner naar de grond gericht dan nodig is, of zijn ze niet perfect symmetrisch uitgelijnd. De helderste koplampen in de wereld zijn niet nuttig als de weg voor je niet naar behoren verlichten. Dus als je merkt dat het moeilijk is om de weg voor je te zien tijdens het rijden in het donker, dan kun je overwegen om je koplampen opnieuw bij te stellen. Bij een professionele garage is deze procedure meestal heel snel en goedkoop geregeld. Het is ook mogelijk om zelf je koplampen bij te stellen. Aangezien iedere auto anders is, zal je de handleiding van je auto moeten raadplegen. Wees geduldig, want het kan even duren om koplampen perfect uitgelijnd te krijgen. In een perfecte wereld zouden andere bestuurders altijd hun verstralers dimmen als ze je zien, net zoals jij voor hen zou doen. Helaas willen automobilisten dit nog wel eens vergeten. Als een tegemoetkomende auto verstralers aan heeft staan, kijk daar dan niet naar, want het felle licht kan je tijdelijk verblinden. Kijk in plaats daarvan naar de rechterkant van je rijbaan (of in landen waar je aan de linkerkant van de weg rijdt, naar links), terwijl je vanuit je perifere zicht op gevaren let. Dit houdt je zo opmerkzaam mogelijk op de gevaren om je heen, met behoud van je zicht. Als een auto achter je verstralers aan heeft staan, probeer dan je achteruitkijkspiegel te verstellen om het licht uit je ogen te houden. Je kunt zelfs de spiegel zo instellen dat het licht weerkaatst naar de bestuurder van die auto, om hem te wijzen op zijn fout. Als je verwacht dat je veel 's nachts gaat rijden en onder mistige omstandigheden, dan kun je overwegen om te investeren in een set mistlampen. Vaak zijn deze lichten laag gemonteerd op de voorbumper om zoveel mogelijk wegdek te verlichten (mist is het dunst tot op een halve meter of zo boven het wegdek). Niet alle aftermarket lichten zijn even goed gemaakt, dus praat met je autodealer alvorens deze aanschaf te doen. Gebruik nooit je standaard verstralers in de mist. De reflecterende waterdeeltjes waaruit mist bestaat kunnen het heldere licht naar je terugkaatsen, waardoor je nog minder van de weg kunt zien dan zonder licht. De koplampen van andere auto's (en vooral verstralers) kunnen unieke uitdagingen vormen voor chauffeurs met een bril. Glazen kunnen soms tegemoetkomend licht op manieren reflecteren die een verduisterende schittering vormt voor de brildrager. Om dit te voorkomen kun je contactlenzen proberen of een brilglazen kopen met een anti-reflecterende coating, om deze effecten te minimaliseren. Als je een paar speciale brilglazen koopt, leg die dan in je auto zodat je ze altijd bij de hand hebt wanneer je de weg op gaat.\",\n  \"target_text\": \"Houd je koplampen, spiegels en voorruit in topconditie. Gebruik je verstraler voor situaties met weinig licht. Pas eventueel je koplampen aan. Ga op de juiste manier om met verstralers van andere weggebruikers door naar de kant van de weg te kijken. Overweeg om lage mistlampen te installeren. Draag je een bril, gebruik dan een anti-reflecterende coating.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Over het algemeen hebben raszuivere Cavaliers voorspelbare eigenschappen. Als je een raszuivere Cavalier koopt, kun je verwachten dat ze energieke, knuffelbare huisdieren zijn met een redelijk te onderhouden vacht. Genetisch bepaald hebben Cavaliers een neiging tot zorgeloosheid. Als je een rashond koopt, kun je een dergelijk karakter verwachten. Niet raszuivere Cavaliers kunnen sommige van de biologische eigenschappen overnemen van om het even welk ander ras waar ze mee gekruist zijn. Als ze zijn gekruist met een jachthond, dan kunnen ze een sterker jachtinstinct hebben, op dezelfde manier kunnen ze, als ze met een ras zijn gekruist met minder energie, zoals de shih tzu, dat energieke enthousiasme kwijtraken waar je in de eerste plaats op gevallen bent. Mensen hebben hun zinnen gezet op raszuivere Cavaliers. Dit betekent dat ze uit een beperkte genenpoel gefokt zijn. Om aangeduid te worden als raszuiver, wordt er op veel plaatsen inteelt gedaan met hun honden, en anderen hebben onwetend gefokt met een genenpoel die te klein is. Dit heeft heel realistische en bijzonder ongewenste consequenties. Raszuivere Cavaliers hebben een verhoogd risico op hartklachten, hernia en/of ernstige neurologische aandoeningen.   Hartziekte: in Engeland heeft 59% van de Cavaliers ouder dan 4 jaar een hartruis. Zijnde bijna tweederden van de populatie Cavaliers in Engeland is dit een uitzonderlijk statistisch gegeven.  Chiari misvorming en Syringomyelia: Kort gezegd betekent deze aandoening dat de schedel van de hond te klein is voor zijn hersenen. Dit veroorzaakt afschuwelijke zenuwpijn. Het diergeneeskundige leerboek \\\"Breed Predispositions to Disease in the Dogs and Cats\\\" bestempelt deze aandoening als \\\"veel voorkomend\\\" met tekenen die zich ontwikkelen tussen de leeftijd van 5 maanden tot 3 jaar.   Epilepsie: Honden kunnen op elk moment aanvallen ontwikkelen, maar tussen de 6 maanden en 6 jaar is de meest voorkomende periode.  Hernia:  Dit is een andere \\\"veelvoorkomende\\\" afwijking, vooral als Cavaliers ouder worden.  In de meeste gevallen zul je niet weten dat je Cavalier gevoelig is voor een hernia, tot je hem stijf ziet lopen of zijn hoofd met tegenzin naar beneden brengt naar zijn voerbak of waterbak.\",\n  \"target_text\": \"Overweeg de voordelen als je kiest voor een raszuivere Cavalier. Stel vast wat de schaduwzijden zijn van het kopen van een rashond. Houd algemene gezondheidsproblemen van de Cavalier in gedachten.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Hieronder volgen artikelen met bijbehorende samenvattingen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSamenvatting: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSchrijf een samenvatting van het bovenstaande artikel.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset wiki-lingua-nl\n</code></pre>"},{"location":"datasets/english/","title":"\ud83c\uddec\ud83c\udde7 English","text":"<p>This is an overview of all the datasets used in the English part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/english/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/english/#sst-5","title":"SST-5","text":"<p>This dataset was published in this paper and is based on movie reviews from rottentomatoes.com, labelled by crowdsourced workers on Amazon Mechanical Turk.</p> <p>The original full dataset consists of 8,540 / 1,100 / 2,210 samples for the training, validation and test splits, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. All the new splits are subsets of the original splits.</p> <p>The original dataset consists of 5 labels instead of our usual 3, but we map them to <code>positive</code>, <code>neutral</code> and <code>negative</code> as follows:</p> <ul> <li><code>very negative</code> \u27a1\ufe0f <code>negative</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negative</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>positive</code> \u27a1\ufe0f <code>positive</code></li> <li><code>very positive</code> \u27a1\ufe0f <code>positive</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"the leads are natural and lovely , the pace is serene , the humor wry and sprightly .\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"labute ca n't avoid a fatal mistake in the modern era : he 's changed the male academic from a lower-class brit to an american , a choice that upsets the novel 's exquisite balance and shreds the fabric of the film .\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"no cliche escapes the perfervid treatment of gang warfare called ces wild .\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>The following are texts and their sentiment, which can be 'positive', 'neutral' or 'negative'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nClassify the sentiment in the text. Answer with 'positive', 'neutral' or 'negative'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positive</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negative</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sst5\n</code></pre>"},{"location":"datasets/english/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/english/#conll-2003-en","title":"CoNLL-2003-En","text":"<p>This dataset was published in this paper and was part of the CoNNL-2003 shared task. The data comes from the Reuters Corpus and consists of news articles between August 1996 and August 1997, labelled with named entities.</p> <p>The original full dataset consists of 14,041 / 3,250 / 3,453 samples for the training, validation and test splits, respectively. We use 1,024 / 256 / 2,048 samples for our training, validation and test splits, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['SK', 'Slavia', 'Praha', '3', '1', '2', '0', '6', '3', '5'], dtype=object),\n  'labels': array(['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Guy', 'Whittingham', 'stole', 'three', 'points', 'for', 'the', 'Yorkshire', 'side', 'with', 'a', 'goal', '10', 'minutes', 'from', 'time', '.'], dtype=object),\n  'labels': array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.'], dtype=object),\n  'labels': array(['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Below are sentences and JSON dictionaries with the named entities that occur in the given sentence.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nNamed entities: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nIdentify the named entities in the sentence. You should output this as a JSON dictionary with the keys being 'person', 'location', 'organization' and 'miscellaneous'. The values should be lists of the named entities of that type, exactly as they appear in the sentence.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>location</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>location</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organization</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organization</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>miscellaneous</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>miscellaneous</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset conll-en\n</code></pre>"},{"location":"datasets/english/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/english/#scala-en","title":"ScaLA-En","text":"<p>This dataset was published in this paper and was automatically created from the English Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original full dataset consists of 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"And so we have to labour and to work, and to work hard, to give reality to our dreams.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"This couch is also quite big, it fits three people quite comfortably, and if I have or friends staying over, it opens up into a full double bed.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"While studies the psychology of art have focused on individual works and distinctions between representative / non-representative topics, no work has been completed on the aesthetic appreciation of collections or of devotional themes.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>The following are sentences and whether they are grammatically correct.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nGrammatically correct: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nDetermine whether the sentence is grammatically correct or not. Reply with 'yes' if the sentence is correct and 'no' if it is not.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>yes</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>no</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-en\n</code></pre>"},{"location":"datasets/english/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/english/#squad","title":"SQuAD","text":"<p>This dataset was published in this paper, which is based on English Wikipedia articles and the questions and answers are written by crowdworkers.</p> <p>The original full dataset consists of 130,000 / 11,900 samples for training and validation, respectively. We use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). The new training split is a subset of the original training split, and the new validation and test splits are disjoint subsets of the original validation split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'The Federation of International Gymnastics (FIG) was founded in Liege in 1881. By the end of the nineteenth century, men\\'s gymnastics competition was popular enough to be included in the first \"modern\" Olympic Games in 1896. From then on until the early 1950s, both national and international competitions involved a changing variety of exercises gathered under the rubric, gymnastics, that would seem strange to today\\'s audiences and that included for example, synchronized team floor calisthenics, rope climbing, high jumping, running, and horizontal ladder. During the 1920s, women organized and participated in gymnastics events. The first women\\'s Olympic competition was primitive, only involving synchronized calisthenics and track and field. These games were held in 1928, in Amsterdam.',\n  'question': 'When was gymnastics included in the Olympics?',\n  'answers': {\n    'answer_start': array([219], dtype=int32),\n    'text': array(['1896'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"London's buildings are too diverse to be characterised by any particular architectural style, partly because of their varying ages. Many grand houses and public buildings, such as the National Gallery, are constructed from Portland stone. Some areas of the city, particularly those just west of the centre, are characterised by white stucco or whitewashed buildings. Few structures in central London pre-date the Great Fire of 1666, these being a few trace Roman remains, the Tower of London and a few scattered Tudor survivors in the City. Further out is, for example, the Tudor period Hampton Court Palace, England's oldest surviving Tudor palace, built by Cardinal Thomas Wolsey c.1515.\",\n  'question': \"The area west of London's city is characterized by what type of building?\",\n  'answers': {\n    'answer_start': array([328], dtype=int32),\n    'text': array(['white stucco or whitewashed'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Along with the rest of South West England, Plymouth has a temperate oceanic climate (K\u00f6ppen Cfb) which is generally wetter and milder than the rest of England. This means a wide range of exotic plants can be grown. The annual mean temperature is approximately 11 \u00b0C (52 \u00b0F). Due to the modifying effect of the sea the seasonal range is less than in most other parts of the UK. As a result of this summer highs are lower than its southerly latitude should warrant, but as a contrast the coldest month of February has mean minimum temperatures as mild as between 3 and 4 \u00b0C (37 and 39 \u00b0F). Snow is rare, not usually equating to more than a few flakes, but there have been exclusions, namely the European winter storms of 2009-10 which, in early January, covered Plymouth in at least 1 inch (2.5 cm) of snow; more on higher ground. Another period of notable snow occurred from 17\u201319 December 2010 when up to 8 inches (20 cm) of snow fell through the period \u2013 though only 2 inches (5.1 cm) would lie at any one time due to melt. Over the 1961\u20131990 period, annual snowfall accumulation averaged less than 7 cm (3 in) per year. July and August are the warmest months with mean daily maxima over 19 \u00b0C (66 \u00b0F).',\n  'question': 'What month in Plymouth has the lowest temperatures?',\n  'answers': {\n    'answer_start': array([503], dtype=int32),\n    'text': array(['February'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>The following are texts with accompanying questions and answers.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nQuestion: {question}\nAnswer in max 3 words:\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nAnswer the following question about the above text in at most 3 words.\n\nQuestion: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset squad\n</code></pre>"},{"location":"datasets/english/#knowledge","title":"Knowledge","text":""},{"location":"datasets/english/#mmlu","title":"MMLU","text":"<p>This dataset was published in this paper and features questions within 57 different topics, such as elementary mathematics, US history and law.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Use the following key to translate the given formula of PL to natural, English sentences. A: Marina reads a Percy Jackson book. B: Izzy plays Minecraft. C: Emily stops working. D: Russell makes dinner. E: Ashleigh stops by. ~(A \u2283 B) \u2022 (B \u2283 ~E)\\nChoices:\\na. It's not the case that Marina's reading a Percy Jackson book entails that Izzy plays Minecraft, but Izzy's playing Minecraft does entail that Ashleigh doesn't stop by.\\nb. If Marina doesn't read a Percy Jackson book, then Izzy plays Minecraft, which entails that Ashleigh doesn't stop by.\\nc. Marina's reading a Percy Jackson book does not entail that Izzy plays Minecraft, but Izzy plays Minecraft provided that Ashleigh doesn't stop by.\\nd. It's not true that Marina reads a Percy Jackson book only when Izzy plays Minecraft, but Izzy plays Minecraft only when Ashleigh stops by.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"As of 2017, the share of GDP spent on the military by the United States is about\\nChoices:\\na. 1%\\nb. 3%\\nc. 6%\\nd. 10%\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Question 13. A buyer sent a signed letter to a seller that stated: \\\"Ship 100 boxes of nails at $3 per box, the price quoted in your circular.\\\" The seller mailed the buyer a signed form acknowledgment that agreed to the buyer's terms and stated on the reverse side: \\\"Disputes regarding quality shall be arbitrated.\\\" The buyer did not reply to the seller's acknowledgment, and the seller shipped the nails. When the buyer received the nails, it found their quality to be unsatisfactory and sued the seller for breach of warranty. The seller has asked an attorney whether the parties' contract requires arbitration of the buyer's claim. What is the best advice the attorney can provide?\\nChoices:\\na. A contract was formed pursuant to conduct when the buyer received the nails, and a court would exclude the arbitration provision from the contract.\\nb. A contract was formed when the seller mailed its acknowledgment, and the arbitration term became part of the contract. arbitration term became part of the contract.\\nc. A contract was formed when the seller mailed its acknowledgment, and the court must decide whether the arbitration term should be excluded as a material alteration of the contract.\\nd. No contract exists, because the arbitration term in the seller's acknowledgment created a counteroffer that the buyer never accepted.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu\n</code></pre>"},{"location":"datasets/english/#unofficial-arc","title":"Unofficial: ARC","text":"<p>This dataset was published in this paper and features US grade-school science questions.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Several horses grazed in a fenced area across from a home. On rainy days, soil would wash down a slope and run toward the home. After the horses were moved a few years later, the soil no longer washed down when it rained. What could account for this change?\\nChoices:\\na. The grass grew and kept the soil intact.\\nb. The fence kept the soil contained.\\nc. The soil was completely gone.\\nd. The amount of rain decreased.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"How do moose use a learned behavior to protect themselves?\\nChoices:\\na. They have hollow hair to keep warm in the winter.\\nb. They roll in a pool of muddy water to avoid fly bites.\\nc. They have keen hearing to sense danger in the forest.\\nd. They use their wide hooves to prevent sinking in deep snow.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"A plant that grows red flowers was crossed with the same kind of plant that grows white flowers. Their offspring grew pink flowers. Which best explains why the offspring grew pink flowers?\\nChoices:\\na. The offspring experienced a genetic mutation.\\nb. The offspring resulted from asexual reproduction.\\nc. The genes for flower color exhibited incomplete dominance.\\nd. A gene for pink-colored flowers was recessive in one of the parents.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc\n</code></pre>"},{"location":"datasets/english/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/english/#hellaswag","title":"HellaSwag","text":"<p>This dataset was published in this paper and is based on both video descriptions from ActivityNet as well as how-to articles from WikiHow.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] How to solo travel to chile [title] Decide how you will get to chile. [step] Start by figuring out how you will get to chile. If you live in north america, you may decide to fly into a major city in the country, such as santiago, and then take public transit to get around or fly within chile.\\nChoices:\\na. If you live in south america, it may be possible to take a bus or a train into chile, depending on your budget and your timeframe for the trip. [substeps] There is no special visa required for you to travel into chile and no fee to cross the border into chile.\\nb. If you live in australia, you will need to negotiate a road trip, such as a train or bus, to get around chile. [substeps] Plan out the route in advance of arrival so that you can do the same to chile in the future.\\nc. If you live in a rural area or you do not plan to travel for a long time, you may opt to take a bus. Using a bus or subway to get around chile is a good route to travel.\\nd. If you live in a smaller area, or if you live near a large tourist attraction, you may decide to fly in the opposite direction. [substeps] Skiing, mountain climbing, and bicycle riding are examples of solo travel.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"The video begins with a title sequence. a young man\\nChoices:\\na. prepares to black out.\\nb. is shown in a gym performing tricks with a jump rope as music plays in the background.\\nc. is seen talking continuously about slamming the mouth of a chimpanzee into the camera.\\nd. is standing outside with a basketball in his hand, alternating between shots of dribbling for the ball.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"A herb garden appears with a woman standing next to it in a large garden next to a wheelbarrow filled with mulch. the woman\\nChoices:\\na. moves the mulch across the ground in the wheelbarrow, falling backwards on attempts.\\nb. takes some of the mulch away and starts bagging it in the wheelbarrow.\\nc. begins to talk to the camera while gesturing to the flowerbed and the mulch, before eventually picking up a handful of the mulch.\\nd. then begins to mulch close to the wheelbarrow with mulching tool in her hand and while waving her arms in the air.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>The following are multiple choice questions (with answers).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nAnswer: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nOptions:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nAnswer the above question by replying with 'a', 'b', 'c' or 'd', and nothing else.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag\n</code></pre>"},{"location":"datasets/english/#summarization","title":"Summarization","text":""},{"location":"datasets/english/#cnndailymail","title":"CNN/DailyMail","text":"<p>This dataset was published in this paper and is based on news articles from CNN and DailyMail, with the summaries derived from bullet points written by the authors of the articles.</p> <p>The original full dataset consists of 287,113 / 13,368 / 11,490 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Reality TV star and conservative firebrand Sarah Palin said today she's 'interested' in running for president in 2016 but stopped short of saying she'd actually seek higher office. 'Yeah, I mean, of course, when you have a servant\u2019s heart, when you know that there is opportunity to do all you can to put yourself forward in the name of offering service, anybody would be interested,' Palin told ABC News reporter Neal Karlinsky. Later stating, 'America has had enough of seeing that...sign on the Oval Office door saying, \\\"No Girls Allowed.\\\" ' 'It doesn't necessarily have to be me though,' she said. Scroll down for video . Conservative firebrand Sarah Palin said today she's 'interested' in running for president in 2016 but stopped short of saying she'd actually seek higher office . GIRL POWER: 'America has had enough of seeing that...sign on the Oval Office door saying, \\\"No Girls Allowed,\\\" ' Palin said . NOM NOM NOM: Palin made the comments while serving wild boar chili to the Salvation Army in Las Vegas, Nevada, on Friday. She was hosting an episode of Sportsman Channel program Hunt.Fish.Feed . ABC News caught up with Palin while she was serving wild boar chili to the homeless at a Las Vegas, Nevada, Salvation Army for an episode of Sportsman Channel program Hunt.Fish.Feed. She's also in the midst of promoting her hunting show, Amazing Alaska, about to begin its second season. Palin said the GOP needs to nominate a candidate 'who can take on Hillary' and 'show the nation what it is going to take to get the country back on the right track.' 'Because we can't afford status quo,' the former Alaska governor said in a clip of the interview released by ABC this afternoon. 'Status quo lately has been Latin for, \\\"We're getting screwed,' and status quo has got to go.' The Republican nominee out to be someone can 'turn things around, someone who will, in some respects, I don\u2019t know, maybe be considered a bit avant garde, to the establishment anyway, because this next person has got to realize this is war, this is war for our hunters\u2019 future,' she said at another point in the interview, according to ABC. Asked about former Florida Gov. Jeb Bush's candidacy and 2012 Republican presidential nominee Mitt Romney, Palin snarked, 'I can\u2019t wait for new energy.' Moments later asserting that the GOP primary 'had better be a competition and not a coronation.' Palin, the 2008 vice presidential nominee, said she doesn't 'have to be' the Republican candidate for president but she's 'happy to drive that competition, because competition will make everyone better and produce more and be more candid regarding their solutions they will offer this country. 'I am very interested in that competitive process and, again, not necessarily me.' Former Alaska Palin is pictured here on Thursday at an event to promote her television show, Amazing America with Sarah Palin, at the Shooting, Hunting and Outdoor Trade Show in Las Vegas . The hard-charging, Tea Party icon appears to have a change of heart in the last week about the Oval Office needing a female touch. 'I don't give a flying flip about what gender the person will be,' Palin told Inside Edition after host Deborah Norville asker her about the importance of electing a female president. 'I want the absolute best because America deserves the best, in terms of leadership, getting this country on the right track,' she continued. She ultimately concluded 'it would be nice' to have a woman president, though, 'and it will be nice to see women jump into the ring.' Voicing her support for female candidates in December, Palin told\u00a0Extra TV, 'I would love to see a woman on both sides of the aisle shooting for that top spot.'\",\n  \"target_text\": \"'When you know that there is opportunity to do all you can to put yourself forward in the name of offering service, anybody would be interested'\\nPalin added: 'It doesn't necessarily have to be me though'\\nThe conservative firebrand appears to have a change of heart about the Oval Office needing a female touch .\\nLast week she said: 'I don't give a flying flip about what gender the person will be'\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"By . Amanda Williams . The dictionary makers have taken to Twitter to find new words for the next edition of the lexicon - asking users to choose which words should make the final edition . The latest edition of the Collins English Dictionary could include Twitter slang words such as 'adorkable' and 'fatberg'. The dictionary makers have taken to Twitter to find new words for the next edition of the lexicon - asking users to choose which words should make the final edition. The list of suggested words includes fracktivist - someone who protests against fracking - and felfie, a term used to describe a farmer who takes a selfie, or photograph of themselves. The 12th edition of the dictionary will be the first to contain a word that has been voted for by Twitter users - who have until midnight on May 28 to vote for the new word. Once selected, it will be included in the next edition of the dictionary, which is released in October. The dictionary publisher says that the rise of social media and the hashtag has seen new words and ideas - that they scout for every year - become mainstream much quicker than in the past. Andrew Freeman, associate publisher at Collins, said: 'Twitter offers us an immediate snapshot of how much a word is used. 'The tried and tested approach to compiling dictionaries has to adapt to embrace the ways in which language is developing through use on social media, and this is a fun way to get Twitter users involved in defining the English language.' Collins has been publishing the dictionary since 1819 and is the largest single volume dictionary in print, with the words it contains sourced from the Collins Corpus, which contains more than 4.5 billion words, as well as the open source site collinsdictionary.com, where users can submit words for consideration. The latest edition of the Collins English Dictionary could include Twitter slang words such as 'adorkable' The word felfie, a term used to describe a farmer who takes a selfie, or photograph of themselves could also be included . Nomakeupselfie - a selfie of a woman without make-up, posted online to raise awareness for a charity - is also in the running to be used in the dictionary . Lucy Mangan, a blogger for collinsdictionary.com and a contributor to the Collins English Dictionary, said: 'Twitter is the perfect place to find out what people are really saying and how they\u2019re saying it. 'It\u2019s a space in which you\u2019re freer than almost anywhere else to combine old words, resurrect others or invent totally new ones whenever the need arises.' According to language experts, the list, which also contains the word adorkable, referring to someone who is dorky in an adorable way, is a sign of the way language is changing in the 21st century. Ian Brookes, lexicographer and consultant editor to the Collins English Dictionary, said: 'Language has always had to develop in response to changes in society and technology. In the 20th century the development of the motor car, air travel, television, and the personal computer changed the things that people did and so brought many new words into the language. 'In the 21st century, the growth of social media has had a comparable effect. Twitter users can vote for their choice by visiting twictionary.collinsdictionary.com . Adorkable - dorky in an adorable way . Fatberg - a large mass of solid waste, grease etc, clogging a sewage system . Felfie - a farmer selfie . Gaybourhood - a gay-friendly neighbourhood, e.g. Castro in San Francisco . Nomakeupselfie - a selfie of a woman without make-up, posted online to raise awareness for a charity . Vaguebooking - posting a deliberately vague status updates on social media to prompt a response . Duckface - the traditional pouting facial expression in selfies . Fracktivist - an activist who protests against fracking . Euromaiden - the original pro-Europe protests in Ukraine, named for Maidan Square in Kiev .\",\n  \"target_text\": \"Dictionary makers have taken to Twitter to find new words for next edition .\\nThe suggested words include fracktivist - an anti-fracking protester .\\nFelfie - a term used to describe a farmer who takes a selfie - also included .\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"There were three of them, one of them probably a child, and at least one met a gruesome end at the hands of a terrifying predator. About 67 million years later, a Wyoming rancher led scientists to their remains. Now experts are digging out one of the most complete skeletons yet of a Triceratops, the three-horned, plant-eating dinosaur that was one of the last of the giant reptiles. \\\"There's only three other skeletons that will match the completeness of one of the specimens we're excavating right now,\\\" said paleontologist Peter Larson, president of the Black Hills Institute of Geological Research. Most of the remains found before now have included fewer than half of the prehistoric creatures' bones, Larson said Monday. The most complete to date, now on display at the Houston Museum of Natural Science in Texas, has about 76% of its skeleton. \\\"The largest, more mature individual appears to be the most complete,\\\" Larson said. \\\"One is just a bit smaller, and there's another one that by live weight is probably only half the size.\\\" Will mammoths be brought back to life? Liquid blood fuels cloning hopes . The dig is going on near Newcastle, Wyoming, more than 200 miles north of Cheyenne. \\\"The fact that there are three of them together is really cool,\\\" Larson said. The trio could be male and female and their young, or they could be two females looking after a juvenile dinosaur, he said. And before now, there was no indication that the Triceratops moved in groups. The Black Hills Institute is working with the Naturalis Biodiversity Center, from the Netherlands, on the dig. Larson called the discovery of a young Triceratops a \\\"very significant\\\" find as well, since it will give scientists an insight into how the great lizards grew up. Newly discovered dinosaur fossil is a primitive bird . Triceratops lived in the twilight of the Cretaceous Period, about a half a million years before the dinosaurs' extinction. Much of what is now the Great Plains and southern Canada was once part of a vast inland sea, and the region is rich in fossils. \\\"Like most of the specimens that were found, it was brought to our attention by a rancher,\\\"  Larson said. The rancher sent photos to the Black Hills Institute, located in neighboring South Dakota, in late 2012. Excavation began in May and is expected to take about a month. So far, the bones that have turned up point to a violent end, probably at the hands of the feared Tyrannosaurus rex. On the largest of the three specimens, at least two of the major limb bones were \\\"bitten through,\\\" Larson said. \\\"If you can imagine, this is a bone that is nearly four feet long,\\\" he said. But a T.rex \\\"would kind of chop the carcass up with their giant, shearing jaws,\\\" ripping through flesh and bone alike. \\\"I think we also have a feeding site for Tyrannosaurus rex, which is very exciting,\\\" he said. \\\"This is potentially a site where we can learn the behavior of two different species.\\\" More science news on CNN's Light Years blog .\",\n  \"target_text\": \"A rancher led scientists to the remains of three Triceratops .\\nOne of the three may be the most complete skeleton yet found .\\nA young dinosaur is among the trio .\\nAt least one may have been killed by a Tyrannosaurus rex .\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>The following are articles with accompanying summaries.\n</code></pre></li> <li>Base prompt template:   <pre><code>News article: {text}\nSummary: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>News article: {text}\n\nWrite a summary of the above article.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset cnn-dailymail\n</code></pre>"},{"location":"datasets/faroese/","title":"\ud83c\uddeb\ud83c\uddf4 Faroese","text":"<p>This is an overview of all the datasets used in the Faroese part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/faroese/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/faroese/#fosent","title":"FoSent","text":"<p>This dataset was published in this paper and is based on 170 news articles from the Faroese news sites Portalurin and Dimmal\u00e6tting. The sentiment labels were manually annotated by two native speakers.</p> <p>The original full dataset consists of 245 samples, which consisted of both a news article, a chosen sentence from the article, and the sentiment label. We use both the news article and the chosen sentence as two separate samples, to increase the size of the dataset (keeping them within the same dataset split). In total, we use a 74 / 35 / 283 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Eg koyri teg, t\u00fa koyrir meg Hetta er \u00e1rst\u00ed\u00f0in, har vit vanliga fara \u00ed j\u00f3labor\u00f0hald at hugna okkum saman vi\u00f0 vinum og starvsfel\u00f8gum. Og h\u00f3ast vit kanska ikki hittast og koma saman \u00e1 j\u00fast sama h\u00e1tt, sum \u00e1\u00f0renn korona rakti samfelagi\u00f0, so eru \u00f3iva\u00f0 n\u00f3gv sum kortini gle\u00f0a seg til hesa t\u00ed\u00f0ina vi\u00f0 hugna og veitslulag Eins og undanfarin \u00e1r, fara R\u00e1\u00f0i\u00f0 fyri Fer\u00f0slutrygd (\u00ed samstarvi vi\u00f0 Betri Trygging og Trygd) at fremja \u00e1tak fyri at ste\u00f0ga r\u00faskoyring. Hetta ver\u00f0ur gj\u00f8rt vi\u00f0 filminum \u00a0\u201dEg koyri teg, t\u00fa koyrir meg\u201d, i\u00f0 er \u00farsliti\u00f0 av st\u00f3ru hugskotskappingini hj\u00e1 R\u00e1\u00f0num fyri Fer\u00f0slutrygd s\u00ed\u00f0sta vetur. Filmsl\u00fdsingin ver\u00f0ur \u00ed\u00a0hesum d\u00f8gum v\u00edst \u00ed sj\u00f3nvarpi, biografi og \u00e1 sosialum mi\u00f0lum. Brynhild Nols\u00f8e \u00ed L\u00e1gab\u00f8 \u00far V\u00e1gi vann kappingina, og luttekur saman vi\u00f0 vinf\u00f3lki \u00ed l\u00fdsingini. Brynhild kennir sj\u00e1lv til avbj\u00f3\u00f0ingarnar av at vera partur av n\u00e1ttarl\u00edvinum \u00ed\u00a0a\u00f0rari bygd, enn teirri t\u00fa b\u00fdrt \u00ed. T\u00ed bygdi hennara hugskot \u00e1 egnar royndir. \u00cd vinarb\u00f3lkinum hj\u00e1 Brynhild hava tey gj\u00f8rt eina avtalu, i\u00f0 byggir \u00e1 tankan: \u201dEg koyri teg, t\u00fa koyrir meg.\u201d Hetta merkir, at tey skiftast um at koyra: - Avtalan er tann, at um eitt vinf\u00f3lk er fari\u00f0 \u00ed b\u00fdin og eg liggi heima, so ringja tey til m\u00edn, og eg fari upp at koyra tey. Um eg eri farin \u00ed b\u00fdin og okkurt vinf\u00f3lk liggur heima,\u00a0so koma tey eisini upp at koyra meg. Ta\u00f0 er l\u00edkamiki\u00f0 um ta\u00f0 er morgun, dagur ella n\u00e1tt, greiddi Brynhild fr\u00e1 \u00ed l\u00fdsingarfilminum, i\u00f0 er komin burtur \u00far hugskotinum hj\u00e1\u00a0Brynhild. Vit valdu at gera eina hugskotskapping, har ung f\u00f3lk sluppu at seta dagsskr\u00e1nna, og \u00farsliti\u00f0 gj\u00f8rdist hesin filmurin, i\u00f0 byggir \u00e1 tey hugskot, i\u00f0 tey ungu sj\u00e1lvi h\u00f8vdu, sigur Lovisa Petersen Glerfoss, stj\u00f3ri \u00ed R\u00e1\u00f0num fyri Fer\u00f0slutrygd. Eftir at vinnarin var\u00f0 funnin, hevur Brynhild arbeitt saman vi\u00f0 eini l\u00fdsingarstovu vi\u00f0 at menna hugskoti\u00f0 til eina lidna l\u00fdsing. \u00cd l\u00fdsingini s\u00edggja vit Brynhild og hennara\u00a0vinf\u00f3lk \u00ed b\u00fdnum og \u00e1 veg til h\u00fas. \u00cd samr\u00e1\u00f0 vi\u00f0 Brynhild er l\u00fdsingin blivin jalig og uppbyggjandi, heldur enn ford\u00f8mandi og neilig. Hugbur\u00f0urin til r\u00faskoyring er broyttur munandi seinastu n\u00f3gvu \u00e1rini, og heili 98% av f\u00f8royingum siga at r\u00faskoyring ver\u00f0ur ikki g\u00f3\u00f0tikin. Men kortini ver\u00f0a bilf\u00f8rarar\u00a0javnan tiknir vi\u00f0 promillu \u00ed bl\u00f3\u00f0inum. Harafturat er r\u00faskoyring ors\u00f8k til fj\u00f3r\u00f0u hv\u00f8rja dey\u00f0svanlukku \u00ed fer\u00f0sluni, v\u00edsa t\u00f8l \u00far nor\u00f0urlondum. T\u00ed er ta\u00f0 eisini \u00ed 2021\u00a0t\u00fddningarmiki\u00f0 at tosa um at ste\u00f0ga r\u00faskoyring! \u00c1taki\u00f0 heldur fram hetta til n\u00fdggj\u00e1rs og l\u00f8greglan ger r\u00faskanningar, me\u00f0an \u00e1taki\u00f0 er. Eisini fer l\u00f8greglan at lata bilf\u00f8rarum, sum hava s\u00edni vi\u00f0urskifti \u00ed ordan, sn\u00f8ggar lyklaringar vi\u00f0 bo\u00f0skapinum \\\"Eg koyri teg, t\u00fa koyrir meg\\\". \",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vestmanna sk\u00fali hevur hesar lei\u00f0reglur \u00ed sambandi vi\u00f0 sj\u00fakar n\u00e6mingar: Ta\u00f0 er \u00f3gvuliga umr\u00e1\u00f0andi at n\u00e6mingar, sum ikki eru koppsettir, og hava veri\u00f0 \u00ed samband vi\u00f0 f\u00f3lk, sum eru testa\u00f0 positiv fyri koronu, halda tilm\u00e6lini. \",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Landsverk arbei\u00f0ur \u00ed l\u00f8tuni vi\u00f0 at f\u00e1a trailaran, sum er fult lasta\u00f0ur, upp aftur, og arbei\u00f0i\u00f0 fer v\u00e6ntandi at taka nakrar t\u00edmar, t\u00ed st\u00f3rar maskinur skulu til, og t\u00e6r mugu koyra um Ei\u00f0iskar\u00f0 fyri at koma til hj\u00e1lpar. \",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Her eru nakrir tekstir flokka\u00f0ir eftir lyndi, sum kann vera 'positivt', 'neutralt' ella 'negativt'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nLyndi: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekstur: {text}\n\nFlokka lyndi\u00f0 \u00ed tekstinum. Svara vi\u00f0 'positivt', 'neutralt' ella 'negativt'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positivt</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutralt</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fosent\n</code></pre>"},{"location":"datasets/faroese/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/faroese/#fone","title":"FoNE","text":"<p>This dataset was published in this paper and is based on news articles from Sosialurin. The named entities were automatically tagged, but verified manually. They use a superset of the CoNNL-2003 dataset, with the following additional entity types: <code>Date</code>, <code>Money</code>, <code>Percent</code> and <code>Time</code>. We remove these additional entity types from our dataset and keep only the original CoNNL-2003 entity types (<code>PER</code>, <code>ORG</code>, <code>LOC</code>, <code>MISC</code>).</p> <p>The original full dataset consists of 6,286 samples, which we split into 1,024 / 256 / 2,048 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Millum', 'teirra', 'er', 'Tommy', 'Petersen', ',', 'sum', 'eitt', 'skifti', 'hev\u00f0i', 'ES', 'sum', 's\u00edtt', 'm\u00e1ls\u00f8ki', '\u00ed', 'Tinganesi', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Fleiri', 'l\u00e6rarat\u00edmar', 'skulu', '\u00ed', '\u00e1r', 'br\u00fakast', '\u00e1', 'HF', '-', 'sk\u00falanum', '\u00ed', 'Klaksv\u00edk', ',', 'men', 'samb\u00e6rt', 'lei\u00f0aranum', '\u00e1', 'sk\u00falanum', 'hevur', 'ta\u00f0', 'bara', 'vi\u00f0', 's\u00e6r', ',', 'at', 'l\u00e6rarar', ',', 'sum', 'eru', 'b\u00fasitandi', '\u00ed', 'Klaksv\u00edk', ',', 'koma', 'at', 'fer\u00f0ast', 'minni', '\u00e1', 'Kambsdal', 'og', '\u00edsta\u00f0in', 'br\u00faka', 'meira', 'undirv\u00edsingart\u00ed\u00f0', '\u00ed', 'b\u00fdnum', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Solei\u00f0is', ',', 'at', 'Starvsstovan', 'kann', 'fylgja', 'vi\u00f0', ',', 'at', 'ta\u00f0', 'ikki', 'er', 'n\u00fdliga', 'heiliv\u00e1gsvi\u00f0gj\u00f8rdur', 'fiskur', ',', 'sum', 'tikin', 'ver\u00f0ur', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar eindir, sum eru \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nNevndar eindir: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', 'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fone\n</code></pre>"},{"location":"datasets/faroese/#unofficial-wikiann-fo","title":"Unofficial: WikiANN-fo","text":"<p>This dataset was part of the WikiANN dataset (also known as PAN-X), published in this paper. It is based on Wikipedia articles, and the labels have been automatically annotated using knowledge base mining. There are no <code>MISC</code> entities in this dataset, so we only keep the <code>PER</code>, <code>LOC</code> and <code>ORG</code> entities.</p> <p>The original full dataset consists of an unknown amount of samples, which we split into 1,024 / 256 / 2,048 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array([\"'\", \"''\", 'P\u00f3lland', \"''\", \"'\"], dtype=object),\n  'labels': array(['O', 'O', 'B-LOC', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Skulu', '\u00farvalssvimjararnir', 'betra', '\u00farslit', 's\u00edni', ',', 'so', 'er', 'ney\u00f0ugt', 'hj\u00e1', 'teimum', 'at', 'fara', 'uttanlands', 'at', 'venja', '(', 'Danmark', ',', 'USA', ')', ';', 'hinvegin', 'minkar', 'hetta', 'um', 'kappingina', 'hj\u00e1', 'teimum', 'heimligu', 'svimjarunum', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Nor\u00f0uramerika', '-', '16', '%'], dtype=object),\n  'labels': array(['B-LOC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar eindir, sum eru \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nNevndar eindir: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', 'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>pers\u00f3nur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0ur</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>felagsskapur</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>ymiskt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset wikiann-fo\n</code></pre>"},{"location":"datasets/faroese/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/faroese/#scala-fo","title":"ScaLA-fo","text":"<p>This dataset was published in this paper and was automatically created from the Faroese Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 1,621 samples, from which we use 1,024 / 256 / 1,024 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hann tala\u00f0i t\u00ed \u00ed samkomuh\u00fasinum vi\u00f0 J\u00f6darnar og vi\u00f0 teir, sum \u00f3tta\u00f0ust Gu\u00f0, og \u00e1 torginum hv\u00f6nn dag vi\u00f0 teir, sum hann har hitti vi\u00f0.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hann finnur fyrst br\u00f3\u00f0ur s\u00edn, S\u00edmun, og sigur vi\u00f0 hann: \\\"hava Vit funni\u00f0 Messias\\\" sum er ta\u00f0 sama sum Kristus; ta\u00f0 er: salva\u00f0ur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hetta hendi tr\u00edggjar fer\u00f0ir, og alt fyri eitt var\u00f0 luturin tikin upp aftur himmals til.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setningur: {text}\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setningur: {text}\n\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um hann ikki er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-fo\n</code></pre>"},{"location":"datasets/faroese/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/faroese/#foqa","title":"FoQA","text":"<p>This dataset will be published in an upcoming paper and is based on the Faroese Wikipedia. The questions and answers were automatically generated using GPT-4-turbo, which were verified by a native speaker, and some of them were also corrected by the same native speaker.</p> <p>The original full dataset consists of 2,000 samples, and we split these into 848 / 128 / 1,024 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'Felagsskapur ST fyri undirv\u00edsing, v\u00edsindum og mentan (\u00e1 enskum: United Nations Educational, Scientific and Cultural Organization, stytt UNESCO) er ein serstovnur undir Sameindu Tj\u00f3\u00f0um, stovna\u00f0ur \u00ed 1946. Endam\u00e1li\u00f0 vi\u00f0 felagskapinum er at menna \u00fatb\u00fagving, gransking og mentan og at fremja samstarv millum tey 195 limalondini og teir 8 atlimirnar, i\u00f0 eru F\u00f8royar, Cura\u00e7ao, Aruba, Jomfr\u00faoyggjar, Caymanoyggjar, Makao, Ni\u00f0urlendsku Antillurnar og Tokelau. F\u00f8royar fingu atlimaskap \u00ed 2009 . Atlimaskapur gevur \u00f8ll tey somu r\u00e6ttindi sum limaskapur. Limalondini skipa seg vi\u00f0 hv\u00f8r s\u00edni UNESCO nevnd. Fyrsta f\u00f8royska UNESCO nevndin var\u00f0 skipa\u00f0 \u00ed mai 2012. \\n\\nUNESCO tekur s\u00e6r millum anna\u00f0 av at meta um, hv\u00f8rji pl\u00e1ss \u00ed heiminum skulu f\u00e1a status sum World Heritage Sites (heimsarvur). Limalond UNESCO samtyktu \u00ed 1972 millumtj\u00f3\u00f0as\u00e1ttm\u00e1lan um at verja heimsins mentanar- og n\u00e1tt\u00faruarv. Ors\u00f8kin er vandin fyri, at n\u00e1tt\u00faru\u00f8ki, fornfr\u00f8\u00f0ilig minnismerki og mentanarvir\u00f0i forfarast orsaka\u00f0 av fer\u00f0af\u00f3lkavinnu, d\u00e1lking, kr\u00edggi ella vanligari \u00f3r\u00f8kt.\\n\\nHygg eisini at \\n\\n Millumtj\u00f3\u00f0as\u00e1ttm\u00e1li UNESCO um vernd av heimsins mentanar- og n\u00e1tt\u00faruarvi.\\n\\nKeldur\\n\\nSl\u00f3\u00f0ir \u00fateftir \\n\\n UNESCO World Heritage Centre\\n\\nST\\nHeimsarvar',\n  'question': 'Hvat g\u00f3\u00f0kendu UNESCO-limalondini \u00ed 1972?',\n  'answers': {\n    'answer_start': array([806]),\n    'text': array(['millumtj\u00f3\u00f0as\u00e1ttm\u00e1lan um at verja heimsins mentanar- og n\u00e1tt\u00faruarv'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Levi Niclasen, sum yrkjari betri kendur sum \u00d3\u00f0in \u00d3dn (f\u00f8ddur 1. mai 1943 \u00e1 Tv\u00f8royri, uppvaksin \u00ed Hvalba) er ein f\u00f8royskur rith\u00f8vundur, t\u00f3nleikari, l\u00e6rari og politikari. \\n\\nAftan \u00e1 barnask\u00falan arbeiddi hann \u00ed kolinum \u00ed Hvalba. \u00cd 1957 stovna\u00f0i hann saman vi\u00f0 br\u00f8\u00f0um s\u00ednum ein t\u00f3nleikab\u00f3lk, og br\u00e1tt blivu teir kendir sum Hvalbiarbr\u00f8\u00f0urnir. Teir g\u00f3vu \u00fat tv\u00e6r stak pl\u00e1tur \u00ed 1962. Hann var \u00ed Gr\u00f8nlandi 1960 og 1961 og arbeiddi \u00e1 landi \u00ed F\u00f8royingahavnini fyri Nordafar. \\nHann f\u00f3r s\u00ed\u00f0an \u00e1 l\u00e6rarask\u00fala \u00ed Havn og t\u00f3k pr\u00f3gv fr\u00e1 F\u00f8roya L\u00e6rarask\u00fala \u00ed 1967. Var settur sum l\u00e6rari vi\u00f0 Hvalbiar sk\u00fala 1. august 1967. Hevur veri\u00f0 sk\u00falalei\u00f0ari vi\u00f0 Hvalbiar sk\u00fala fr\u00e1 1. august 1979. Hann hevur eisini veri\u00f0 \u00e1 Fr\u00f3\u00f0skaparsetri F\u00f8roya og fullf\u00f8rt n\u00e1m \u00ed f\u00f8royskum og b\u00f3kmentum 1969-70. Hann hevur \u00fatgivi\u00f0 fleiri yrkingas\u00f8vn og eisini eitt stutts\u00f8gusavn og eina b\u00f3k vi\u00f0 b\u00e6\u00f0i yrkingum og stutts\u00f8gum. Hann hevur eisini t\u00fdtt tv\u00e6r b\u00f8kur til f\u00f8royskt.\\n\\n\u00datg\u00e1vur  \\nGivi\u00f0 \u00fat \u00e1 egnum forlagi:\\nHvirlur (yrkingasavn) 1970\\nEg eri \u00ed iva (yrkingasavn) 1970 \\nTey \u00ed ur\u00f0ini (s\u00f8gusavn) 1973 \\nRey\u00f0ibarmur (yrkingar og stutts\u00f8gur) 1974\\nVi\u00f0r\u00e1k og M\u00f3tr\u00e1k (yrkingasavn) 1975\\n\u00d3ttast ikki (yrkingasavn) 1975\\nN\u00edvandi ni\u00f0a (yrkingasavn) 1983 \\nLova\u00f0 er lygnin (yrkingasavn) 1983 \\nEg eigi eina mynd (yrkingasavn) 1987\\n\\nT\u00fd\u00f0ingar \\nEydnur\u00edki prinsurin (Oscar Wilde) (F\u00f8roya L\u00e6rarafelag 1977). \\nHeilaga landi\u00f0 (P\u00e4r Lagerkvist) (felagi\u00f0 Var\u00f0in 1986).\\n\\nFamilja \\nForeldur: Thomasia Niclasen, f. Thomasen \u00e1 Giljanesi \u00ed V\u00e1gum og Hentzar Niclasen, kongsb\u00f3ndi \u00e1 Hamri \u00ed Hvalba. Giftist \u00ed 1971 vi\u00f0 S\u00fasonnu Niclasen, f. Holm. Hon er f\u00f8dd \u00ed Hvalba \u00ed 1950. Tey eiga tr\u00edggjar synir: T\u00f3rarinn, T\u00f3roddur og Nj\u00e1lur.\\n\\nKeldur \\n\\nF\u00f8royskir t\u00fd\u00f0arar\\nF\u00f8royskir rith\u00f8vundar\\nF\u00f8royskir yrkjarar\\nF\u00f8royskir l\u00e6rarar\\nHvalbingar\\nF\u00f8\u00f0ingar \u00ed 1943',\n  'question': 'Hvar var Levi Niclasen settur \u00ed starv \u00ed Gr\u00f8nlandi \u00ed 1961?',\n  'answers': {\n    'answer_start': array([431]),\n    'text': array(['F\u00f8royingahavnini'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Giro d'Italia (\u00e1 f\u00f8royskum Kring Italia) er ein av teimum trimum st\u00f3ru teinas\u00fakklukappingunum og ver\u00f0ur hildin hv\u00f8rt \u00e1r \u00ed mai/juni og varir \u00ed 3 vikur. Kappingin fer fram \u00ed Italia, men partar av kappigini kunnu eisini fara fram \u00ed onkrum \u00f8r\u00f0um landi \u00ed Evropa, t.d. byrja\u00f0i Giro d'Italia \u00ed Ni\u00f0urlondum \u00ed 2016 og \u00ed Danmark \u00ed 2014.\\n\\nGiro d'Italia var\u00f0 fyrstu fer\u00f0 hildi\u00f0 \u00ed 1909, har i\u00f0 tilsamans 8 teinar \u00e1 2448\\xa0km v\u00f3ru s\u00fakkla\u00f0ir. Kappingin er saman vi\u00f0 Tour de France og Vuelta a Espa\u00f1a ein av teimum trimum klassisku teinakappingunum, har Tour de France t\u00f3 er tann mest t\u00fd\u00f0andi.\\n\\nHar tann fremsti s\u00fakklarin \u00ed Tour de France er kendur fyri at s\u00fakkla \u00ed gulari troyggju, so s\u00fakklar fremsti s\u00fakklarin \u00ed Giro d\u00b4Italia \u00ed lj\u00f3sarey\u00f0ari troyggju, \u00e1 italskum nevnd Maglia rosa. Tann fremsti fjallas\u00fakklarin s\u00fakklar \u00ed gr\u00f8nari troyggju (Maglia Verde), me\u00f0an s\u00fakklarin vi\u00f0 flestum stigum koyrir \u00ed lilla (Maglia ciclimano). \u00cd 2007 var\u00f0 tann hv\u00edta ungd\u00f3mstroyggjan innf\u00f8rd aftur, eftir at hon hev\u00f0i veri\u00f0 burturi \u00ed n\u00f8kur \u00e1r, hon nevnist Maglia Bianca.\\n\\nTr\u00edggir s\u00fakklarar hava vunni\u00f0 kappingina fimm fer\u00f0ir: Alfredo Binda, Fausto Coppi og Eddy Merckx. Italiuma\u00f0urin Felice Gimondi hevur sta\u00f0i\u00f0 \u00e1 sigurspallinum n\u00edggju fer\u00f0ir, har hann tr\u00edggjar fer\u00f0ir hevur vunni\u00f0, tv\u00e6r fer\u00f0ir \u00e1 \u00f8\u00f0rum pl\u00e1ssi og f\u00fdra fer\u00f0ir \u00e1 tri\u00f0japl\u00e1ssi.\\n\\nYvirlit yvir vinnarar\\n\\nByrjan \u00ed \u00f8\u00f0rum londum\\n\\nKeldur \\n\\nGiro d'Italia\",\n  'question': \"Hv\u00f8r hevur fimm fer\u00f0ir vunni\u00f0 Giro d'Italia?\",\n  'answers': {\n    'answer_start': array([1089]),\n    'text': array(['Alfredo Binda, Fausto Coppi og Eddy Merckx'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Hetta eru tekstir saman vi\u00f0 spurningum og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekstur: {text}\nSpurningur: {question}\nSvara vi\u00f0 \u00ed mesta lagi trimum or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekstur: {text}\n\nSvara hesum spurninginum um tekstin uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\n\nSpurningur: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset foqa\n</code></pre>"},{"location":"datasets/french/","title":"\ud83c\uddeb\ud83c\uddf7 French","text":"<p>This is an overview of all the datasets used in the French part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/french/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/french/#allocine","title":"Allocine","text":"<p>This dataset was published in this Github repository and features reviews from the French movie review website Allocine. The reviews range from 0.5 to 5 (inclusive), with steps of 0.5. The negative samples are reviews with a rating of at most 2, and the positive ones are reviews with a rating of at least 4. The reviews in between were discarded.</p> <p>The original full dataset consists of 160,000 / 20,000 / 20,000 samples for training, validation, and testing, respectively. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Ce 7\u00e8me volet ne m\u00e9rite pas de notre part une grande attention, au vu du pr\u00e9c\u00e9dent New Police Story. \u00c0 la limite du huis clos, Jackie \u00e9volue dans une bo\u00eete de nuit, sorte de pi\u00e8ge du m\u00e9chant cherchant \u00e0 se venger, ou du moins \u00e0 d\u00e9couvrir la v\u00e9rit\u00e9 sur la mort de sa s\u0153ur. Notre cascadeur acteur ne b\u00e9n\u00e9ficie pas d'un d\u00e9cors \u00e0 la hauteur de son potentiel acrobatique et le film d'un sc\u00e9nario \u00e0 la hauteur d'une production, et cette production d'une large distribution, ce qui explique son arriv\u00e9e direct tout \u00e9tag\u00e8re.\",\n  \"label\": \"negative\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Meme pour ceux qui n'aime pas les Chevaliers du Fiel allez voir. 1 il est meilleur que le 1 et cela est rare de voir une suite qui est meilleur que le 1. Des sc\u00e8nes qui peuvent faire rire les petit et les grands. On ne s'ennuie pas. Super film allez le voir. L'interpretation des acteurs sont super. Bonne journ\u00e9e\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Une ambiance envo\u00fbtante, un r\u00e9cit o\u00f9 se m\u00e9langent sorcellerie, croyances indiennes, enqu\u00eate polici\u00e8re sur fond de trafic de drogue, tout est conforme au livre de Tony Hillerman, m\u00eame si ce dernier a \\\"reni\u00e9\\\" le film. Personnellement j'adore. H\u00e9las introuvable en France et diffus\u00e9 seulement sur canal , il y a ..... un certain temps.\",\n  \"label\": \"positive\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Voici des textes et leur sentiment, qui peut \u00eatre 'positif' ou 'n\u00e9gatif'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texte: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texte: {text}\n\nClassez le sentiment dans le texte. R\u00e9pondez par \u2018positif' ou \u2018n\u00e9gatif'.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset allocine\n</code></pre>"},{"location":"datasets/french/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/french/#eltec","title":"ELTeC","text":"<p>This dataset was published in this paper and consists of sentences from 100 novels in French during the period 1840-1920, all of which are in the public domain. These novels were automatically labelled with named entities using Stanza-NER, and then manually corrected.</p> <p>The original dataset consists of 100 samples, one for each novel. We split the novels into sentences using the French NLTK sentence splitter, resulting in 4,815 samples. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003 labelling scheme, which is more common in the NER literature. The mapping is as follows:</p> <ul> <li><code>PERS</code> \u27a1\ufe0f <code>PER</code></li> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>OTHER</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DEMO</code> \u27a1\ufe0f <code>O</code></li> <li><code>ROLE</code> \u27a1\ufe0f <code>O</code></li> <li><code>EVENT</code> \u27a1\ufe0f <code>O</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Jamais', 'ils', 'ne', 'firent', 'de', 'provisions', ',', 'except\u00e9', 'quelques', 'bottes', \"d'ail\", 'ou', \"d'oignons\", 'qui', 'ne', 'craignaient', 'rien', 'et', 'ne', 'co\u00fbtaient', 'pas', \"grand'chose\", ';', 'le', 'peu', 'de', 'bois', \"qu'ils\", 'consommaient', 'en', 'hiver', ',', 'la', 'Sauviat', \"l'achetait\", 'aux', 'fagotteurs', 'qui', 'passaient', ',', 'et', 'au', 'jour', 'le', 'jour', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['I', 'Il', 'y', 'avait', 'plus', 'de', 'soixante', 'ans', 'que', \"l'empereur\", 'Napol\u00e9on', ',', 'press\u00e9', \"d'argent\", ',', 'avait', 'vendu', 'les', 'provinces', 'de', 'la', 'Louisiane', '\u00e0', 'la', 'R\u00e9publique', 'des', '\u00c9tats-Unis', ';', 'mais', ',', 'en', 'd\u00e9pit', 'de', \"l'infiltration\", 'yankee', ',', 'les', 'traditions', 'des', 'cr\u00e9oles', 'fran\u00e7ais', 'se', 'perp\u00e9tuaient', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Les', 'fen\u00eatres', 'de', 'la', 'vieille', 'demeure', 'royale', ',', 'ordinairement', 'si', 'sombres', ',', '\u00e9taient', 'ardemment', '\u00e9clair\u00e9es', ';', 'les', 'places', 'et', 'les', 'rues', 'attenantes', ',', 'habituellement', 'si', 'solitaires', ',', 'd\u00e8s', 'que', 'neuf', 'heures', 'sonnaient', '\u00e0', \"Saint-Germain-l'Auxerrois\", ',', '\u00e9taient', ',', \"quoiqu'il\", 'f\u00fbt', 'minuit', ',', 'encombr\u00e9es', 'de', 'populaire', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Vous trouverez ci-dessous des phrases et des dictionnaires JSON avec les entit\u00e9s nomm\u00e9es qui apparaissent dans la phrase donn\u00e9e.\n</code></pre></li> <li>Base prompt template:   <pre><code>Sentence: {text}\nEntit\u00e9s nomm\u00e9es: {label}\n</code></pre></li> <li> <p>Instruction-tuned prompt template:   <pre><code>Sentence: {text}\n\nIdentifiez les entit\u00e9s nomm\u00e9es dans la phrase. Vous devez produire ceci sous forme de dictionnaire JSON avec les cl\u00e9s 'personne', 'lieu', 'organisation' et 'divers'. Les valeurs doivent \u00eatre des listes des entit\u00e9s nomm\u00e9es de ce type, exactement comme elles apparaissent dans la phrase.\n</code></pre></p> </li> <li> <p>Label mapping:</p> <ul> <li><code>B-PER</code> \u27a1\ufe0f <code>personne</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>personne</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>lieu</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>lieu</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>divers</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>divers</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset eltec\n</code></pre>"},{"location":"datasets/french/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/french/#scala-fr","title":"ScaLA-fr","text":"<p>This dataset was published in this paper and was automatically created from the French Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 16,342 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Le dessert est une part minuscule de g\u00e2teau.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Le trafic international sera normal vendredi sur Eurostar, Thalys, et sur les trains \u00e0 grande vitesse \u00e0 destination de l', a indiqu\u00e9 la SNCF dans un communiqu\u00e9.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Certains craignent qu' un avantage comp\u00e9titif trop net et trop durable favorise les positions dominantes, monopoles et oligopoles, qui limitent la et concurrence finissent par peser sur le consommateur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Les phrases suivantes indiquent si elles sont grammaticalement correctes.\n</code></pre></li> <li>Base prompt template:   <pre><code>Phrase: {text}\nCorrect du point de vue grammatical: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Phrase: {text}\n\nD\u00e9terminez si la phrase est grammaticalement correcte ou non. R\u00e9pondez par 'oui' si la phrase est correcte et par 'non' si elle ne l'est pas, et rien d'autre.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>oui</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>non</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-fr\n</code></pre>"},{"location":"datasets/french/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/french/#fquad","title":"FQuAD","text":"<p>This dataset was published in this paper, and is a manually annotated dataset of questions and answers from the French Wikipedia.</p> <p>The original full dataset consists of 20,731 / 3,188 / 2,189 samples for training, validation and testing, respectively. Note that the testing split is not publicly accessible, however, so we only use the training and validation split. We use 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively. Our training split is a subset of the original training split, and our validation and testing splits are subsets of the original validation split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': \"Parmi leurs th\u00e8mes r\u00e9currents, on en trouve qui sont communs \u00e0 beaucoup d'autres groupes contemporains ou plus anciens : les Stranglers ont d\u00e9crit, \u00e0 plusieurs reprises, la vie d'un groupe de rock dans toutes ses dimensions (fans, autres groupes, vie en tourn\u00e9e). Le th\u00e8me rebattu - chez les groupes des ann\u00e9es 1960-1970 - de la drogue, est abord\u00e9e sur une demi-douzaine de chansons (Don't Bring Harry), tandis que la vision angoiss\u00e9e du futur, dans le contexte de la guerre froide ou en lien avec les avanc\u00e9es de la science, a donn\u00e9 lieu \u00e0 plusieurs titres (Curfew). On retrouve \u00e9galement chez eux des pr\u00e9occupations \u00e9cologiques (Dreamtime) ou sociales. La guerre, notamment les deux guerres mondiales (Northwinds), mais aussi les guerres contemporaines (I Don't Agree), sont \u00e0 l'origine de divers textes. Mais le th\u00e8me qui les a le plus inspir\u00e9s, c'est de loin les femmes (The Man They Love to Hate).\",\n  'question': 'Sur combien de chanson le th\u00e8me de la drogue est il abord\u00e9 ?',\n  'answers': {\n    'answer_start': array([353]),\n    'text': array(['une demi-douzaine'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Au cours de cette p\u00e9riode, Cavour se distingue par son talent de financier. Il contribue de mani\u00e8re pr\u00e9pond\u00e9rante \u00e0 la fusion de la Banque de G\u00eanes et de la nouvelle Banque de Turin au sein de la Banque Nationale des \u00c9tats sardes (Banca Nazionale degli Stati Sardi). Apr\u00e8s le succ\u00e8s \u00e9lectoral de d\u00e9cembre 1849, Cavour devient \u00e9galement une des figures dominantes de la politique pi\u00e9montaise et il prend la fonction de porte-parole de la majorit\u00e9 mod\u00e9r\u00e9e qui vient de se cr\u00e9er. Fort de cette position, il fait valoir que le moment des r\u00e9formes est arriv\u00e9, favoris\u00e9 par le Statut albertin qui a cr\u00e9\u00e9 de r\u00e9elles perspectives de progr\u00e8s. Le Pi\u00e9mont peut ainsi s'\u00e9loigner du front catholique et r\u00e9actionnaire, qui triomphe dans le reste de l'Italie. \",\n  'question': \"En quel ann\u00e9e sort-il vainqueur d'une \u00e9lection ?\",\n  'answers': {\n    'answer_start': array([305]),\n    'text': array(['1849'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': \"Pour autant, le ph\u00e9nom\u00e8ne m\u00e9t\u00e9orologique se d\u00e9cline sous d'autres variantes : ocelles du paon, \u00e9voquant les cent yeux d'Argus, fleurs champ\u00eatres et ornant les jardins o\u00f9 s'\u00e9tablit l'osmose entre couleurs compl\u00e9mentaires. La po\u00e9sie tient en main la palette du peintre,, celle de Claude Gell\u00e9e ou de Poussin. Pour autant, il ne s'agit pas l\u00e0 d'une posture habituelle chez lui, qui privil\u00e9gie les paysages quasi-monochromes.\",\n  'question': \"Qu'est ce que l'auteur pr\u00e9f\u00e8re d\u00e9crire ?\",\n  'answers': {\n    'answer_start': array([394]),\n    'text': array(['paysages'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Les textes suivants sont accompagn\u00e9s de questions et de r\u00e9ponses.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texte: {text}\nQuestion: {question}\nR\u00e9ponse en 3 mots maximum: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texte: {text}\n\nR\u00e9pondez \u00e0 la question suivante sur le texte ci-dessus en 3 mots maximum.\n\nQuestion: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset fquad\n</code></pre>"},{"location":"datasets/french/#knowledge","title":"Knowledge","text":""},{"location":"datasets/french/#mmlu-fr","title":"MMLU-fr","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to French was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En 2013, la part des personnes en Ethiopie qui pensent que les partis politiques sont corrompus est\\nChoix:\\na. 24%\\nb. 44%\\nc. 64%\\nd. 84%\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Combien de nombres entiers positifs et n\u00e9gatifs $12$ est-il un multiple?\\nChoix:\\na. 3\\nb. 12\\nc. 4\\nd. 6\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Quelle affirmation suivante concernant les r\u00e9actions d\u00e9pendantes de la lumi\u00e8re de la photosynth\u00e8se est correcte?\\nChoix:\\na. Ils fournissent le carbone qui est incorpor\u00e9 dans le sucre.\\nb. Ils produisent du PGA, qui est converti en glucose par la fixation du carbone dans les r\u00e9actions ind\u00e9pendantes de la lumi\u00e8re.\\nc. L'eau est s\u00e9par\u00e9e en fournissant des ions hydrog\u00e8ne et des \u00e9lectrons \u00e0 la NADP pour un stockage temporaire.\\nd. Ils se produisent dans le stroma des chloroplastes.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Les questions suivantes sont des questions \u00e0 choix multiples (avec r\u00e9ponses).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_c}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', 'b', 'c' ou 'd', et rien d'autre.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-fr\n</code></pre>"},{"location":"datasets/french/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/french/#hellaswag-fr","title":"HellaSwag-fr","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Comment dire \u00e0 vos enfants que vous allez divorcer [title] Contr\u00f4lez vos \u00e9motions. [step] Vos enfants seront probablement en col\u00e8re et boulevers\u00e9s lorsque vous leur annoncerez le divorce, essayez donc de ne pas r\u00e9agir de la m\u00eame mani\u00e8re. Attendez de rompre la nouvelle lorsque vous pourrez discuter du sujet de mani\u00e8re efficace et rester ma\u00eetre de vos \u00e9motions.\\nChoix:\\na. Rappelez-vous, le but de la discussion est d'\u00eatre l\u00e0 pour les enfants - ils ne devraient pas avoir \u00e0 vous r\u00e9conforter. [title] Essayez de le faire ensemble, si possible.\\nb. [substeps] Trouvez un moyen d'\u00e9viter que vos enfants ne vous agressent verbalement. Assurez-vous d'\u00eatre calme et pos\u00e9 et ne donnez pas l'impression que la nouvelle du divorce est quelque chose qui vous d\u00e9range.\\nc. [substeps] Si vos enfants ont du mal \u00e0 comprendre la nouvelle \u00e0 distance, posez-leur des questions lors d'une conversation intime et priv\u00e9e. Laissez-les utiliser les questions pour traiter et comprendre ce qu'ils ressentent \u00e0 propos de l'annonce.\\nd. [substeps] Si vous ne voulez pas qu'ils le sachent imm\u00e9diatement, partez en silence et r\u00e9fl\u00e9chissez un peu plus longtemps avant de leur dire. Cherchez un endroit confortable pour vous deux pour parler en priv\u00e9, afin que vous puissiez tous deux prendre du temps pour traiter vos sentiments et accepter la situation.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Certains stands servent des hot-dogs aux gens alors qu'ils p\u00eachent sur la glace. Un petit gar\u00e7on et une petite fille tentent d'attraper un poisson. ils\\nChoix:\\na. attrapent un poisson et continuent de nager.\\nb. sont interview\u00e9s pendant qu'ils p\u00eachent.\\nc. essaient \u00e0 plusieurs reprises, errant tout pr\u00e8s de leur poisson.\\nd. sont rapidement emport\u00e9s par le courant alors qu'ils luttent pour s'\u00e9loigner du banc de la rivi\u00e8re et pagayent pour \u00e9chapper \u00e0 de l\u00e9g\u00e8res infestations de poissons dans l'eau\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Comment se calmer [title] Respirer. [step] Respirer. Lentement.\\nChoix:\\na. Concentrez-vous sur votre respiration et d\u00e9tendez votre corps. Continuez \u00e0 inspirer et expirer lentement par le nez, en mettant une pression sur votre diaphragme et vos muscles fessiers (vos poumons).\\nb. Si votre c\u0153ur bat vite ou fort, vous pourriez \u00eatre en danger de tachycardie, d'AVC ou de toute autre crise cardiaque. [title] Allongez-vous sur le dos et inspirez et expirez profond\u00e9ment.\\nc. Inspirez pendant 5 secondes; retenez votre souffle pendant 5 secondes, puis expirez pendant 5 secondes. Cela fonctionne parce que vous faites l'oppos\u00e9 de ce qu'une personne excit\u00e9e ferait.\\nd. Inspirez pendant un compte de cinq et abaissez-vous. Expirez, expirez quatre fois de plus, aussi profond\u00e9ment que vous pouvez sentir, et r\u00e9p\u00e9tez pour un total de dix.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Les questions suivantes sont des questions \u00e0 choix multiples (avec r\u00e9ponses).\n</code></pre></li> <li>Base prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_c}\nR\u00e9ponse: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Question: {text}\nChoix:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', 'b', 'c' ou 'd', et rien d'autre.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-fr\n</code></pre>"},{"location":"datasets/french/#summarization","title":"Summarization","text":""},{"location":"datasets/french/#orange-sum","title":"Orange Sum","text":"<p>This dataset was published in this paper and consists of news articles from Orange Actu. The summaries were written by the journalists themselves (the \"abstract\" field in the original dataset).</p> <p>The original full dataset consists of 21,401 / 1,500 / 1,500 samples for training, validation and testing, respectively. We use 1,024 / 256 / 1,024 samples for training, validation, and testing, respectively. All our splits are subsets of the original ones.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"R\u00e9clam\u00e9 puis annonc\u00e9 par Emmanuel Macron, le d\u00e9bat parlementaire sur l'immigration s'est ouvert ce lundi 7 octobre avec une allocution d'Edouard Philippe devant les d\u00e9put\u00e9s. Le Premier ministre a commenc\u00e9 son discours en empruntant les mots d'un de ses pr\u00e9d\u00e9cesseurs, Michel Rocard. Il a ensuite fait \u00e9tat d'un syst\u00e8me fran\u00e7ais d'asile \\\"satur\u00e9\\\". \\\"En 2018, la France a enregistr\u00e9 le record de 123.000 demandes d'asile\\\", a t-il rappel\u00e9, estimant que la France \\\"n'a pas atteint tous\\\" ses objectifs en mati\u00e8re de politique migratoire et de lutte contre l'immigration irr\u00e9guli\u00e8re. \\\"La question d'un pilotage par objectifs de l'admission au s\u00e9jour n'est pas tabou. Je n'ai pas peur de r\u00e9fl\u00e9chir \u00e0 l'id\u00e9e de quotas. Il nous faut donc regarder sujet apr\u00e8s sujet. On sait depuis longtemps que les quotas ne s'appliquent ni \u00e0 l'asile ni \u00e0 l'immigration familiale. Pour autant, celle-ci ne pourrait \u00e9chapper \u00e0 toute ma\u00eetrise. Il faut lutter contre les abus et les fraudes, et resserrer les crit\u00e8res l\u00e0 o\u00f9 cela s'impose\\\" a t-il poursuivi.Le Premier ministre a en revanche balay\u00e9 l'id\u00e9e de la fin du droit du sol, r\u00e9clam\u00e9e par des \u00e9lus de droite. \\\"Je ne vois pas bien en quoi \u00e0 l'\u00e9chelle du pays, la fin du droit du sol serait une r\u00e9ponse\\\". Il a \u00e9galement adress\u00e9 une critique virulente \u00e0 l'\u00e9gard de la th\u00e9orie de \\\"l'immigration de remplacement\\\", un \\\"vocable d'une laideur certaine qui fait appel aux ressorts les plus d\u00e9testables du complotisme.Ces th\u00e9ories \\\"inspiraient encore r\u00e9cemment des discours dont j'ai eu l'occasion de dire qu'ils \u00e9taient profond\u00e9ment contraires \u00e0 l'id\u00e9e dont nous nous faisons de la France et de la R\u00e9publique\\\" a t-il encore ass\u00e9n\u00e9, en r\u00e9f\u00e9rence \u00e0 la r\u00e9cente \\\"Convention de la droite\\\" organis\u00e9e le 28 septembre dernier autour de Marion Mar\u00e9chal et Eric Zemmour.\",\n  \"target_text\": \"Le Premier ministre a ouvert ce lundi 7 octobre le d\u00e9bat sur l'immigration \u00e0 l'Assembl\u00e9e nationale, d\u00e9clarant que le syst\u00e8me fran\u00e7ais d'asile est aujourd'hui \\\"satur\u00e9\\\". Il a au passage pourfendu la th\u00e9orie de \\\"l'immigration de remplacement\\\", qui fait selon lui appel \\\"aux ressorts les plus d\u00e9testables du complotisme\\\".\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Un supermarch\u00e9 a \u00e9t\u00e9 d\u00e9truit par une explosion, samedi 2 janvier, \u00e0 Grasse, dans les Alpes-Maritimes, a rapport\u00e9 France 3. Aucun bless\u00e9 n'est \u00e0 d\u00e9plorer.L'explosion s'est produite vers 6h du matin dans ce supermarch\u00e9 Aldi de Grasse. Elle a \u00e9t\u00e9 suivie par un violent incendie. Le b\u00e2timent a \u00e9t\u00e9 \\\"totalement d\u00e9truit\\\", selon le maire de la ville, qui a \u00e9voqu\u00e9 une cause \\\"accidentelle\\\" sur sa page Facebook. Une centaine de pompiers, ainsi que des policiers ont \u00e9t\u00e9 mobilis\u00e9s pour lutter contre le sinistre et s\u00e9curiser le p\u00e9rim\u00e8tre.Selon Nice-Matin, deux employ\u00e9es du supermarch\u00e9 ont \u00e9t\u00e9 souffl\u00e9es par l'explosion en allumant la lumi\u00e8re au moment d'arriver sur leur lieu de travail. Aucune des deux n'a \u00e9t\u00e9 bless\u00e9e physiquement, mais elles sont tr\u00e8s choqu\u00e9es.Vers 9h, le feu \u00e9tait ma\u00eetris\u00e9, a indiqu\u00e9 \u00e0 France 3 un porte-parole du Service d'incendie et de secours des Alpes-Maritimes. Soixante pompiers et 40 engins de secours \u00e9taient toujours mobilis\u00e9s sur place.\",\n  \"target_text\": \"Une centaine de pompiers ont \u00e9t\u00e9 mobilis\u00e9s pour lutter contre l'incendie.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Trois ans et demi apr\u00e8s la d\u00e9cision des Britanniques de quitter l'Union europ\u00e9enne, le Brexit est finalement intervenu vendredi 31 janvier. Une mesure qui va s\u00e9rieusement changer la donne pour les Britanniques qui si\u00e8gent aujourd'hui dans les conseils municipaux en France. Comme tous les citoyens europ\u00e9ens, les Britanniques avaient jusqu'\u00e0 pr\u00e9sent le droit de vote et d'\u00e9ligibilit\u00e9 aux \u00e9lections municipales fran\u00e7aises. Actuellement sur 2.493 conseillers \u00e9trangers, 757 viennent du Royaume-Uni, soit environ 30%, selon le R\u00e9pertoire national des \u00e9lus. Ils sont nettement plus nombreux que les Belges (544 \u00e9lus) et les Portugais (357). Ils r\u00e9sident pour la plupart dans un grand quart Sud-Ouest de la France : Charente (70 \u00e9lus), Dordogne (59), Aude (52), Haute-Vienne (40), Lot-et-Garonne (31), H\u00e9rault (30), Deux S\u00e8vres (28), Gers (26), Lot (23)...Or, avec le Brexit, ils ne pourront pas briguer de nouveau mandat, \u00e0 moins d'avoir acquis une autre nationalit\u00e9 europ\u00e9enne depuis les derni\u00e8res \u00e9lections. C'est notamment le cas \u00e0 Poupas, village de 85 habitants dans le Tarn-et-Garonne, o\u00f9 deux des trois conseillers municipaux britanniques, sur les 11 au total que compte la commune, ont obtenu la nationalit\u00e9 fran\u00e7aise. Le droit \\\"de payer et de se taire\\\"Pour certaines petites communes, o\u00f9 il est souvent difficile de trouver des candidats, c'est un vrai casse-t\u00eate. \u00c0 Perriers-en-Beauficel, dans la Manche, Patrick Head , originaire du Wiltshire (sud de l'Angleterre), va ainsi terminer son mandat. Le sexag\u00e9naire avait rafl\u00e9 pas moins de 89,74% des suffrages dans ce petit village normand, o\u00f9 il a \u00e9lu domicile en 2004. Soit le meilleur score de cette commune de 216 habitants, o\u00f9 les \u00e9lecteurs peuvent rayer ou ajouter un nom. \\\"\u00c7a va nous manquer car Patrick nous aidait beaucoup\\\", regrette la maire Lydie Brionne, qui explique que son colistier faisait \\\"le lien\\\" avec la cinquantaine de Britanniques install\u00e9s dans ce coin de campagne normande. \u00c0 Perriers-en-Beauficel, sur les onze \u00e9lus de 2014, deux sont Britanniques. \\\"Il va falloir trouver deux nouveaux candidats. C'est difficile de trouver des gens motiv\u00e9s dans une petite commune\\\", souligne la maire, par ailleurs \u00e9leveuse de vaches laiti\u00e8res. \\\"Depuis 20 ans, beaucoup de Britanniques se sont install\u00e9s, ils ont repeupl\u00e9 la commune, \u00e7a a donn\u00e9 du dynamisme\\\", raconte l'\u00e9lue. Avec le Brexit, \\\"j'ai peur qu'ils soient oblig\u00e9s de repartir.\\\"Loin d'\u00eatre isol\u00e9, le cas de ce village normand se retrouve partout o\u00f9 les Britanniques sont fortement implant\u00e9s. \u00c0 Bellegarde-du-Raz\u00e8s, commune de 240 habitants dans l'Aude, les deux \u00e9lus d'Outre-Manche \\\"apportent une valeur ajout\u00e9e\\\" au village, avec \\\"leur importante implication dans le milieu associatif\\\", estime le maire Gilbert De Paoli. L'\u00c9cossaise Alisson Mackie, 63 ans, install\u00e9e depuis 2011, est d\u00e9pit\u00e9e de ne plus pouvoir se repr\u00e9senter en mars. \\\"On a construit notre maison ici, on paye des imp\u00f4ts ici, on consomme ici mais on a \u00e9t\u00e9 ray\u00e9s des listes \u00e9lectorales\\\", d\u00e9plore-t-elle.\u00c0 Jouac, village de 180 habitants en Haute-Vienne, la maire Virginie Windridge, 39 ans, elle-m\u00eame mari\u00e9e \u00e0 un Britannique, trouve aussi \\\"tr\u00e8s injuste que des gens qui sont l\u00e0 depuis des ann\u00e9es, payent des imp\u00f4ts et contribuent \u00e0 la vie de la commune, aient du jour au lendemain le droit 'de payer et de se taire'\\\". \\\"C'est dur \u00e0 avaler\\\", dit-elle.Les deux \u00e9lus britanniques actuels ont \\\"un apport important\\\", souligne la maire. \\\"D\u00e9j\u00e0 ils sont un relais avec la communaut\u00e9 britannique de la commune. Et puis ils apportent des id\u00e9es diff\u00e9rentes, une autre fa\u00e7on de fonctionner, de voir les choses\\\", d\u00e9crit Mme Windridge. \\\"Ils am\u00e8nent parfois un regard sur ce qui existe ou se fait ailleurs, une autre perspective\\\". \\\"Et, il faut bien le dire, culturellement, quelquefois, les Britanniques sont plus ouverts aux changements que nous, ont un peu moins peur de l'inconnu\\\", ajoute-t-elle en donnant en exemple la d\u00e9cision d'\u00e9teindre l'\u00e9clairage public nocturne. \\\"Les \u00e9lus britanniques \u00e9taient naturellement les plus ouverts sur cette id\u00e9e-l\u00e0, ils voyaient de suite le gagnant-gagnant, pour l'environnement et le budget de la commune\\\", estime-t-elle.\",\n  \"target_text\": \"\u00c0 l'heure actuelle, plus de 750 Britanniques si\u00e8gent dans les conseils municipaux en France. Or, avec la sortie du Royaume-Uni de l'Union europ\u00e9enne, ils ne pourront pas se repr\u00e9senter en mars prochain.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Les articles suivants sont accompagn\u00e9s d'un r\u00e9sum\u00e9.\n</code></pre></li> <li>Base prompt template:   <pre><code>Article de presse: {text}\nR\u00e9sum\u00e9: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Article de presse: {text}\n\nR\u00e9digez un r\u00e9sum\u00e9 de l'article ci-dessus.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset orange-sum\n</code></pre>"},{"location":"datasets/german/","title":"\ud83c\udde9\ud83c\uddea German","text":"<p>This is an overview of all the datasets used in the German part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/german/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/german/#sb10k","title":"SB10k","text":"<p>This dataset was published in this paper and is based on German tweets, which were manually annotated by three annotators.</p> <p>The original full dataset consists of 1,840 / 324 / 870 samples, and we use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. The splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"ALEMANHA (4-5-1): Neuer; Schmelzer, Hummels, Mertesacker, Lahm; G\u00fcndogan, Khedira, \u00d6zil, M\u00fcller, Reus; Klose\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@user ok. Bin jetzt dann hernach gleich nochmal weg, aber schreib ruhig.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"@user Schw\u00fcle 34\u00b0, Tendenz steigend. #schrecklich\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die 'positiv', 'neutral' oder 'negativ' sein kann.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tweet: {text}\nStimmungslage: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tweet: {text}\n\nKlassifizieren Sie die Stimmung im Tweet. Antworten Sie mit 'positiv', 'neutral' oder 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset sb10k\n</code></pre>"},{"location":"datasets/german/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/german/#germeval","title":"GermEval","text":"<p>This dataset was published in this paper and is based on German Wikipedia as well as news articles, and was manually annotated. It roughly follows the CoNLL-2003 format, but also allows overlapping entities and derived entities (such as \"English\" for \"England\"). We remove the derived entities and convert the partially overlapping entities to non-overlapping entities (e.g., <code>B-ORGpart</code> to <code>B-ORG</code>).</p> <p>The original full dataset consists of 24,000 / 2,200 / 5,100 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training,</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Am', 'Ende', 'der', 'Saison', '2006/07', 'soll', 'es', 'f\u00fcr', 'die', 'L\u00f6wen', 'wieder', 'zu', 'einem', 'Europapokal-Platz', 'reichen', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-LOC', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['In', 'einer', 'Stichwahl', 'gegen', 'seinen', 'Vorg\u00e4nger', 'Georg', 'Kronawitter', 'wurde', 'Erich', 'Kiesl', 'am', '1.', 'April', '1984', 'abgew\u00e4hlt', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Noch', 'im', '13.', 'Jahrhundert', 'wurde', 'sie', 'in', 'manchen', 'Handschriften', 'mit', 'der', 'Christherre-Chronik', 'verschmolzen', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Satz: {text}\nBenannte Entit\u00e4ten: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Satz: {text}\n\nIdentifizieren Sie die benannten Entit\u00e4ten im Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', 'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>ort</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>ort</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>verschiedenes</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>verschiedenes</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset germeval\n</code></pre>"},{"location":"datasets/german/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/german/#scala-de","title":"ScaLA-de","text":"<p>This dataset was published in this paper and was automatically created from the German Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 15,590 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Im In dem Sommer drau\u00dfen zu sitzen ist immer wieder eine \\\"Wonne\\\", so man noch einen Platz bekommt\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Eine 65 m lange Betonmauer tr\u00e4gt nachts einen Leucht - Schriftzug \\\"HOSTAL HOSTILE HOTEL HOSTAGE GOSTIN OSTILE HOSTEL HOSTIL HOST\\\", was in seinem etymologischen Wortspiel so viel bedeutet, dass aus einem feindlichen ein gastfreundlicher Ort geworden ist, in Anspielung auf das auf dem Gel\u00e4nde des ehemaligen Frauenlagers genau gegen\u00fcber liegende Novotel Goldene Bremm (heute Mercure Saarbr\u00fccken - S\u00fcd), das konzeptionell insoweit in die Idee einbezogen ist.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Allerdings wurde nachgewiesen, dass sich der ebenfalls in Extremlebensr\u00e4umen vorkommende Nematode Halicephalobus mephisto im in dem Labor bevorzugt Desulforudis audaxviator ern\u00e4hrt, wenn er eine Wahl hat (Alternative: E. coli).\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\n</code></pre></li> <li>Base prompt template:   <pre><code>Satz: {text}\nGrammatikalisch richtig: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Satz: {text}\n\nBestimmen Sie, ob der Satz grammatikalisch korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und 'nein', wenn er es nicht ist.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nein</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-de\n</code></pre>"},{"location":"datasets/german/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/german/#germanquad","title":"GermanQuAD","text":"<p>This dataset was published in this paper and is based on German Wikipedia articles, and was manually annotated.</p> <p>The original full dataset consists of 11,518 / 2,204 samples for training and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': \"Mali\\n\\n=== Verwaltungsgliederung ===\\nDer Staat gliedert sich in zehn Regionen und den Hauptstadtdistrikt. Diese teilen sich in 49 Kreise ''(cercles)'' und 703 Gemeinden ''(communes)''. Die Regionen sind nach ihren Hauptst\u00e4dten benannt. Zwei dieser zehn Regionen, M\u00e9naka und Taoud\u00e9nit, wurden 2012 per Gesetzesbeschluss gebildet. Die Einrichtung ist seit 2016 im Gange.\\nDie Angaben der Regionen Gao und Timbuktu, aus denen die Regionen M\u00e9naka und Taoud\u00e9nit ausgegliedert wurden, spiegeln noch den Stand vor der Aufspaltung wider.\\nUm auch Fl\u00fcchtlinge und vor allem Nomaden in das Verwaltungssystem eingliedern zu k\u00f6nnen, entstanden sogenannte ''Fractions'' (''Fractions Nomades'', ein Begriff, den schon die Kolonialregierung nutzte), die es dementsprechend vor allem im Norden in der N\u00e4he von D\u00f6rfern gibt. Seit den gro\u00dfen Trockenphasen entstanden durch Wanderungsbewegungen solche Verwaltungseinheiten allerdings auch verst\u00e4rkt im S\u00fcden.\",\n  'question': 'Wie viele verschiedene Regionen hat Mali? ',\n  'answers': {\n    'answer_start': array([63], dtype=int32),\n    'text': array(['zehn Regionen und den Hauptstadtdistrikt'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Iran\\n\\n=== Automobilindustrie ===\\nIn der Automobilindustrie waren 2010 rund 500.000 Menschen besch\u00e4ftigt, damit ist die Branche der zweitgr\u00f6\u00dfte Arbeitgeber nach der \u00d6lindustrie und der Iran der gr\u00f6\u00dfte Automobilproduzent im Mittleren Osten. 2012 ist die Automobilproduktion des Iran jedoch scharf eingebrochen; es wurden nur noch 989.110 Fahrzeuge produziert \u2013 40 Prozent weniger als 2011. Darunter fallen 848.000 PKW und 141.110 Nutzfahrzeuge.\\nDie beiden gr\u00f6\u00dften Automobilhersteller sind die staatliche SAIPA \u2013 derzeit im Privatisierungsprozess \u2013 und Iran Khodro (IKCO). Die IKCO produziert neben einheimischen Modellen wie Dena und Runna in Lizenz Modelle u.\\xa0a. von Peugeot. SAIPA hat die IKCO im Jahr 2010 das erste Mal in der Rangfolge \u00fcberholt. Nach Ansicht des Business Monitor International\u2019s Iran Autos Report wird sich die Belastbarkeit der iranischen Automobilindustrie erst in den n\u00e4chsten Jahren zeigen, wenn der einheimische Markt ges\u00e4ttigt ist und der Iran zunehmend auf dem internationalen Markt agiert, denn bisher ist der Produktionsanstieg noch \u00fcberwiegend auf die Unterst\u00fctzung der Regierung zur\u00fcckzuf\u00fchren. 12,64 % der zugelassenen Kraftfahrzeuge werden mit Gas betrieben. Der Iran liegt damit weltweit an f\u00fcnfter Stelle der Nutzung von gasbetriebenen Kraftfahrzeugen.\\nDer schwedische LKW-Produzent Scania er\u00f6ffnete 2011 eine neue Produktionslinie in Qazvin und l\u00f6st damit Daimler-Chrysler ab, das seine Gesch\u00e4ftskontakte mit dem Iran abgebrochen hat.',\n  'question': 'Wie hei\u00dfen die Automodelle von Iran Khodro?',\n  'answers': {\n    'answer_start': array([622], dtype=int32),\n    'text': array([' Dena und Runna'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Griechenland\\n\\n=== Klima ===\\nGriechenland hat \u00fcberwiegend ein mediterranes Klima mit feucht-milden Wintern und trocken-hei\u00dfen Sommern. An der K\u00fcste ist es im Winter sehr mild und es regnet h\u00e4ufig; Schnee f\u00e4llt nur selten. Die Sommer sind relativ hei\u00df und es gibt nur gelegentlich Sommergewitter. Mit 48\u00b0 wurde 1977 in Griechenland der kontinentaleurop\u00e4ische Hitzerekord gemessen.\\nIm Landesinneren ist es vor allem im Winter deutlich k\u00fchler und es gibt h\u00e4ufig Nachtfrost, manchmal auch starke Schneef\u00e4lle. Der Fr\u00fchling ist kurz, verw\u00f6hnt aber \u201emit einem Feuerwerk aus Lavendel und Anemonen, Klatschmohn und Kamille\u201c. Im Sommer ist es \u00e4hnlich wie an der K\u00fcste hei\u00df und trocken. Die j\u00e4hrlichen Niederschl\u00e4ge schwanken zwischen 400 und 1000\\xa0mm. Da Griechenland sehr gebirgig ist, ist Wintersport durchaus m\u00f6glich, es existieren 19 Wintersportgebiete unterschiedlicher Gr\u00f6\u00dfe. Ein kleiner Teil im Nordwesten des Festlandes liegt in der gem\u00e4\u00dfigten Klimazone.',\n  'question': 'Wie oft schneit es in Griechenland?',\n  'answers': {\n    'answer_start': array([209], dtype=int32),\n    'text': array(['nur selten'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und Antworten.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nFragen: {question}\nFragen Antwort in maximal 3 W\u00f6rtern: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nBeantworten Sie die folgende Frage zum obigen Text in h\u00f6chstens 3 W\u00f6rtern.\n\nFrage: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset germanquad\n</code></pre>"},{"location":"datasets/german/#knowledge","title":"Knowledge","text":""},{"location":"datasets/german/#mmlu-de","title":"MMLU-de","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to German was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Teotihuac\u00e1n wurde im Becken von Mexiko bekannt, nachdem sein Rivale Cuicuilco,\\nAntwortm\u00f6glichkeiten:\\na. von einem Vulkanausbruch gel\u00e4hmt wurde.\\nb. einem B\u00fcrgerkrieg unter seinen herrschenden Familien erlag.\\nc. unter einer Ernteplage litt.\\nd. von einem Hurrikan an der Golfk\u00fcste \u00fcberschwemmt wurde.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wer von den folgenden ist der industrielle Philanthrop?\\nAntwortm\u00f6glichkeiten:\\na. Frederick Taylor\\nb. Seebohm Rowntree\\nc. Henry Ford\\nd. Max Weber\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Verglichen mit der Varianz der Maximum-Likelihood-Sch\u00e4tzung (MLE) ist die Varianz der Maximum-A-Posteriori (MAP)-Sch\u00e4tzung ________\\nAntwortm\u00f6glichkeiten:\\na. h\u00f6her\\nb. gleich\\nc. niedriger\\nd. es kann jede der obigen Optionen sein\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-de\n</code></pre>"},{"location":"datasets/german/#unofficial-arc-de","title":"Unofficial: ARC-de","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to German was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Callahan zitiert die Ergebnisse des Oregon Death with Dignity Legal Defense and Education Center, wonach es \\\"nach vier vollen Jahren keine Missteps, Missbr\u00e4uche oder Zwangstendenzen\\\" bez\u00fcglich der Gesetze zur Euthanasie gab. Er argumentiert dagegen, dass\\nAntwortm\u00f6glichkeiten:\\na. sie dies ohne eine anonyme Umfrage nicht sicher wissen k\u00f6nnen.\\nb. andere Studien haben widerspr\u00fcchliche Ergebnisse gefunden.\\nc. selbst wenn das Ergebnis wahr ist, ist es irrelevant f\u00fcr den moralischen Status der Euthanasie.\\nd. die Ergebnisse sind verd\u00e4chtig, weil die Studie von Bef\u00fcrwortern der Euthanasie durchgef\u00fchrt wurde.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>  \"text\": \"Eine Frau besa\u00df ein Land im absoluten Besitz. Die Frau \u00fcbertrug das Land an einen Freund \u201cauf Lebenszeit\u201d und als der Freund starb, sollte das Land an den Nachbarn der Frau \\\"und ihre Erben\\\" weitergegeben werden. Der Nachbar starb und in ihrem ordnungsgem\u00e4\u00df beglaubigten Testament vermachte sie ihre gesamte Hinterlassenschaft an eine \u00f6rtliche Wohlt\u00e4tigkeitsorganisation. Wenn sie intestat gestorben w\u00e4re, w\u00e4re ihre Tochter ihre einzige Erbin gewesen. Ein Jahr nach dem Tod des Nachbarn \u00fcbertrug dessen Tochter durch eine Quitclaims-Erkl\u00e4rung jegliches Interesse, das sie an dem Land haben k\u00f6nnte, an die Freundin der Frau. Das Common Law Rule Against Perpetuities ist in der Gerichtsbarkeit unver\u00e4ndert. Es gibt keine weiteren anwendbaren Gesetze. Wer hat welches Interesse an dem Land?\\nAntwortm\u00f6glichkeiten:\\na. Der Freund hat einen absoluten Besitz, weil sein Lebensbesitz mit dem verbleibenden Teil verschmolzen ist, den ihm die Tochter \u00fcbertragen hat.\\nb. Der Freund hat einen Lebensbesitz und die Wohlt\u00e4tigkeitsorganisation hat einen unverfallbaren Rest, weil das Interesse des Nachbarn \u00fcbertragbar war.\\nc. Der Freund hat einen Lebensbesitz und die Tochter hat einen unverfallbaren Rest, weil die Erkl\u00e4rung der Frau ein Interesse an den Erben des Nachbarn schuf.\\nd. Der Freund hat einen Lebensbesitz und die Frau hat einen R\u00fcckfall, weil das Restinteresse des Nachbarn gem\u00e4\u00df dem Rule Against Perpetuities ung\u00fcltig war.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"F\u00fcr welches dieser beiden Szenarien tut der Hauptcharakter (der ich/mich/mein verwendet) etwas, das nach den gew\u00f6hnlichen moralischen Standards in den USA im Jahr 2020 eindeutig moralisch falsch ist? Szenario 1 | Ich gehe gerne in die Kirche, weil es mich in h\u00f6chstem Ma\u00dfe Gott nahe f\u00fchlen l\u00e4sst. Szenario 2 | Ich habe den Test meines Nachbarn angesehen, um eine gute Note zu bekommen.\\nAntwortm\u00f6glichkeiten:\\na. Falsch, Falsch\\nb. Falsch, Nicht falsch\\nc. Nicht falsch, Falsch\\nd. Nicht falsch, Nicht falsch\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-de\n</code></pre>"},{"location":"datasets/german/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/german/#hellaswag-de","title":"HellaSwag-de","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Wie man sich trennt, wenn Kinder involviert sind [title] Erstellen Sie einen Trennungsplan mit Ihrem Partner. [step] Sie sollten sich auch auf das Gespr\u00e4ch mit Ihren Kindern vorbereiten, indem Sie vorher mit Ihrem Partner einen Plan f\u00fcr die Zukunft erstellen. Sie sollten gemeinsam besprechen, wer wo leben wird, wer f\u00fcr bestimmte t\u00e4gliche Bed\u00fcrfnisse und Aktivit\u00e4ten der Kinder verantwortlich sein wird und wann der offizielle Scheidungsprozess beginnen wird.\\nAntwortm\u00f6glichkeiten:\\na. Indem Sie hier\u00fcber klare Vorstellungen haben, k\u00f6nnen Sie Ihre Kinder besser beruhigen und einheitlich auftreten. [substeps] Zum Beispiel, k\u00f6nnten Sie vereinbaren, dass Ihr Partner auszieht und in einer nahegelegenen Wohnung oder einem anderen Haus lebt.\\nb. Sie beide sollten Ihre Aktionen in den Monaten bis zur Eheschlie\u00dfung sowie dar\u00fcber, wie Sie alles tun werden, planen, sobald das Kind wieder mit seinem Vater vereint ist. [title] Entscheiden Sie, was Sie mit dem Kind machen werden.\\nc. Stellen Sie sicher, dass Ihr Partner einverstanden ist und zustimmt, immer Pausen zu machen. [substeps] Sie sollten sich nun auf die Urlaubsdaten und Reisepl\u00e4ne einigen, zu denen Ihre Kinder gehen werden.\\nd. Der erste Schritt zu diesem Plan ist, ein Telefongespr\u00e4ch zu vereinbaren, damit Sie mit Ihrem Partner pers\u00f6nlich sprechen k\u00f6nnen. Sprechen Sie ruhig und deutlich, um den Ton f\u00fcr dieses Gespr\u00e4ch zu setzen.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Wie man Festival-Make-up macht [title] Bereiten Sie Ihr Gesicht vor. [step] Bevor Sie Ihr Augen-Make-up auftragen, m\u00fcssen Sie eine Basis schaffen. Dies hilft sicherzustellen, dass Ihr Augen-Make-up den ganzen Tag h\u00e4lt.\\nAntwortm\u00f6glichkeiten:\\na. [substeps] Zeichnen Sie eine runde, quadratische oder diagonale Linie um Ihr Auge. Verfolgen Sie den Kreis um Ihr Auge und ziehen Sie dann einen rechteckigen Streifen in der Mitte.\\nb. [substeps] Beginnen Sie mit einem sauberen, mit Feuchtigkeit versorgten Gesicht. Reinigen Sie Ihr Gesicht zun\u00e4chst mit einem sanften Reinigungsmittel und tragen Sie dann einen leichten Feuchtigkeitsspender auf Ihr Gesicht und Ihren Hals auf, um das Erscheinungsbild feiner Linien zu reduzieren.\\nc. Bevor Sie Lidschatten auftragen, w\u00e4hlen Sie einen einzelnen Lidschatten aus und messen Sie ihn so aus, dass er etwas gr\u00f6\u00dfer ist als das Auge, das Sie verblenden m\u00f6chten. Tragen Sie den Lidschatten auf die Spitze jedes Auges auf und streichen Sie mit einem Verblendpinsel dar\u00fcber.\\nd. Make-up am fr\u00fchen Morgen zu tragen ist nicht immer eine Option, aber Sie k\u00f6nnen es am Abend tun. [substeps] Duschen Sie, um Ihre Haut sauber und mit Feuchtigkeit versorgt zu halten.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Wir sehen einen Mann in einem Orchester Grimassen schneiden. Der Mann steht dann auf und spielt die Violine. Wir sehen Menschen an Spinden. wir\\nAntwortm\u00f6glichkeiten:\\na. sehen Menschen in einem Bus.\\nb. sehen Menschen beim \u00dcben von Kampfsport und Musik spielen.\\nc. kehren zum Mann zur\u00fcck, der die Violine spielt.\\nd. sehen den Mann am Keyboard wieder.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\n</code></pre></li> <li>Base prompt template:   <pre><code>Frage: {text}\nAntwort: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frage: {text}\nAntwortm\u00f6glichkeiten:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBeantworten Sie die obige Frage mit 'a', 'b', 'c' oder 'd', und nichts anderes.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-de\n</code></pre>"},{"location":"datasets/german/#summarization","title":"Summarization","text":""},{"location":"datasets/german/#mlsum","title":"MLSum","text":"<p>This dataset was published in this paper and features news articles and their summaries in five languages, including German. The German part of the dataset is based on news articles from S\u00fcddeutsche Zeitung, with human-written summaries.</p> <p>The original full dataset consists of 221,000 / 11,400 / 10,700 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Jede neue Schlagzeile ein Stich ins Herz: F\u00fchrende Muslime beklagen in einem offenen Brief die wachsende \\\"Feindseligkeit\\\" gegen Migranten in Deutschland. Sie fordern Bundespr\u00e4sident Wulff auf, Stellung zu beziehen. In einem offenen Brief haben 15 namhafte deutsche Muslime Bundespr\u00e4sident Christian Wulff aufgefordert, in der schwelenden Debatte um Integrationsprobleme Stellung zu beziehen. Ausl\u00f6ser der Kontroverse war das Buch Deutschland schafft sich ab des SPD-Politikers und scheidenden Bundesbankvorstandes Thilo Sarrazin. Detailansicht \u00f6ffnen In der von SPD-Politiker und Noch-Bundesbanker Thilo Sarrazin ausgel\u00f6sten Integrationsdebatte fordern namhafte deutsche Muslime nun von Bundespr\u00e4sident Christian Wulff, Stellung zu beziehen. (Foto: dpa) Intellektuelle wie der Regisseur Fatih Akin und der Schriftsteller Feridun Zaimoglu beklagten in dem in der taz ver\u00f6ffentlichten Brief wachsende \\\"Feindseligkeit\\\" gegen Muslime in Deutschland. W\u00f6rtlich hei\u00dft es: \\\"F\u00fcr Musliminnen und Muslime ist derzeit nicht einmal der Gang zum Zeitungsh\u00e4ndler leicht, weil sie nie wissen, welche Schlagzeile, welches stereotype Bild sie dort erwartet.\\\" Die Unterzeichner erinnerten Wulff an seine Antrittsrede, in der er die Chancen der Integration betont hatte. \\\"Wir bitten Sie, gerade in der derzeitigen angespannten Stimmung f\u00fcr die Leits\u00e4tze einer offenen, von gegenseitigem Respekt gepr\u00e4gten demokratischen Kultur einzustehen und \u00f6ffentlich f\u00fcr sie zu werben\\\", hei\u00dft es in dem Appell an Wulff. Ausl\u00f6ser f\u00fcr den offenen Brief sei der Aufruf der Bild-Zeitung gewesen, an Pr\u00e4sident Wulff zu schreiben, sagte Shermin Langhoff, Intendantin des Berliner Theaters Ballhaus Naunynstra\u00dfe. \\\"Wir dachten uns, das k\u00f6nnen wir nicht so stehen lassen\\\", sagte die Mitunterzeichnerin zur SZ. Sie sprach von \\\"biologistischen Wahnthesen\\\" Sarrazins und hofft auf ein \\\"Wort der Vernunft\\\" aus Bellevue. Auch andere Unterzeichnerinnen setzen darauf, dass sich das Staatsoberhaupt in die Debatte einschaltet. Aylin Selcuk, Initiatorin des Vereins Deukische Generation, w\u00fcnscht sich ein starkes Zeichen Wulffs. Der Pr\u00e4sident m\u00f6ge zeigen, dass die Muslime in Deutschland dazugeh\u00f6ren. \\\"Wir bitten Sie: Bekennen Sie sich zu uns.\\\" Lamya Kaddor vom Liberal-Islamischen Bund sprach von einem \\\"\u00f6ffentlichen Bekenntnis\\\" des Pr\u00e4sidenten. In der laufenden Debatte gehe es nicht nur um Muslime, sondern um den \\\"Zusammenhalt in der Gesellschaft\\\", warnte Selcuk. Die Studentin hatte Sarrazin nach seinen \u00c4u\u00dferungen zur vererbten Intelligenz wegen Volksverhetzung angezeigt. Seitdem erreichten sie unz\u00e4hlige E-Mails, in denen sie geschm\u00e4ht und bedroht werde, sagte Selcuk. Nun hofft sie auf Wulff. \\\"Wir werden dieses Land nicht aufgeben\\\", hei\u00dft es in dem Brief an Christian Wulff. \\\"Dieses Land ist unsere Heimat und Sie sind unser Pr\u00e4sident.\\\"\",\n  \"target_text\": \"Jede neue Schlagzeile ein Stich ins Herz: F\u00fchrende Muslime beklagen in einem offenen Brief die wachsende \\\"Feindseligkeit\\\" gegen Migranten in Deutschland. Sie fordern Bundespr\u00e4sident Wulff auf, Stellung zu beziehen.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hoch flog der erste Schl\u00e4ger in die Luft, und viele andere Gegenst\u00e4nde folgten ihm. \u00dcberall auf dem Eis lag die Ausr\u00fcstung der deutschen Mannschaft zerstreut, Handschuhe, Helme, Schl\u00e4ger, weg damit, wer braucht so etwas schon, wenn er hemmungslos jubeln kann? In einer Ecke des Eises versammelten sich die Spieler der deutschen Eishockey-Mannschaft. Sie h\u00fcpften und tanzten und schrien, und wenn es nicht zu den Gepflogenheiten des Sports z\u00e4hlen w\u00fcrde, irgendwann zum H\u00e4ndesch\u00fctteln mit dem Gegner in der Mitte des Feldes zu erscheinen, dann h\u00e4tten sie wahrscheinlich noch eine ganze Weile so weitergemacht. Es war nun wirklich ein sporthistorischer Moment, den das Team des Deutschen Eishockey-Bundes (DEB) dort zelebrierte. Mit 4:3 (1:0, 3:1, 0:2) hatte es in einem ph\u00e4nomenalen Spiel den Rekord-Olympiasieger Kanada bezwungen und sich damit f\u00fcr das Finale des Turniers gegen die Olympischen Athleten aus Russland (5.10 Uhr MEZ) qualifiziert. Zum ersten Mal \u00fcberhaupt kann eine deutsche Mannschaft Olympiasieger werden, es ist der gr\u00f6\u00dfte Erfolg in der Geschichte des deutschen Eishockeys. \\\"Verr\u00fcckt, ne, verr\u00fcckt, verr\u00fcckte Welt\\\", sagte Bundestrainer Marco Sturm: \\\"Das ist einmalig.\\\" Ein ohnehin schon irres Turnier kulminiert in diesem 4:3 im Halbfinale Ja, einmalig war es in der Tat, was seine Mannschaft da geleistete hatte. Und es war interessant mitzuerleben, wie nach dem Spiel ein Akteur nach dem anderen in die Kabine trottete und sich unterwegs kurz den Journalisten stellte. Da war etwa der Torwart Danny aus den Birken, der v\u00f6llig ausgelaugt war. Oder Defensivspieler Moritz M\u00fcller, der seine Tr\u00e4nen kaum halten konnte. Oder die NHL-gest\u00e4hlten Routiniers Christian Erhoff und Marcel Goc, die schon so viel erlebt haben, aber so etwas wie an diesem Abend dann doch noch nicht. Keiner hatte schon so recht begriffen, was da geschehen war, und keiner wollte zu gro\u00dfen sportfachlichen Analysen ansetzen, als es um die Gr\u00fcnde f\u00fcr den Erfolg ging. Ein jeder sagte nur: Team. Mannschaft. Teamgeist. Mannschaftsgeist. Diese W\u00f6rter fallen oft im Sport, aber soweit sich das von au\u00dfen beurteilen l\u00e4sst, trifft das bei den Eishockey-Spielern tats\u00e4chlich zu. Sturm hat in den drei Jahren eine bemerkenswerte Mannschaft geformt, die ohnehin ein irres Turnier spielt. Das knappe 0:1 gegen Schweden in der Vorrunde, der Penalty-Sieg \u00fcber Norwegen, der Erfolg nach Verl\u00e4ngerung gegen die Schweiz, das denkw\u00fcrdige 4:3 gegen Schweden im Viertelfinale. Aber all das kulminierte jetzt in diesem 4:3 gegen Kanada im Halbfinale. In einem \\\"Jahrhundertspiel\\\", wie Alfons H\u00f6rmann, Pr\u00e4sident des Deutschen Olympischen Sportbundes, nicht ganz zu Unrecht schw\u00e4rmte.\",\n  \"target_text\": \"Nach dem sensationellen 4:3-Sieg gegen Kanada kann das deutsche Eishockey-Team erstmals Olympiasieger werden. Im Finale ist der Gegner der Favorit - doch die Mannschaft von Marco Sturm glaubt an sich.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Monatelang haben Sicherheitsbeh\u00f6rden nach Salah Abdeslam gefahndet. Jetzt ist der 26-j\u00e4hrige Terrorverd\u00e4chtige festgenommen worden. Er soll an den Anschl\u00e4gen von Paris beteiligt gewesen sein, bei denen am 13. November drei Killerkommandos 130 Menschen get\u00f6tet hatten. Was man bisher \u00fcber den Mann wei\u00df Salah Abdeslam ist in Br\u00fcssel geboren, aber franz\u00f6sischer Staatsb\u00fcrger. Er ist der Bruder des Selbstmordattent\u00e4ters Brahim, der ebenfalls bei den Anschl\u00e4gen dabei war. Die verst\u00fcmmelte Leiche des 31-j\u00e4hrigen Brahim Abdeslam hatte die Polizei am Tag des Anschlags am Boulevard Voltaire in der N\u00e4he des Konzertsaals Bataclan gefunden, wo er sich in die Luft gesprengt hatte. Salah wohnte im Br\u00fcsseler Vorort Molenbeek, der als eine Hochburg von gewaltbereiten Islamisten in Belgien gilt. Abdeslam soll in Deutschland gewesen sein Laut Recherchen des SWR soll sich Abdeslam Anfang Oktober 2015 kurzzeitig in Baden-W\u00fcrttemberg aufgehalten und dort wom\u00f6glich Komplizen abgeholt haben. Demnach fuhr er in der Nacht vom 2. auf den 3. Oktober 2015 mit einem auf seinen Namen angemieteten Wagen nach Ulm und offenbar nach etwa einer Stunde wieder zur\u00fcck. Er k\u00f6nnte in Ulm laut SWR drei M\u00e4nner, die sich als Syrer ausgegeben hatten, aus einer Fl\u00fcchtlingsunterkunft abgeholt haben. Bei einer Anwesenheitskontrolle am 3. Oktober wurde festgestellt, dass die drei M\u00e4nner in der Unterkunft fehlten. Ihre Identit\u00e4t werde vom Bundeskriminalamt gemeinsam mit franz\u00f6sischen und belgischen Sicherheitsbeh\u00f6rden gepr\u00fcft, hie\u00df es. Die deutschen Beh\u00f6rden wollten sich nicht zu dem Vorgang \u00e4u\u00dfern. Familie bat ihn, sich zu stellen Wie andere Islamisten auch ist Abdeslam im Br\u00fcsseler Stadtteil Molenbeek aufgewachsen. Er war der Polizei wegen Drogendelikten bekannt. Seinen Job als Mechaniker verlor er 2011 wegen h\u00e4ufiger Abwesenheit. Ab 2013 betrieb er eine Bar in Molenbeek, die schlie\u00dflich von den Beh\u00f6rden geschlossen wurde, weil G\u00e4ste dort Drogen genommen haben sollen. Mit Abdelhamid Abaaoud, der die Anschl\u00e4ge von Paris vermutlich geplant hat, war Salah Abdeslam seit seiner Kindheit befreundet. Nach den Anschl\u00e4gen in Frankreich wurde er per internationalem Haftbefehl gesucht. Fahnder beschrieben ihn als \\\"gef\u00e4hrlich\\\" und m\u00f6glicherweise \\\"schwer bewaffnet\\\". Zwischenzeitlich war auch \u00fcber einen Aufenthalt in Syrien spekuliert worden. Salahs Bruder Mohamed hatte in Fernsehinterviews an den Gesuchten appelliert, sich zu stellen. Er selbst war nach den Anschl\u00e4gen kurzzeitig festgenommen, aber bald wieder freigelassen worden. Seine Anw\u00e4ltin sagte, er habe \\\"nicht das gleiche Leben gew\u00e4hlt\\\" wie seine Br\u00fcder. Mohamed berichtete, dass Brahim und Salah in den Monaten vor den Anschl\u00e4gen im November in Paris ges\u00fcnder gelebt, gebetet, keinen Alkohol mehr getrunken h\u00e4tten und hin und wieder in die Moschee gegangen seien. Er wollte darin aber \\\"nicht direkt ein Zeichen f\u00fcr Radikalisierung\\\" sehen. Zur Rolle seines Bruders bei den Anschl\u00e4gen in Paris sagte Mohamed: \\\"Salah ist sehr intelligent. Er hat in letzter Minute kehrtgemacht\\\". Salah sollte angeblich in Paris auch ein Selbstmordattentat ver\u00fcben. Er z\u00fcndete die Bombe aber nicht, sondern warf seinen Sprengstoffg\u00fcrtel in einem Pariser Vorort in einen M\u00fclleimer.\",\n  \"target_text\": \"Dort soll der Terrorist drei Komplizen aus einer Fl\u00fcchtlingsunterkunft abgeholt haben. Die belgischen Beh\u00f6rden haben den 26-J\u00e4hrigen jetzt wegen Mordes angeklagt.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen Zusammenfassungen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nachrichtenartikel: {text}\nZusammenfassung: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nachrichtenartikel: {text}\n\nSchreiben Sie eine Zusammenfassung des obigen Artikels.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mlsum\n</code></pre>"},{"location":"datasets/icelandic/","title":"\ud83c\uddee\ud83c\uddf8 Icelandic","text":"<p>This is an overview of all the datasets used in the Icelandic part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/icelandic/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/icelandic/#hotter-and-colder-sentiment","title":"Hotter and Colder Sentiment","text":"<p>This dataset is being published in an upcoming paper, and consists of texts from Icelandic blog post, annotated with sentiment labels (and many others) via a crowdsourcing platform.</p> <p>The original full dataset consists of 2,901 samples, and we use a 1,024 / 256 / 1,621 split for training, validation and testing, respectively (so all samples are used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Til hamingju me\u00f0 gott framtak. \u00deetta eru g\u00f3\u00f0ir \u00fatgangspunktar me\u00f0 stj\u00f3rnarskr\u00e1na, \u00fe\u00f3 margt fleira \u00feurfi a\u00f0 laga svo h\u00fan \u00fej\u00f3ni vel\u00a0 n\u00fdju l\u00fd\u00f0veldi framt\u00ed\u00f0arinnar.\u00c9g sty\u00f0 heils hugar \u00feetta framtak ykkar.\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"J\u00fa, j\u00fa, au\u00f0vita \u00e1 hann ekki a\u00f0 vera\u00a0samstarfsma\u00f0ur e\u00f0a einu sinni \u00ed sama h\u00fasi og s\u00e9rstakir r\u00edkissaks\u00f3knarar \u00ed \u00feessu m\u00e1li. S\u00e9rstakir r\u00edkissaks\u00f3knarar fyrir \u00feetta m\u00e1l\u00a0eiga a\u00f0 liggja\u00a0liggja beint undir r\u00e1\u00f0uneytinu og vera algerlega sj\u00e1lfst\u00e6\u00f0ir, \\\"untouchables\\\". \u00c9g hef ekki enn s\u00e9\u00f0 nein r\u00f6k fyrir \u00fev\u00ed a\u00f0\u00a0Valt\u00fdr \u00feurfi a\u00f0 v\u00edkja \u00far s\u00ednu starfi ef \u00feessi lei\u00f0 ver\u00f0ur valin? Best v\u00e6ri ef s\u00e9rstakir r\u00edkissaks\u00f3knarar \u00ed \u00feessu m\u00e1li v\u00e6ri \u00ferepinu h\u00e6rri \u00ed valdastiganum en Valt\u00fdr, ef \u00fea\u00f0 er h\u00e6gt a\u00f0 koma \u00fev\u00ed \u00ed gegn me\u00f0 sn\u00f6ggum lagabreytingum? Varla er \u00feetta Stj\u00f3rnarskr\u00e1rm\u00e1l?\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Meira a\u00f0 segja h\u00f6r\u00f0ustu klappst\u00fdrur \u00de\u00f3r\u00f3lfs hlj\u00f3ta a\u00f0 hugsa, \u00fe\u00f3 ekki v\u00e6ri \u00ed nema augnablik: Miki\u00f0 er skr\u00fdti\u00f0 a\u00f0 hann s\u00e9 ekki me\u00f0 \u00e1 hreinu af hverju f\u00e1ir handleggir eru a\u00f0 bj\u00f3\u00f0a sig \u00ed \u00feri\u00f0ju sprautuna!Annars er bara sama handriti\u00f0 a\u00f0 fara spilast aftur: N\u00fa er hausti\u00f0 komi\u00f0 og \u00e1rst\u00ed\u00f0arbundnar pestir munu rj\u00faka upp, allar sem ein, og \u00fe\u00e1 ver\u00f0ur skellt \u00ed l\u00e1s og tala\u00f0 um a\u00f0 hafa opna\u00f0 of snemma.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Yfirfer\u00f0: {text}\nLyndi: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nFlokka\u00f0u tilfinninguna \u00ed textanum. Svara\u00f0u me\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>j\u00e1kv\u00e6tt</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>hlutlaust</code></li> <li><code>negative</code> \u27a1\ufe0f <code>neikv\u00e6tt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hotter-and-colder-sentiment\n</code></pre>"},{"location":"datasets/icelandic/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/icelandic/#mim-gold-ner","title":"MIM-GOLD-NER","text":"<p>This dataset was published in this paper and is based on the Tagged Icelandic Corpus (MIM), which consists of Icelandic books, news articles, periodicals, parliament speeches, legal texts, adjudications and government websites. It has been annotated with named entities in a semi-automated fashion, where each labels has been manually verified. The entity types in the dataset is a superset of the CoNLL-2003 tags, with the following additional labels: <code>DATE</code>, <code>TIME</code>, <code>MONEY</code>, <code>PERCENT</code>. These labels have been removed.</p> <p>The original full dataset consists of 1,000,000 tokens. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'tokens': array(['Sj\u00e1lfsagt', 'er', 'a\u00f0', 'mi\u00f0a', 'endurgrei\u00f0sluna', 'ver\u00f0i', 'n\u00faverandi', 'heimild', 'framlengd', 'vi\u00f0', 'EUROIII', '\u00ed', 'sta\u00f0', 'EUROII', 'eins', 'og', 'n\u00fa', 'er', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['\u00dea\u00f0', 'var', 'br\u00f3\u00f0ir', 'Sandlers', 'sem', 'hvatti', 'hann', 'til', 'a\u00f0', 'leggja', 'gr\u00edni\u00f0', 'fyrir', 'sig', '\u00feegar', 'hann', 'var', '17', '\u00e1ra', 'a\u00f0', 'aldri', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['2.-', 'Erla', 'Gu\u00f0n\u00fd', 'Gylfad.', ',', 'Smyrill', 'fr\u00e1', 'Stokkh\u00f3lma', ',', '7,01', '.'], dtype=object),\n  'labels': array(['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-LOC', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum sem koma fyrir \u00ed setningunum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nNefndar einingar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', 'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>einstaklingur</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>einstaklingur</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sta\u00f0setning</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sta\u00f0setning</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>stofnun</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>stofnun</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>\u00fdmislegt</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>\u00fdmislegt</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mim-gold-ner\n</code></pre>"},{"location":"datasets/icelandic/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/icelandic/#scala-is","title":"ScaLA-is","text":"<p>This dataset was published in this paper and was automatically created from the Icelandic Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 3,535 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Utanrrh.: \u00c9g hef \u00c9g hef\u00f0i \u00f3ska\u00f0 \u00feess a\u00f0 h\u00e6stv. utanr\u00edkisr\u00e1\u00f0herra hef\u00f0i meiri \u00e1hrif \u00e1 fors\u00e6tisr\u00e1\u00f0herra en raun ber vitni Gripi\u00f0 fram \u00ed. \u00fev\u00ed a\u00f0 hann er sem betur fer ekki a\u00f0 tala ni\u00f0ur \u00fe\u00e1 atvinnugrein sem tengist sj\u00e1var\u00fatveginum eins og h\u00e6stv. fors\u00e6tisr\u00e1\u00f0herra gerir alla jafna.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00dea\u00f0 v\u00e6ri mun sk\u00e1rra, \u00fea\u00f0 hef\u00f0i veri\u00f0 h\u00e6gt a\u00f0 gera \u00fea\u00f0 meiri me\u00f0 s\u00e1tt, en \u00fea\u00f0 var einfaldlega ekki gert.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mig l\u00edka a\u00f0 koma a\u00f0, \u00e9g gleymdi \u00fev\u00ed \u00e1\u00f0an og kom \u00fev\u00ed heldur ekki a\u00f0, komugj\u00f6ldunum eins og \u00feau heita v\u00edst n\u00fana, ekki legugj\u00f6ld lengur.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-iceec","title":"Unofficial: IceEC","text":"<p>This dataset was published here and consists of texts in modern Icelandic from student essays, online news texts and Wikipedia articles, annotated for mistakes related to spelling, grammar, and other issues.</p> <p>The original full dataset consists of 58,200 / 5,270 samples for training and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, where the training and testing splits are subsets of the original training and testing splits, and the validation split is a disjoint subset of the training split.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Kannski erum vi\u00f0 me\u00f0 meiri s\u00f6lu \u00ed \u00f6\u00f0rum skrokkhlutum en s\u00ed\u00f0um t.d., \u201c segir Stein\u00fe\u00f3r.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00de\u00f3 svo a\u00f0 hann s\u00e9 lei\u00f0inlegur og ekkert t\u00edvol\u00ed gaman, \u00fe\u00e1 er mi\u00f0lar hann \u00feekkingu til okkar og \u00e1n hans mundi enginn menntun vera.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"S\u00edminn er hvers manns \u00e1byrg\u00f0.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset ice-ec\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-icelinguistic","title":"Unofficial: IceLinguistic","text":"<p>This dataset was published here, with the source of the documents unknown. It consists of Icelandic sentences annotated with whether they are grammatically correct or not (along with other linguistic properties).</p> <p>The original full dataset consists of 382 samples, and we use a 94 / 32 / 256 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"\u00c9g afla\u00f0i uppl\u00fdsinganna og \u00fe\u00fa peninganna.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Af hverju f\u00f3r \u00fe\u00fa ekki heim?\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00de\u00fa bor\u00f0a\u00f0ir k\u00f6kuna og \u00e9g kleinuhringurinn.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er ekki.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>j\u00e1</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset ice-linguistic\n</code></pre>"},{"location":"datasets/icelandic/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/icelandic/#nqii","title":"NQiI","text":"<p>This dataset was published in this paper and is based on articles from the Icelandic Wikipedia. Annotators were asked to write both questions (only seeing the beginning of the article) as well as answers as they appear in the article.</p> <p>The original full dataset consists of 2,234 / 259 / 244 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively. Our splits are new, and there can thus be some overlap between the new test split and the old training and validation splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': 'Gr\u00f3\u00f0urh\u00fasalofttegund er lofttegund , \u00ed lofthj\u00fapi sem drekkur \u00ed sig og gefur fr\u00e1 s\u00e9r innrau\u00f0a geislun . \u00dea\u00f0 ferli er a\u00f0al \u00e1st\u00e6\u00f0a gr\u00f3\u00f0urh\u00fasa\u00e1hrifa . Helstu gr\u00f3\u00f0urh\u00fasalofttegundirnar \u00ed lofthj\u00fapi jar\u00f0ar eru vatnsgufa , kold\u00edox\u00ed\u00f0 , metan , tv\u00edk\u00f6fnunarefnisox\u00ed\u00f0 og \u00f3son . \u00c1n gr\u00f3\u00f0urh\u00fasalofttegunda v\u00e6ri me\u00f0alhiti yfirbor\u00f0s jar\u00f0ar \u2212 18 \u00b0 C , n\u00faverandi me\u00f0altals 15 \u00b0 C . \u00cd s\u00f3lkerfinu , eru Venus , Mars og T\u00edtan einnig me\u00f0 lofthj\u00fap sem veldur gr\u00f3\u00f0urh\u00fasa\u00e1hrifum .',\n  'question': 'Hverjar eru gr\u00f3\u00f0urh\u00fasalofttegundirnar ?',\n  'answers': {\n    'answer_start': array([202], dtype=int32),\n    'text': array([' vatnsgufa , kold\u00edox\u00ed\u00f0 , metan , tv\u00edk\u00f6fnunarefnisox\u00ed\u00f0 og \u00f3son'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Hvannadalshn\u00fakur e\u00f0a Hvannadalshnj\u00fakur er h\u00e6sti tindur eldkeilunnar undir \u00d6r\u00e6faj\u00f6kli og jafnframt h\u00e6sti tindur \u00cdslands . Samkv\u00e6mt n\u00fdjustu m\u00e6lingu er h\u00e6\u00f0 hans 2.109,6 metrar yfir sj\u00e1varm\u00e1li . Tindurinn er sta\u00f0settur innan Vatnaj\u00f6kuls\u00fej\u00f3\u00f0gar\u00f0s og er vins\u00e6ll hj\u00e1 fjallg\u00f6nguf\u00f3lki , reyndu sem og \u00f3reyndu . Tindurinn er ekki fl\u00f3kinn uppg\u00f6ngu og \u00fearfnast ekki mikillar reynslu e\u00f0a t\u00e6kni \u00ed fjallg\u00f6ngum , gangan krefst samt mikils \u00fathalds \u00fear sem oftast er gengi\u00f0 \u00e1 tindinn og ni\u00f0ur aftur \u00e1 sama deginum . H\u00e6kkunin er r\u00famir 2000 metrar , gangan tekur oftast 12 - 14 klst \u00ed heild .',\n  'question': 'Hvert er h\u00e6sta fjall \u00e1 \u00cdslandi ?',\n  'answers': {\n    'answer_start': array([20,  0, 20], dtype=int32),\n    'text': array([' Hvannadalshnj\u00fakur', 'Hvannadalshn\u00fakur', ' Hvannadalshnj\u00fakur er h\u00e6sti tindur eldkeilunnar undir \u00d6r\u00e6faj\u00f6kli og jafnframt h\u00e6sti tindur \u00cdslands'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Falklandseyjar er l\u00edtill eyjaklasi \u00fat af Su\u00f0ur-Amer\u00edku , um 500 km til su\u00f0austurs fr\u00e1 Argent\u00ednu . \u00de\u00e6r eru undir stj\u00f3rn Bretlands en Argent\u00edna hefur einnig gert tilkall til \u00feeirra og olli \u00fea\u00f0 Falklandseyjastr\u00ed\u00f0inu milli \u00fej\u00f3\u00f0anna 1982 .',\n  'question': 'Hvar eru Falklandseyjar ?',\n  'answers': {\n    'answer_start': array([34, 34], dtype=int32),\n    'text': array([' \u00fat af Su\u00f0ur-Amer\u00edku', ' \u00fat af Su\u00f0ur-Amer\u00edku , um 500 km til su\u00f0austurs fr\u00e1 Argent\u00ednu'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texti: {text}\nSpurning: {question}\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 h\u00e1marki \u00ed 3 or\u00f0um.\n\nSpurning: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset nqii\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-icelandicqa","title":"Unofficial: IcelandicQA","text":"<p>This dataset was published here and consists of an automatically created Icelandic question-answering dataset based on the Icelandic Wikipedia as well as Icelandic news articles from the R\u00daV corpus.</p> <p>Both questions and answers were generated automatically, meaning that the answers might not appear in the context. To remedy this, we used GPT-4o to rephrase the answers to ensure that they appear in the context.</p> <p>The original full dataset consists of 2,000 samples, and we use a 531 / 128 / 1,024 split for training, validation and testing, respectively. These are all the samples where the (rephrased) answer appears in the context.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  'context': '\u00d3mar Ragnarsson - Syngur fyrir b\u00f6rnin  er 33 sn\u00faninga LP hlj\u00f3mplata gefin \u00fat af SG - hlj\u00f3mpl\u00f6tum \u00e1ri\u00f0 1981. \u00c1 henni syngur \u00d3mar Ragnarsson \u00ferett\u00e1n barnal\u00f6g. Platan er safnplata af \u00e1\u00f0ur \u00fatgefnum \"hit\" l\u00f6gum af 45 sn\u00faninga pl\u00f6tum.\\n\\nLagalisti \\n \u00c9g er a\u00f0 baka - Lag - texti: E. Shuman/B. Bower - \u00d3mar Ragnarsson\\n Br\u00f3\u00f0ir minn - Lag - texti: W. Holt -\u00d3mar Ragnarsson\\n Eitthva\u00f0 \u00fat \u00ed lofti\u00f0 - Lag - texti: P. McCartney - \u00d3mar Ragnarsson \\n Lok, lok og l\u00e6s - Lag - texti: Brezkt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n Aha, sei-sei, j\u00e1-j\u00e1 - Lag - texti: \u00d3mar Ragnarsson\\n Ligga, ligga l\u00e1 - Lag - texti: \u00d3mar Ragnarsson \\n Hl\u00e1turinn lengir l\u00edfi\u00f0 - Lag - texti: Ortega - \u00d3mar Ragnarsson\\n Sumar og s\u00f3l - Lag - texti: \u00d3mar Ragnarsson\\n J\u00f3i \u00fatherji - Lag - texti: \u00c1stralskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n \u00d3li drj\u00f3li - Lag - texti: \u00d3mar Ragnarsson)\\n Minkurinn \u00ed h\u00e6nsnakofanum - Lag - texti: Norskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson \\n Kenni\u00f0 m\u00e9r krakkar - Lag - texti: A. Johansen - \u00d3mar Ragnarsson\\n H\u00ed \u00e1 \u00feig - Lag - texti: Amer\u00edskt \u00fej\u00f3\u00f0lag - \u00d3mar Ragnarsson\\n\\nSG-hlj\u00f3mpl\u00f6tur\\nHlj\u00f3mpl\u00f6tur gefnar \u00fat \u00e1ri\u00f0 1981\\n\u00d3mar Ragnarsson',\n  'question': 'Hva\u00f0a \u00e1r var LP-hlj\u00f3mplatan \u201e\u00d3mar Ragnarsson - Syngur fyrir b\u00f6rnin\u201c gefin \u00fat?',\n  'answers': {\n    'answer_start': 102,\n    'text': array(['1981'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Tj\u00f6rn er kirkjusta\u00f0ur \u00ed Dalv\u00edkurbygg\u00f0 \u00ed Svarfa\u00f0ardal. B\u00e6rinn stendur a\u00f0 vestanver\u00f0u \u00ed dalnum um 5 km innan vi\u00f0 Dalv\u00edk. \u00de\u00f3rarinn Kr. Eldj\u00e1rn l\u00e9t reisa n\u00faverandi \u00edb\u00fa\u00f0arh\u00fas 1931. Tjarnartj\u00f6rn er l\u00edti\u00f0 og grunnt st\u00f6\u00f0uvatn \u00e1 flatlendinu ne\u00f0an vi\u00f0 b\u00e6inn. Tj\u00f6rnin er innan Fri\u00f0lands Svarfd\u00e6la sem teygir sig allt til strandar. \u00dear er miki\u00f0 fuglal\u00edf. Tj\u00f6rn er me\u00f0 st\u00e6rri j\u00f6r\u00f0um \u00ed Svarfa\u00f0ardal og a\u00f0 l\u00edkindum landn\u00e1msj\u00f6r\u00f0 \u00fe\u00f3tt b\u00e6jarins s\u00e9 ekki geti\u00f0 \u00ed Landn\u00e1mu. \u00dear hafa veri\u00f0 stunda\u00f0ar \u00farkomum\u00e6lingar \u00e1 vegum Ve\u00f0urstofunnar fr\u00e1 \u00e1rinu 1970. \u00cd hl\u00ed\u00f0inni ofan vi\u00f0 Tj\u00f6rn eru volgrur og \u00ed framhaldi af \u00feeim er jar\u00f0hitinn \u00ed Laugahl\u00ed\u00f0 \u00fear sem Sundsk\u00e1li Svarfd\u00e6la f\u00e6r vatn sitt.\\nKristj\u00e1n Eldj\u00e1rn forseti f\u00e6ddist \u00e1 Tj\u00f6rn 1916 og \u00f3lst \u00fear upp.\\nS\u00f6ngh\u00f3purinn Tjarnarkvartettinn var kenndur vi\u00f0 Tj\u00f6rn \u00ed Svarfa\u00f0ardal.\\n\\nTjarnarb\u00e6ndur \u00e1 20. \u00f6ld:\\n Sr. Kristj\u00e1n Eldj\u00e1rn \u00de\u00f3rarinsson og Petr\u00edna Soff\u00eda Hj\u00f6rleifsd\u00f3ttir\\n \u00de\u00f3rarinn Kr. Eldj\u00e1rn og Sigr\u00fan Sigurhjartard\u00f3ttir\\n Hj\u00f6rtur Eldj\u00e1rn \u00de\u00f3rarinsson og Sigr\u00ed\u00f0ur Hafsta\u00f0\\n Kristj\u00e1n Eldj\u00e1rn Hjartarson og Kristjana Arngr\u00edmsd\u00f3ttir\\n\\nTjarnarkirkja \\n\\nKirkja hefur l\u00edklega veri\u00f0 reist \u00e1 Tj\u00f6rn flj\u00f3tlega eftir a\u00f0 kristni var l\u00f6gleidd \u00ed landinu. Hennar er \u00fe\u00f3 ekki geti\u00f0 me\u00f0 beinum h\u00e6tti \u00ed heimildum fyrr en \u00ed Au\u00f0unarm\u00e1ldaga fr\u00e1 1318. \u00dear segir a\u00f0 kirkjan s\u00e9 helgu\u00f0 Mar\u00edu gu\u00f0sm\u00f3\u00f0ur, Mikj\u00e1li erkiengli, J\u00f3hannesi sk\u00edrara og Andr\u00e9si postula. Kirkjan \u00e1tti \u00fe\u00e1 h\u00e1lft heimalandi\u00f0, Ingvarasta\u00f0aland og h\u00f3lminn \u00d6rgumlei\u00f0a. \u00c1 16. \u00f6ld er Tj\u00f6rn or\u00f0in beneficium, \u00fe.e. \u00f6ll komin \u00ed eigu kirkjunnar og \u00feannig h\u00e9lst \u00fear til sr. Kristj\u00e1n Eldj\u00e1rn \u00de\u00f3rarinsson (1843-1917) keypti j\u00f6r\u00f0ina \u00e1ri\u00f0 1915. Sr. Kristj\u00e1n var s\u00ed\u00f0asti prestur \u00e1 Tj\u00f6rn. \u00cd Svarfa\u00f0ardal voru lengi fj\u00f3rar s\u00f3knir en \u00fer\u00edr prestar \u00fev\u00ed Ur\u00f0akirkja var annex\u00eda fr\u00e1 Tj\u00f6rn. Upsas\u00f3kn var s\u00ed\u00f0an l\u00f6g\u00f0 undir Tjarnarprest 1859 en 1917 var Tjarnarprestakall me\u00f0 s\u00ednum \u00feremur s\u00f3knum sameina\u00f0 Vallaprestakalli. Eftir a\u00f0 prestssetri\u00f0 var flutt fr\u00e1 V\u00f6llum 1969 hefur Tjarnarkirkju veri\u00f0 \u00fej\u00f3na\u00f0 af fr\u00e1 Dalv\u00edk. Tjarnars\u00f3kn n\u00e6r fr\u00e1 Steindyrum a\u00f0 Ytraholti.\\n\\nN\u00faverandi kirkja var reist 1892. H\u00fan er \u00far timbri \u00e1 hl\u00f6\u00f0num grunni og tekur 60-70 manns \u00ed s\u00e6ti. \u00cd henni eru steindir gluggar teikna\u00f0ir af Valger\u00f0i Hafsta\u00f0 listm\u00e1lara. Kirkjugar\u00f0ur er umhverfis kirkjuna. Kirkjan skemmdist nokku\u00f0 \u00ed Kirkjurokinu svokalla\u00f0a, miklu \u00f3ve\u00f0ri sem gekk yfir landi\u00f0 \u00feann 20. september \u00e1ri\u00f0 1900. \u00de\u00e1 ey\u00f0il\u00f6g\u00f0ust kirkjurnar \u00e1 Ur\u00f0um og Upsum og Vallakirkja var\u00f0 fyrir skemmdum. Tjarnarkirkja snara\u00f0ist \u00e1 grunni s\u00ednum og halla\u00f0ist mj\u00f6g til nor\u00f0urs en j\u00e1rnkr\u00f3kar miklir, sem h\u00e9ldu timburverkinu vi\u00f0 hla\u00f0inn grunninn, v\u00f6rnu\u00f0u \u00fev\u00ed a\u00f0 verr f\u00e6ri. Nokkru eftir f\u00e1rvi\u00f0ri\u00f0 ger\u00f0i hvassvi\u00f0ri af nor\u00f0ri sem f\u00e6r\u00f0i hana til \u00e1 grunninum og r\u00e9tti hana a\u00f0 mestu vi\u00f0 \u00e1 n\u00fd. M\u00f6rgum \u00fe\u00f3ttu \u00feetta st\u00f3rmerki. Gert var vi\u00f0 kirkjuna eftir \u00feetta og m.a. voru \u00fatb\u00fain \u00e1 hana j\u00e1rnst\u00f6g sem lengi settu skemmtilegan svip \u00e1 bygginguna og minntu \u00e1 hi\u00f0 mikla f\u00e1rvi\u00f0ri sem h\u00fan haf\u00f0i sta\u00f0i\u00f0 af s\u00e9r. Kirkjan st\u00f3\u00f0 einnig af s\u00e9r Dalv\u00edkurskj\u00e1lftann 1934 en \u00fe\u00f3 ur\u00f0u skemmdir \u00e1 grunni hennar.\\n\\nHeimildir \\n \\n \\n Kirkjur \u00cdslands 9. bindi. Tjarnarkirkja bls. 271-307. Reykjav\u00edk 2007\\n\\nTenglar\\nTjarnarkirkja \u00e1 kirkjukort.net \\n\\n\u00cdslenskir sveitab\u00e6ir\\nKirkjusta\u00f0ir \u00ed Eyjafjar\u00f0ars\u00fdslu\\nKirkjur \u00e1 \u00cdslandi\\nSvarfa\u00f0ardalur',\n  'question': '\u00c1 hva\u00f0a b\u00e6 \u00ed Svarfa\u00f0ardal hafa veri\u00f0 stunda\u00f0ar \u00farkomum\u00e6lingar \u00e1 vegum Ve\u00f0urstofunnar fr\u00e1 \u00e1rinu 1970?',\n  'answers': {\n    'answer_start': 0,\n    'text': array(['Tj\u00f6rn'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  'context': 'Fyrir greinina um \u00fe\u00e1ttinn sem er \u00ed gangi \u00ed dag, sj\u00e1 Kastlj\u00f3s (d\u00e6gurm\u00e1la\u00fe\u00e1ttur)\\nKastlj\u00f3s var fr\u00e9ttask\u00fdringa\u00fe\u00e1ttur sem var \u00e1 dagskr\u00e1 R\u00edkis\u00fatvarpsins fr\u00e1 1974 til 1998. Hann h\u00f3f g\u00f6ngu s\u00edna sem fr\u00e9ttask\u00fdringa\u00fe\u00e1ttur um innlendar fr\u00e9ttir \u00e1ri\u00f0 1974 og t\u00f3k \u00fe\u00e1 vi\u00f0 af \u00fe\u00e6tti sem nefndist Landshorn. \u00de\u00e1tturinn var um fj\u00f6rut\u00edu m\u00edn\u00fatna langur, \u00ed umsj\u00f3n fr\u00e9ttastofunnar og s\u00fdndur \u00e1 f\u00f6stud\u00f6gum \u00e1 besta t\u00edma. Umsj\u00f3narmenn voru mismunandi fr\u00e9ttamenn \u00ed hvert skipti. Annar \u00fe\u00e1ttur \u00e1 mi\u00f0vikud\u00f6gum fjalla\u00f0i \u00fe\u00e1 um erlendar fr\u00e9ttir. 1980 var \u00fe\u00e1ttunum tveimur slegi\u00f0 saman \u00ed eitt Kastlj\u00f3s \u00e1 f\u00f6stud\u00f6gum \u00ed umsj\u00f3n tveggja stj\u00f3rnenda. 1987 var \u00fe\u00e6ttinum aftur breytt \u00ed fr\u00e9ttask\u00fdringa\u00fe\u00e1tt um innlend m\u00e1lefni stutt skei\u00f0. 1988 h\u00e9t \u00fe\u00e1tturinn Kastlj\u00f3s \u00e1 sunnudegi og 1990 Kastlj\u00f3s \u00e1 \u00feri\u00f0judegi eftir breyttum \u00fatsendingart\u00edma en 1992 var \u00fe\u00e1tturinn aftur fluttur \u00e1 besta t\u00edma \u00e1 f\u00f6studegi. 1993 var Kastlj\u00f3s teki\u00f0 af dagskr\u00e1 um skei\u00f0 \u00feegar d\u00e6gurm\u00e1la\u00fe\u00e1tturinn Dagslj\u00f3s h\u00f3f g\u00f6ngu s\u00edna. \\n\\n\u00cdslenskir sj\u00f3nvarps\u00fe\u00e6ttir',\n  'question': '\u00c1 hva\u00f0a \u00e1rum var fr\u00e9ttask\u00fdringa\u00fe\u00e1tturinn Kastlj\u00f3s upphaflega \u00e1 dagskr\u00e1 R\u00edkis\u00fatvarpsins?',\n  'answers': {\n    'answer_start': 147,\n    'text': array(['Fr\u00e1 1974 til 1998'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Texti: {text}\nSpurning: {question}\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 or\u00f0um: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Texti: {text}\n\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 h\u00e1marki \u00ed 3 or\u00f0um.\n\nSpurning: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset icelandic-qa\n</code></pre>"},{"location":"datasets/icelandic/#knowledge","title":"Knowledge","text":""},{"location":"datasets/icelandic/#arc-is","title":"ARC-is","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The dataset was translated by Mi\u00f0eind using the Claude 3.5 Sonnet model.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"L\u00edkamar manna hafa fl\u00f3kna uppbyggingu sem sty\u00f0ur v\u00f6xt og l\u00edfsl\u00edkur. Hver er grundvallaruppbygging l\u00edkamans sem stu\u00f0lar a\u00f0 vexti og l\u00edfsl\u00edkum?\\nSvarm\u00f6guleikar:\\na. fruma\\nb. vefur\\nc. l\u00edff\u00e6ri\\nd. l\u00edff\u00e6rakerfi\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ve\u00f0urfr\u00e6\u00f0ingur skr\u00e1ir g\u00f6gn fyrir borg \u00e1 \u00e1kve\u00f0num degi. G\u00f6gnin innihalda hitastig, sk\u00fdjahulu, vindhra\u00f0a, loft\u00fer\u00fdsting og vind\u00e1tt. Hva\u00f0a a\u00f0fer\u00f0 \u00e6tti ve\u00f0urfr\u00e6\u00f0ingurinn a\u00f0 nota til a\u00f0 skr\u00e1 \u00feessi g\u00f6gn fyrir flj\u00f3tlega tilv\u00edsun?\\nSvarm\u00f6guleikar:\\na. skriflega l\u00fdsingu\\nb. t\u00f6flu\\nc. st\u00f6\u00f0varl\u00edkan\\nd. ve\u00f0urkort\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hva\u00f0a breytingar ur\u00f0u \u00feegar reikistj\u00f6rnurnar hitnnu\u00f0u \u00e1 me\u00f0an \u00fe\u00e6r myndu\u00f0ust?\\nSvarm\u00f6guleikar:\\na. Massi \u00feeirra j\u00f3kst.\\nb. \u00de\u00e6r t\u00f6pu\u00f0u meirihluta geislavirkra sams\u00e6ta sinna.\\nc. Uppbygging \u00feeirra a\u00f0greindist \u00ed mismunandi l\u00f6g.\\nd. \u00de\u00e6r byrju\u00f0u a\u00f0 sn\u00faast \u00ed kringum s\u00f3lina.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-mmlu-is","title":"Unofficial: MMLU-is","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The dataset was translated using Mi\u00f0eind's Greynir translation model.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Af hverju er \u00f6ruggara a\u00f0 horfa \u00e1 tungli\u00f0 en a\u00f0 horfa \u00e1 s\u00f3lina?\\nSvarm\u00f6guleikar:\\na. Tungli\u00f0 er minna bjart.\\nb. Tungli\u00f0 er n\u00e6r j\u00f6r\u00f0inni.\\nc. Tungli\u00f0 sk\u00edn a\u00f0allega \u00e1 n\u00f3ttunni.\\nd. Tungli\u00f0 er a\u00f0eins fullt einu sinni \u00ed m\u00e1nu\u00f0i.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hva\u00f0a l\u00f6g jar\u00f0ar eru a\u00f0allega ger\u00f0 \u00far f\u00f6stu efni?\\nSvarm\u00f6guleikar:\\na. innri kjarni og ytri kjarni\\nb. skorpu og innri kjarni\\nc. skorpu og m\u00f6ttli\\nd. m\u00f6ttli og ytri kjarni\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Bekkur er a\u00f0 rannsaka \u00fe\u00e9ttleika bergs\u00fdna. Hva\u00f0a v\u00edsindalegan b\u00fana\u00f0 \u00feurfa \u00feau til a\u00f0 \u00e1kvar\u00f0a \u00fe\u00e9ttleika bergs\u00fdnanna?\\nSvarm\u00f6guleikar:\\na. sm\u00e1sj\u00e1 og vog\\nb. bikar og m\u00e6ligl\u00f6s\\nc. m\u00e6ligl\u00f6s og vog\\nd. sm\u00e1sj\u00e1 og m\u00e6ligl\u00f6s\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-icelandicknowledge","title":"Unofficial: IcelandicKnowledge","text":"<p>This dataset is based on the IcelandicQA dataset, which was published here, but is here phrased as a knowledge dataset. The candidate answers has been generated by GPT-4o, using the following prompt for each <code>row</code> in the original dataset:</p> <p><pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": f\"For the question: {row.question} where the correct answer is: {row.answer}, please provide 3 plausible alternatives in Icelandic.\",\n    }\n]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o\", messages=messages, response_format=CandidateAnswers\n)\n</code></pre> where <code>CandidateAnswers</code> is a Pydantic model that is used to ensure structured outputs.</p> <p>The original dataset has 2,000 samples, but only 1,997 unique questions, and the total length of this dataset is therefore 1,997. The split is given by 845 / 128 / 1024 for train, val, and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"Hva\u00f0a gamla ver\u00f0eining var jafngildi einnar k\u00fdr a\u00f0 ver\u00f0m\u00e6ti?\\nSvarm\u00f6guleikar:\\na. Sau\u00f0f\u00e9\\nb. K\u00fagildi\\nc. Mj\u00f3lkurtollur\\nd. Hrossgildi\",\n    \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Hven\u00e6r komu \u00cdslendingar fyrst til Gimli \u00ed Manitoba?\\nSvarm\u00f6guleikar:\\na. 15. september 1875\\nb. 25. okt\u00f3ber 1874\\nc. 10. okt\u00f3ber 1876\\nd. 21. okt\u00f3ber 1875\",\n    \"label\": \"d\"\n}\n</code></pre> <pre><code>{\n    \"text\": \"Hva\u00f0a \u00e1r var byggingin sem gaf Bar\u00f3nsst\u00edg \u00ed Reykjav\u00edk nafn reist?\\nSvarm\u00f6guleikar:\\na. 1901\\nb. 1897\\nc. 1899\\nd. 1898\",\n    \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset icelandic-knowledge\n</code></pre>"},{"location":"datasets/icelandic/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/icelandic/#winogrande-is","title":"Winogrande-is","text":"<p>This dataset was published in this paper and is a manually translated and adapted version of the English WinoGrande dataset. The samples are sentences containing two nouns and an ambiguous pronoun, and the task is to determine which of the two nouns the pronoun refers to.</p> <p>The original full dataset consists of 1,095 samples, and we use a 64 / 128 / 896 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Eiginma\u00f0urinn hennar Myrru keypti handa henni h\u00e1lsmen me\u00f0 perlu og h\u00fan h\u00e9lt a\u00f0 \u00fea\u00f0 v\u00e6ri ekki ekta. _ var of gyllt.\\nSvarm\u00f6guleikar:\\na. perlan\\nb. h\u00e1lsmeni\u00f0\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Bergfinnur l\u00e9t sem hann heyr\u00f0i ekki \u00ed lekanum \u00ed krananum en hann haf\u00f0i ekkert um a\u00f0 velja \u00feegar hundurinn gelti. _ er h\u00e1v\u00e6rari.\\nSvarm\u00f6guleikar:\\na. lekinn\\nb. hundurinn\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Dan\u00eda var spenntari fyrir \u00fev\u00ed a\u00f0 heims\u00e6kja ritstj\u00f3rann en \u00deorl\u00e1ks\u00edna vegna \u00feess a\u00f0 _ fannst n\u00fdja b\u00f3kin geggju\u00f0.\\nSvarm\u00f6guleikar:\\na. \u00deorl\u00e1ks\u00ednu\\nb. Dan\u00edu\",\n  \"label\": \"b\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset winogrande-is\n</code></pre>"},{"location":"datasets/icelandic/#unofficial-hellaswag-is","title":"Unofficial: HellaSwag-is","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated using Mi\u00f0eind's Greynir translation model.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[h\u00f6f.] Hvernig finna m\u00e1 samr\u00e6mi \u00ed l\u00edfinu [titill] Skuldbinda \u00feig til breytinga. [skref] Fyrsta skrefi\u00f0 til a\u00f0 n\u00e1 fram breytingum \u00ed l\u00edfinu er a\u00f0 skuldbinda sig til breytinga. Me\u00f0 \u00fev\u00ed a\u00f0 gefa me\u00f0vita\u00f0a, viljasetta yfirl\u00fdsingu til sj\u00e1lfs s\u00edns um a\u00f0 \u00fe\u00fa munir halda \u00feig vi\u00f0 efni\u00f0 og n\u00e1 settum \u00e1rangri getur \u00fea\u00f0 hj\u00e1lpa\u00f0 \u00fe\u00e9r a\u00f0 halda \u00fe\u00e9r vi\u00f0 efni\u00f0 og \u00fdtt \u00fe\u00e9r \u00e1fram \u00ed \u00e1tt a\u00f0 \u00fev\u00ed markmi\u00f0i.\\nSvarm\u00f6guleikar:\\na. \u00de\u00e1 \u00e6ttir \u00fe\u00fa a\u00f0 vera a\u00f0 skuldbinda \u00feig til a\u00f0 lifa st\u00f6\u00f0ugra og samr\u00e6mdara l\u00edfi. [Undirskrefi] Hugsa\u00f0u um \u00e1st\u00e6\u00f0urnar fyrir \u00fev\u00ed a\u00f0 \u00fe\u00fa vilt lifa samr\u00e6mdara l\u00edfi.\\nb. [undirefni] Byrja\u00f0u \u00e1 \u00fev\u00ed a\u00f0 skuldbinda \u00feig til a\u00f0 breyta einhverju sem kemur \u00fe\u00e9r \u00far jafnv\u00e6gi. Ef \u00fe\u00fa gerir \u00fea\u00f0 ekki \u00fe\u00e1 situr\u00f0u uppi me\u00f0 eitthva\u00f0 sem lo\u00f0ir vi\u00f0 \u00feig heima hj\u00e1 \u00fe\u00e9r, sem ver\u00f0ur ekki au\u00f0veldara a\u00f0 koma \u00ed sta\u00f0inn fyrir \u00fe\u00e1 tilfinningu.\\nc. [Undirefni] Ekki l\u00e1ta sko\u00f0anir \u00fe\u00ednar e\u00f0a sko\u00f0anir stangast \u00e1 vi\u00f0 sj\u00e1lfsvir\u00f0ingu \u00fe\u00edna. Vi\u00f0urkenndu a\u00f0 \u00fe\u00fa s\u00e9rt fullor\u00f0inn og \u00fev\u00ed \u00f3hr\u00e6ddur vi\u00f0 a\u00f0 taka \u00fe\u00ednar eigin \u00e1kvar\u00f0anir var\u00f0andi \u00fea\u00f0 sem \u00fe\u00fa vilt \u00ed l\u00edfinu.\\nd. [Efnisor\u00f0] \u00deegar einhver annar hvetur \u00feig til a\u00f0 breyta, \u00fe\u00e1 skaltu ver\u00f0launa \u00feig fyrir \u00fea\u00f0 g\u00f3\u00f0a sem \u00fe\u00fa n\u00e6r\u00f0 fram \u00fe\u00f3 a\u00f0 \u00fea\u00f0 hafi kannski ekki liti\u00f0 \u00fat \u00e1 einhvern h\u00e1tt. [Titill] Ekki \u00e6tlast til \u00feess a\u00f0 f\u00f3lk breyti s\u00e9r af skyldur\u00e6kni.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ma\u00f0ur er a\u00f0 vinna \u00e1 spor\u00f6skjulaga v\u00e9l. \u00fea\u00f0\\nSvarm\u00f6guleikar:\\na. gr\u00edpur og st\u00fdrir t\u00e6kinu.\\nb. s\u00fdnir skj\u00e1inn \u00e1 v\u00e9linni.\\nc. er s\u00fdnd \u00ed tveimur hlutum, sem hver um sig er festur af manneskju.\\nd. vir\u00f0ist vera vins\u00e6ll eftir \u00fev\u00ed sem hann vinnur sig upp.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Sle\u00f0ast\u00falka \u00e1 uppbl\u00e1snum b\u00e1t heldur \u00e1 streng framan \u00e1 mann, allt \u00ed einu dettur h\u00fan \u00ed holu. F\u00f3lk ber sle\u00f0ab\u00e1ta og sle\u00f0ast\u00falkan er \u00e1 sle\u00f0ab\u00e1ti. eftir h\u00f3p af f\u00f3lki\\nSvarm\u00f6guleikar:\\na. sle\u00f0a saman kan\u00f3um, svo sle\u00f0a a\u00f0rir \u00ed vatninu.\\nb. sle\u00f0a hli\u00f0ar vatnsvatn \u00e1 hestum vi\u00f0 hli\u00f0ina \u00e1 br\u00fa b\u00e1ta.\\nc. sle\u00f0a ni\u00f0ur brekkuna \u00feanga\u00f0 til hitta a\u00f0ra einstaklinga.\\nd. Sle\u00f0amenn ganga \u00e1 torgi, \u00e1 milli annarra og s\u00ed\u00f0an hlaupa allir um.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\n</code></pre></li> <li>Base prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvara: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Spurningar: {text}\nSvarm\u00f6guleikar:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', 'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-is\n</code></pre>"},{"location":"datasets/icelandic/#summarization","title":"Summarization","text":""},{"location":"datasets/icelandic/#rrn","title":"RRN","text":"<p>This dataset was published in this paper and consists of news articles and their summaries from R\u00daV, the Icelandic National Broadcasting Service, from years 2021 and 2022.</p> <p>The original full dataset consists of 3,960 samples, and we use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Vi\u00f0 erum a\u00f0 sj\u00e1 \u00f3tta um truflanir \u00e1 framlei\u00f0sluke\u00f0jum og efnahagsstarfsemi eitthva\u00f0 \u00ed l\u00edkingu vi\u00f0 \u00fea\u00f0 sem var fyrr \u00e1 \u00e1rinu.\\nsegir J\u00f3n Bjarki Bentsson a\u00f0alhagfr\u00e6\u00f0ingur \u00cdslandsbanka. \u00c1hrif Delta afbrig\u00f0isins sj\u00e1st v\u00ed\u00f0a. Eftirspurn hefur ekki haldist \u00ed hendur vi\u00f0 v\u00e6ntingar sem me\u00f0al annars hefur orsaka\u00f0 mikla ver\u00f0l\u00e6kkun \u00e1 ol\u00edu \u00e1 heimsmarka\u00f0i undanfarnar vikur. Hefur ver\u00f0i\u00f0 \u00e1 ekki veri\u00f0 l\u00e6gra \u00ed \u00ferj\u00e1 m\u00e1nu\u00f0i.\\nB\u00edlaframlei\u00f0eindur eru einnig \u00ed vanda, en \u00fear er vandam\u00e1li\u00f0 ekki skortur \u00e1 eftirspurn heldur skortur \u00e1 a\u00f0f\u00f6ngum, \u00e1 svok\u00f6llu\u00f0um h\u00e1lflei\u00f0urum n\u00e1nar tilteki\u00f0. \u00deeir eru a\u00f0allega framleiddir \u00ed As\u00edu og hefur \u00fatbrei\u00f0sla Delta afbrig\u00f0isins raska\u00f0 framlei\u00f0slu og framkalla\u00f0 skort. Margir af st\u00e6rstu b\u00edlaframlei\u00f0endum heims hafa tilkynnt um a\u00f0 \u00feeir ney\u00f0ist til a\u00f0 draga \u00far framlei\u00f0slu og \u00fearf Toyota, st\u00e6rsti b\u00edlaframlei\u00f0andi heims, a\u00f0 minnka framlei\u00f0slu s\u00edna um 40 pr\u00f3sent.\\n\u00c1standi\u00f0 hefur s\u00f6mulei\u00f0is valdi\u00f0 mikilli styrkingu dollars. Mi\u00f0gengi se\u00f0labanka \u00cdslands \u00ed dag er 128 kr\u00f3nur en var \u00ed byrjun sumars 121 kr\u00f3na. \u00c1 sama t\u00edma hefur kr\u00f3nan haldist st\u00f6\u00f0ug gagnvart \u00f6\u00f0rum myntum. Auk \u00fatbrei\u00f0slu Delta afbrig\u00f0isins hafa atbur\u00f0ir li\u00f0inna vikna \u00ed Afganistan \u00fer\u00fdst \u00e1 styrkingu dollarsins.\\n\u00deetta hefur allt \u00e1hrif til \u00feess a\u00f0 hvetja til \u00f3tta \u00ed \u00f6ryggi eins og svo er kalla\u00f0 og dollarinn n\u00fdtur oft g\u00f3\u00f0s af svolei\u00f0is \u00f3tta. \u00deykir n\u00e1tt\u00farlega gr\u00ed\u00f0arlega \u00f6rugg eign a\u00f0 hafa og seljanleiki hans er n\u00e1tt\u00farlega meiri en nokkurs annars eigna flokks.\",\n  \"target_text\": \"\u00datbrei\u00f0sla Delta afbrig\u00f0is k\u00f3r\u00f3nuveirunnar \u00f3gnar bata heimshagkerfisins. Ol\u00eduver\u00f0 hefur hr\u00ed\u00f0falli\u00f0 \u00e1 undanf\u00f6rnum vikum, b\u00edlaframlei\u00f0endur f\u00e1 ekki a\u00f0f\u00f6ng og fj\u00e1rfestar flykkjast \u00ed bandar\u00edkjadollar. \"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Ve\u00f0urfar hefur veri\u00f0 \u00f3venjulegt \u00e1 su\u00f0vesturhorni landsins. L\u00edti\u00f0 snj\u00f3a\u00f0i \u00ed vetur og s\u00ed\u00f0ustu vikur hefur \u00farkoma veri\u00f0 me\u00f0 allra minnsta m\u00f3ti. J\u00f3n \u00de\u00f3r \u00d3lason, forma\u00f0ur Stangvei\u00f0if\u00e9lags Reykjav\u00edkur, segir a\u00f0 vei\u00f0imenn s\u00e9u vissulega or\u00f0nir langeygir eftir rigningunni, en b\u00e6tir vi\u00f0 a\u00f0 eitt helsta einkenni \u00edslenskra vei\u00f0imanna s\u00e9 \u00f3bilandi bjarts\u00fdni.\\nJ\u00f3n \u00de\u00f3r segir a\u00f0 nor\u00f0an- og austanlands s\u00e9u horfurnar betri. \u00deurrkat\u00ed\u00f0in hefur \u00fe\u00f3 ekki haft \u00e1hrif \u00e1 s\u00f6lu vei\u00f0ileyfa. \u00d3vissan um ve\u00f0urfar fylgi me\u00f0 \u00ed kaupunum og n\u00fa \u00feegar eru margar af \u00e1m f\u00e9lagsins uppseldar. \u00de\u00e1 er von \u00e1 fleiri \u00fatlendingum \u00ed \u00e1r en \u00ed fyrra, en k\u00f3r\u00f3nuveirufaraldurinn haf\u00f0i mj\u00f6g mikil \u00e1hrif \u00e1 s\u00f6lu vei\u00f0ileyfa \u00ed fyrra.\",\n  \"target_text\": \"Forma\u00f0ur Stangavei\u00f0if\u00e9lags Reykjav\u00edkur segir vei\u00f0imenn \u00e1 su\u00f0vesturhorni landsins dansa n\u00fa regndans \u00ed von um a\u00f0 langvarandi \u00feurrkat\u00ed\u00f0 s\u00e9 senn \u00e1 enda.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"\u00cd morgun fjarl\u00e6g\u00f0u b\u00e6jarstarfsmenn \u00e1berandi kosningabor\u00f0a frambo\u00f0sins Vina K\u00f3pavogs \u00e1 horni Digranesvegar og Gr\u00e6nutungu. J\u00f3hann Sigurbj\u00f6rnsson, sem er \u00ed18. s\u00e6ti \u00e1 lista Vina K\u00f3pavogs, setti bor\u00f0ana upp og er afar \u00f3s\u00e1ttur vi\u00f0 \u00feeir hafi veri\u00f0 fjarl\u00e6g\u00f0ir. Hann segir a\u00f0 vegi\u00f0 s\u00e9 a\u00f0 tj\u00e1ningarfrelsi s\u00ednu.\\n\u00c9g hengi upp bor\u00f0a vegna \u00feess a\u00f0 \u00e9g tel mig vera \u00ed fullum r\u00e9tti til a\u00f0 tj\u00e1 mig um \u00fe\u00e6r framkv\u00e6mdir sem eru \u00ed gangi h\u00e9rna \u00e1 m\u00f3ti m\u00e9r. \u00c9g hengi upp \u00feessa bor\u00f0a \u00e1 grindverki\u00f0 sem er r\u00e9tt fyrir innan l\u00f3\u00f0am\u00f6rk s\u00ed\u00f0an koma hinga\u00f0 menn \u00ed gulum f\u00f6tum \u00ed morgun fr\u00e1 b\u00e6num sem fjarl\u00e6gja bor\u00f0ana.\\nB\u00e6jarstarfsmenn hafa undanfari\u00f0 veri\u00f0 \u00ed samskiptum vi\u00f0 frambo\u00f0i\u00f0 um a\u00f0 broti\u00f0 hafi veri\u00f0 gegn l\u00f6greglusam\u00feykkt og byggingarregluger\u00f0 me\u00f0 \u00fev\u00ed a\u00f0 setja upp augl\u00fdsingabor\u00f0a \u00e1 l\u00f3\u00f0am\u00f6rkum og utan \u00feeirra, og einnig svo st\u00f3ra augl\u00fdsingabor\u00f0a a\u00f0 s\u00e9rstakt leyfi \u00feurfi.\\nSigr\u00ed\u00f0ur Bj\u00f6rg T\u00f3masd\u00f3ttir uppl\u00fdsingafulltr\u00fai K\u00f3pavogsb\u00e6jar segir \u00ed samtali vi\u00f0 fr\u00e9ttastofu a\u00f0 sk\u00fdrar reglur gildi um uppsetningu augl\u00fdsingaskilta. Reglur um sl\u00edka uppsetningu hafi veri\u00f0 sendar a\u00f0 gefnu tilefni \u00e1 alla frambo\u00f0sflokka \u00ed K\u00f3pavogi fyrir helgi. \u00de\u00e1 hafi st\u00f3rt augl\u00fdsingaskilti \u00e1 vegum Frams\u00f3knarflokksins \u00ed Sk\u00f3garlind veri\u00f0 fjarl\u00e6gt af b\u00e6jaryfirv\u00f6ldum \u00ed s\u00ed\u00f0ustu viku. Sigr\u00ed\u00f0ur segir a\u00f0 skiltin ver\u00f0i a\u00f0 vera undir tveimur fermetrum til a\u00f0 mega vera uppi - annars \u00feurfi a\u00f0 s\u00e6kja um leyfi fr\u00e1 byggingarfulltr\u00faa K\u00f3pavogsb\u00e6jar. Reglurnar s\u00e9u sk\u00fdrar.\\nHelga, Oddviti Vina K\u00f3pavogsb\u00e6jar segist hissa yfir framgangi b\u00e6jaryfirvalda, \u00feetta geti ekki sta\u00f0ist sko\u00f0un og a\u00f0 frambo\u00f0i\u00f0 muni leita r\u00e9ttar s\u00edns.\",\n  \"target_text\": \"Augl\u00fdsingaskilti og frambo\u00f0sbor\u00f0ar hafa veri\u00f0 fjarl\u00e6g\u00f0 af b\u00e6jaryfirv\u00f6ldum \u00ed K\u00f3pavogi v\u00ed\u00f0s vegar um b\u00e6inn s\u00ed\u00f0ustu daga. \"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e9ttagrein: {text}\nSamantekt: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e9ttagrein: {text}\n\nSkrifa\u00f0u samantekt um ofangreindu grein.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset rrn\n</code></pre>"},{"location":"datasets/norwegian/","title":"\ud83c\uddf3\ud83c\uddf4 Norwegian","text":"<p>This is an overview of all the datasets used in the Norwegian part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/norwegian/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/norwegian/#norec","title":"NoReC","text":"<p>This dataset was published in this paper and is based on reviews from three different media organisations: Schibsted Media Group, Aller Media and NRK.</p> <p>The original full dataset consists of 680,792 / 101,106 / 101,594 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Den som ikke blir rystende ber\u00f8rt av \u00ab De utvalgte \u00bb , m\u00e5 v\u00e6re forherdet til det immune .\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Under er noen av funksjonene som er dels unike for LG G3 :\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Tilsvarende f\u00e5r vi ogs\u00e5 lavere score i 3DMark enn hva tilfellet er for f.eks . Xperia Z2 og Galaxy S5 .\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', 'n\u00f8ytral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Anmeldelse: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Anmeldelse: {text}\n\nKlassifiser sentimentet i anmeldelsen. Svar med 'positiv', 'n\u00f8ytral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>n\u00f8ytral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norec\n</code></pre>"},{"location":"datasets/norwegian/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/norwegian/#norne-nb","title":"NorNE-nb","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Bokm\u00e5l Universal Dependencies treebank. The NER labels almost follow the CoNLL-2003 standard, but with some additional labels.</p> <p>The original full dataset consists of 15,696 / 2,410 / 1,939 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>We have mapped the labels into the CoNLL-2003 standard as follows:</p> <ul> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>PER</code> \u27a1\ufe0f <code>PER</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>MISC</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>GPE_LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE_ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>PROD</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DRV</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>EVT</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'fremkommer', 'av', '\u00e5rsmeldingene', 'fra', 'Bergen', 'helser\u00e5d', 'i', '\u00e5rene', '1952', '-', '66', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Viktig', 'var', 'det', 'ogs\u00e5', 'at', 'Kina', 'allerede', 'var', 'blitt', 's\u00e5', 'avhengig', 'av', 'det', 'amerikanske', 'markedet', 'og', 'av', 'dollaren', ',', 'at', 'en', 'nedgang', 'i', 'USA', 'ogs\u00e5', 'ville', 'ramme', 'Kina', 'hardt', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  'tokens': array(['Han', 'tok', 'fram', 'pistolen', 'og', 'dro', 'tilbake', 'til', 'Skaregata', '2', '.'], dtype=object),\n  'labels': array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene som forekommer i den gitte frasen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nNavngitte enheter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentifiser de navngitte enhetene i frasen. Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', 'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene av den typen, akkurat som de vises i frasen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norne-nb\n</code></pre>"},{"location":"datasets/norwegian/#norne-nn","title":"NorNE-nn","text":"<p>This dataset was published in this paper and is a manually NER annotated version of the Nynorsk Universal Dependencies treebank. The NER labels almost follow the CoNLL-2003 standard, but with some additional labels.</p> <p>The original full dataset consists of 14,174 / 1,890 / 1,511 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>We have mapped the labels into the CoNLL-2003 standard as follows:</p> <ul> <li><code>LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>PER</code> \u27a1\ufe0f <code>PER</code></li> <li><code>ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>MISC</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>GPE_LOC</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>GPE_ORG</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>PROD</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>DRV</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>EVT</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['-', 'Ulfr', 'provoserer', 'kjapt', 'fram', 'eit', 'slagsm\u00e5l', ',', 'og', 'han', 'drep', 'hovdingen', '.'], dtype=object),\n  \"labels\": array(['O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['I', 'haust', 'blei', 'det', 'avsl\u00f8rt', 'at', 'minst', 'to', 'tolv\u00e5ringar', 'p\u00e5', 'mellomtrinnet', 'ved', 'Gimle', 'skule', 'hadde', 'med', 'seg', 'alkohol', 'p\u00e5', 'ein', 'skuletur', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['Krigen', 'mot', 'Irak', 'skulle', 'aldri', 'ha', 'vore', 'gjennomf\u00f8rd', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene som forekommer i den gitte frasen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Frase: {text}\nNavngitte enheter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Frase: {text}\n\nIdentifiser de navngitte enhetene i frasen. Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', 'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene av den typen, akkurat som de vises i frasen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>sted</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisasjon</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norne-nn\n</code></pre>"},{"location":"datasets/norwegian/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/norwegian/#scala-nb","title":"ScaLA-nb","text":"<p>This dataset was published in this paper and was automatically created from the Bokm\u00e5l Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 20,044 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En vellykket gjennomf\u00f8ring av denne reformen vil bli en avgj\u00f8rende pr\u00f8ve p\u00e5 Regjeringens handlekraft.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Lunde var ikke blant, mener Andreassen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"72 kjoler g\u00e5r hver med sesong.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nb\n</code></pre>"},{"location":"datasets/norwegian/#scala-nn","title":"ScaLA-nn","text":"<p>This dataset was published in this paper and was automatically created from the Nynorsk Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 17,575 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Dersom Noreg snart g\u00e5r forbi Danmark i folketal, slik framskrivingane tilseier, kan ogs\u00e5 dette langt p\u00e5 veg forklarast med naturressursar.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Eg kan ikkje sj\u00e5 at det er grunn til \u00e5 ha ei slik grense i lova, det kan vurderast i, seier ho.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"SV har elles levert og i dag framsett ei gode forslag som kan bidra til \u00e5 gjera noko med straumprisproblematikken og straumforbruket, om viljen v\u00e5r er der.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-nn\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-jentoft","title":"Unofficial: Jentoft","text":"<p>This dataset was published in this Master's thesis by Matias Jentoft.</p> <p>The original dataset consists of 85,771 / 10,827 / 10487 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. In each split, the distribution of <code>correct</code> and <code>incorrect</code> is 50/50.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"For to uker siden var jeg p\u00e5 en fotoutstilling om Erytrea.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det viser seg at folk ikke kan leve uten mobiltelefonen.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mobiltelefoner dominerer mange av oss, og vi bruker dem over alt, p\u00e5 gatene 'hvert hj\u00f8rne', i gatene, holdeplasser, kaffeteriaene og i parken, der folk burde tilbringe koselig tid sammen i naturen.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\n</code></pre></li> <li>Base prompt template:   <pre><code>Setning: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Setning: {text}\n\nBestem om setningen er grammatisk korrekt eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nei</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset jentoft\n</code></pre>"},{"location":"datasets/norwegian/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/norwegian/#norquad","title":"NorQuAD","text":"<p>This dataset was published in this paper and is a manually annotated dataset based on data from the Bokm\u00e5l Wikipedia.</p> <p>The original full dataset consists of 3,810 / 472 / 472 samples for training, validation and test, respectively. We use a split of 1,024 / 256 / 2,048 samples for training, validation and test, respectively. When creating the splits, we only select samples that contain an answer in the associated context. The splits we use are new, so there might be some samples from the training split in the validation or test splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": 'Sprekpodden: Denne treningen gj\u00f8r deg smartere og lykkeligere\\nHJERNEFORSKER: \u2013 Hjernen er i utgangspunktet programmert for latskap. Derfor m\u00e5 vi i st\u00f8rre grad tvinge oss selv til \u00e5 v\u00e6re mer aktive, sier forsker Ole Petter Hjelle. Foto: Tor Stenersen (arkiv)\\nSPREKPODDEN: Denne uken har programleder Daniel R\u00f8ed-Johansen og Malene Indreb\u00f8-Langlo bes\u00f8k av Ole Petter Hjelle. Foto: Morten Uglum\\n\u2013 Vi var rett og slett lei av \u00e5 sitte og fortelle pasientene v\u00e5re at de m\u00e5tte v\u00e6re i fysisk aktivitet, uten at noe skjedde.\\nFor noen \u00e5r siden startet hjerneforsker og fastlege Ole Petter Hjelle, og de andre legene p\u00e5 \u00c5sg\u00e5rdstrand legekontor, en treningsgruppe for pasientene sine. Det ble stor suksess.\\n\u2013 Folk vet at det er bra \u00e5 trene for den fysiske helsen, men at fysisk aktivitet ogs\u00e5 er bra for den mentale helse, er et underkommunisert tema, sier han.\\nBedre enn sudoku og kryssord\\n\u2013 Er fysisk aktivitet bedre hjernetrim enn sudoku og kryssord?\\n\u2013 L\u00f8ser du masse kryssord, s\u00e5 blir du veldig til \u00e5 l\u00f8se kryssord. Men det har ikke de store ringvirkningene p\u00e5 v\u00e5re kognitive funksjoner, som det \u00e5 huske, planlegge og gjennomf\u00f8re, sier Hjelle.\\nHan forklarer at n\u00e5r pulsen v\u00e5r \u00f8ker, skilles det ut vekstfaktorer i hjernen som beskytter hjernecellene v\u00e5re og gj\u00f8r at cellene kommuniserer bedre.\\nForskning viser ogs\u00e5 at det dannes nye hjerneceller i enkelte deler av hjernen, under aktivitet.\\n\u2013 Men skal man f\u00e5 denne effekten, m\u00e5 man rett og slett v\u00e6re i aktivitet.\\nF\u00e5 opp pulsen\\nForskning viser ogs\u00e5 at fysisk aktivitet reduserer risiko for depresjon og demens, \u00f8ker intelligensen, bedrer hukommelsen, gj\u00f8r deg mer kreativ og gir deg et lengre og bedre liv.\\nHjelle forteller at det viktigste for \u00e5 hente ut disse fordelene er \u00e5 f\u00e5 opp pulsen.\\n\u2013 Men dersom du skulle valgt en aktivitet \u2013 som i st\u00f8rst mulig grad stimulerte flest mulig hjerneomr\u00e5der \u2013 pleier jeg \u00e5 si ballspill. Da f\u00e5r du opp pulsen, du samarbeider, har taktikk, koordinasjon, balanse og strategi, sier Hjelle.\\nH\u00f8r mer fra \u00abtreningslegen\u00bb i ukens Sprekpodden her.',\n  \"question\": 'Hva jobber Daniel som?',\n  \"answers\": {\n    \"answer_start\": array([286]),\n    \"text\": array(['programleder'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Litauiske medier: En utvekslingsavtale skal v\u00e6re p\u00e5 plass for Frode Berg\\nFrode Berg ble d\u00f8mt til 14 \u00e5rs fengsel i Russland. Foto: Tore Meek / NTB scanpix\\nRussland og Litauen er enige om \u00e5 utveksle en spiond\u00f8mt russer mot to litauere og en nordmann, opplyser kilder i den litauiske sikkerhetstjenesten til den litauiske nyhetstjenesten Baltic News Service (BNS).\\n\u2013 Utvekslingsavtalen inkluderer ogs\u00e5 en norsk statsborger som er d\u00f8mt i Russland, sier en anonym tjenestemann i den litauiske sikkerhetstjenesten.\\nAvisen navngir ikke Frode Berg, men Berg er den eneste nordmannen som soner en slik dom i Russland.\\nAftenposten og en rekke norske medier omtalte saken onsdag ettermiddag. Flere russiske medier melder ogs\u00e5 om det samme, alle med BNS som kilde\\n\u2013 H\u00e5per en avtale foreligger\\nFrode Bergs norske advokat Brynjulf Risnes kan ikke bekrefte opplysningene.\\n\u2013 Jeg har ikke informasjon som verken bekrefter eller avkrefter en slik avtale. Vi h\u00e5per selvsagt at en slik avtale foreligger, sier Risnes til NTB.\\nUD vil ikke kommentere saken.\\n\u2013 Norske myndigheter \u00f8nsker \u00e5 f\u00e5 Frode Berg hjem. Vi h\u00e5ndterer saken p\u00e5 den m\u00e5ten som vi mener er best for \u00e5 ivareta hans interesser. Utover det kommenterer vi ikke saken, sier underdirekt\u00f8r Ane Haavardsdatter Lunde i Utenriksdepartementet til NTB.\\nBergs russiske forsvarer, advokat Ilja Novikov, ikke vil kommentere saken, if\u00f8lge NRK.\\nSt\u00f8ttegruppen for Frode Berg h\u00e5per opplysningene stemmer.\\n\u2013 Dersom det viser seg at dette er riktig, er det en ufattelig god nyhet som vi har ventet p\u00e5 skulle skje, sier st\u00f8ttegruppemedlem Thorbj\u00f8rn Brox Webber til NTB.\\n\u2013 En slik avtale m\u00e5 bety at Frode kan komme tilbake til Norge og Kirkenes, legger han til.\\nD\u00f8mt for spionasje\\nBerg er d\u00f8mt til 14 \u00e5rs fengsel for spionasje. Han ble p\u00e5grepet i Moskva i desember 2017 og har sittet fengslet siden.\\nNRK meldte i august at UD er i forhandlinger med Russland om \u00e5 f\u00e5 Berg hjem og har informert hans n\u00e6rmeste familie om dette.\\nMuligheten for en utvekslingsavtale har v\u00e6rt antydet, men et problem har v\u00e6rt hvem den i s\u00e5 fall skal omfatte.',\n  \"question\": 'Hvilken norske advokat representerer Frode Berg?',\n  \"answers\": {\n    \"answer_start\": array([808]),\n    \"text\": array(['Brynjulf Risnes'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Ny nedtur for Ruud\\nCasper Ruud r\u00f8k torsdag ut av challengerturneringen i Koblenz. Bildet er fra en tidligere turnering.\\nAv Ole Henrik Tveten\\nDet ble en frustrerende kamp mot nederlandske Tallpon Griekspoor torsdag. Casper Ruud vant f\u00f8rste sett 6-4, men etter det var det lite som stemte for nordmannen i Tyskland.\\nI andre sett ble Ruud utspilt og tapte 1-6, mens feilene fortsatte \u00e5 florere ogs\u00e5 i tredje sett og Ruud tapte settet 2-6.\\nDen norske 20-\u00e5ringen gikk rett inn i 2. runde i Koblenz-turneringen etter \u00e5 ha f\u00e5tt walkover i den f\u00f8rste. Der slet han seg til seier mot italienske Raul Brancaccio onsdag. Torsdagens motstander, Tallpon Griekspoor, er nummer 233 p\u00e5 verdensrankingen.\\nDet startet bra for Snar\u00f8ya-gutten da han i f\u00f8rste sett br\u00f8t nederlenderens serve og tok ledelsen 4-3. Servebruddet ble avgj\u00f8rende for settet som Ruud vant 6-4, etter blant annet \u00e5 ha reddet en breakball etter en lengre ballveksling.\\nI andre sett begynte problemene for Casper Ruud. Griekspoor br\u00f8t Ruuds serve ved f\u00f8rste anledning og gikk opp i 2-0-ledelse. Deretter vant han egen serve, br\u00f8t Ruuds serve p\u00e5 ny og vant s\u00e5 egen serve. Da ledet plutselig nederlenderen 5-0.\\nNordmannen servet inn til 5-1, men det var dessverre ikke starten p\u00e5 noen snuoperasjon. Nederlenderen vant settet 6-1.\\nNordmannen hadde ikke ristet av seg problemene i pausen, og ble feid av banen av Griekspoor. Ruud kom under 0-4 i tredje sett f\u00f8r han omsider reduserte til 1-4. Men da var det for sent.\\nNederlenderen servet inn 5-1, Ruud reduserte, f\u00f8r Griekspoor servet seieren i land. Dermed tapte Ruud tredje sett 6-2 og r\u00f8k ut av turneringen.\\n\u00c5 ryke ut i Tyskland hjelper ikke nordmannens jakt p\u00e5 rankingpoeng for \u00e5 komme seg inn i topp 100 i verden. Han risikerer \u00e5 falle flere plasser ettersom han mister de 70 rankingpoengene han skaffet seg da han tok seg til 2. runde i Australian Open i fjor. Ruud er akkurat n\u00e5 nummer 112 p\u00e5 verdensrankingen. (NTB)',\n  \"question\": 'Hvordan endte 1. sett mellom Ruud og Griekspoor?',\n  \"answers\": {\n    \"answer_start\": array([244]),\n    \"text\": array(['6-4'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 2</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rsm\u00e5l: {question}\nSvar p\u00e5 maks 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor med maks 3 ord.\n\nSp\u00f8rsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norquad\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-norglm-multi-qa","title":"Unofficial: NorGLM Multi QA","text":"<p>This dataset was released in this paper and features a manually annotated reading comprehension dataset based on Norwegian news articles. This dataset is an abstractive question answering dataset, meaning that the answers do not always feature in the context. To fix this, they were rephrased using this script, which utilised the <code>gpt-4o-2024-05-13</code> model.</p> <p>The original dataset contains 2,406 samples, which we split into 1,024 / 256 / 1,126 samples for training, validation and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": ' Kommer det melding om at ansatte kj\u00f8per aksjer i eget selskap, kan det v\u00e6re gode grunner til at du ogs\u00e5 b\u00f8r gj\u00f8re det. \u2013 V\u00e6r p\u00e5 lag med innsiderne, er ekspertens r\u00e5d.Har du lyst til \u00e5 pr\u00f8ve deg som aksjeinvestor helt gratis og uten reell risiko? Meld deg p\u00e5 Aksje-NM her!Mange assosierer innsidehandel med kj\u00f8p og salg av aksjer basert p\u00e5 tilgang p\u00e5 selskapsnyheter f\u00f8r de blir offentliggjort i markedet. Slik handel kan gi stor \u00f8konomisk gevinst, og er ulovlig.Det finnes derimot ogs\u00e5 en lovlig form for innsidehandel, og denne kan det v\u00e6re lurt \u00e5 f\u00f8lge med p\u00e5, skal vi tro forskningssjef Geir Linl\u00f8kken i Investtech. Aksjeskolen er en del av E24s Aksje-NM. En tidligere versjon av denne artikkelserien ble publisert i 2020.N\u00e5r man snakker om \u00abinnsidehandel\u00bb i b\u00f8rssammenheng, siktes det som regel til handler som direkt\u00f8rer, styremedlemmer og andre n\u00f8kkelmedarbeidere gj\u00f8r. Disse handlene m\u00e5 rapporteres inn til Oslo B\u00f8rs, og kj\u00f8pet eller salget blir offentlig informasjon. Denne informasjonen kan v\u00e6re gull verdt, skal vi tro forskningen til Investtech.\u2013 N\u00f8kkelpersoner som direkt\u00f8rer og styremedlemmer sitter p\u00e5 veldig mye kunnskap om bedriften. N\u00e5r disse enten selger eller kj\u00f8per aksjer i eget selskap, kan det ses p\u00e5 som et signal til andre akt\u00f8rer, sier Linl\u00f8kken. Linl\u00f8kken har forsket p\u00e5 innsidehandel og tatt utgangspunkt i over 11.000 rapporterte innsidekj\u00f8p i norske og svenske selskaper. Han har sett n\u00e6rmere p\u00e5 hvordan kursen utviklet seg i tiden etter innsidekj\u00f8pet. \u2013 Vi fant at disse selskapene p\u00e5 \u00e5rlig basis steg med 7,1 prosentpoeng mer enn andre selskaper. Det kan alts\u00e5 v\u00e6re et godt tips \u00e5 f\u00f8lge med p\u00e5 innsidekj\u00f8p.Dersom det tikker inn meldinger om at innsidere selger aksjene sine, er det ogs\u00e5 lurt \u00e5 f\u00f8lge n\u00f8ye med. Investtech har tatt utgangspunkt i over 6.900 slike tilfeller i Norge og Sverige, og gjorde spennende funn. \u2013 I snitt gjorde disse aksjene det 3,0 prosentpoeng svakere enn b\u00f8rsen, sier han. Linl\u00f8kken forteller at noen av aksjene kan ha falt for eksempel 50 prosent etter innsidesalg, mens det kan ha g\u00e5tt ganske bra i andre selskaper med innsidesalg.\u2013 Men i gjennomsnitt har disse aksjene gjort det d\u00e5rlig, fastsl\u00e5r han.Linl\u00f8kken sier at Investtech anser innsidehandelanalyse som en forenklet fundamental analyse, alts\u00e5 en analyse av om aksjen er billig eller dyr i forhold til verdiene i selskapet. Har man ikke tid eller kunnskap til \u00e5 gj\u00f8re slik analyse selv, er det et godt alternativ \u00e5 se til innsiderne. \u2013 Historisk og statistisk sett, har det v\u00e6rt riktig \u00e5 f\u00f8lge innsiderne og v\u00e6re p\u00e5 lag med dem, svarer Linl\u00f8kken.',\n  \"question\": 'Hva kan man gj\u00f8re dersom man ikke har tid eller kunnskap til \u00e5 gj\u00f8re en analyse av aksjene til et selskap?',\n  \"answers\": {\n    \"answer_start\": 2434,\n    \"text\": array(['Se til innsiderne.'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": ' Alt om pubertet, penis, psyken og livet sj\u00e6l. Nok en fullkommen bok fra duoen bak et par av de st\u00f8rste boksuksessene de siste \u00e5rene. \u00abDe har gjort det igjen\u00bb, skrev jeg i VG for ganske n\u00f8yaktig to \u00e5r siden, da jeg satt her og leste og anmeldte \u00abJenteboka\u00bb av legene Nina Brochmann og Ellen St\u00f8kken Dahl. Da hadde det g\u00e5tt to \u00e5r siden de brak-debuterte med \u00abGleden med skjeden\u00bb. Jeg gav \u00abJenteboka\u00bb terningkast 6. Vel, vel. Du har kanskje gjettet det n\u00e5, men n\u00e5 har de alts\u00e5 gjort det enda en gang: Laget en knallgod, fullkommen bok vi f\u00e5r h\u00e5pe mange leser.For jeg t\u00f8r p\u00e5st\u00e5 at guttene trenger sin Guttebok vel s\u00e5 mye som jentene trenger sin. For selv om det er jentene vi har snakket mest om, er det mange unge gutter som sliter. Unge gutter faller oftere ut av skolen, er mer deprimerte og har mindre fremtidsoptimisme enn f\u00f8r. Det finnes dyster statistikk, kort fortalt: De opplever ogs\u00e5 stress og press og uhelse. Og s\u00e5 er de ikke s\u00e5 flinke til \u00e5 snakke om det. I \u00abGutteboka\u00bb tar Brochmann og Dahl for seg alt man m\u00e5 vite og forst\u00e5 n\u00e5r man er p\u00e5 vei inn i eller st\u00e5r midt i puberteten. (Eller senere i livet, for den saks skyld, jeg plukket opp noen gode tips selv, jeg.) De skriver om kroppsh\u00e5r, kviser, stemmeskifte,  legning, penisst\u00f8rrelse, pung, kj\u00f8nn, s\u00e6d, k\u00e5thet, ereksjonsknipe (!) og svettelukt, for \u00e5 nevne noen av mange h\u00f8ydepunkter.  Legeduoen havnet p\u00e5 denne lista: De ti heteste norske forfatterne i utlandet! Foruten alle de rent kroppslige og fysiske forandringene man kan oppleve p\u00e5 veien fra gutt til mann, inneholder boka gode kapitler om de psykiske aspektene og livet sj\u00e6l. Grensesetting, samtykke, nettvett, om \u00e5 trenge en pornopause, om psykisk uhelse, stress og press. \u00abAlle har det vondt iblant, men ingen har det vondt for alltid. Du kommer til \u00e5 bli glad igjen!\u00bb Det er noe med tonen i boka, som er s\u00e5 fin. Lett, \u00e5pen, sympatisk, avv\u00e6pnende. Smart, kul og og med faglig tyngde. Men aldri formanende, ingen pekefinger. \u00abOnani er godt og sunt. Onani er ikke bare ufarlig \u2013 det er bra for deg.\u00bb \u00abKroppen din er laget for \u00e5 brukes og nytes.\u00bb  \u00abDet er synd at trening ender opp med \u00e5 handle om bare utseendet. \u00c5 trene er nemlig bra for deg. Det er ikke jakten p\u00e5 \u00abdr\u00f8mmekroppen\u00bb.\u00bb Selv de mer alvorlige og kliniske temaene er dessuten en forn\u00f8yelse \u00e5 bla om til, ogs\u00e5 takket v\u00e6re de fantastiske illustrasjonene til Magnhild Wisnes. De er fargerike og morsomme, og gj\u00f8r boka komplett. S\u00e5 mange peniser har jeg ikke sett siden vi fniste og lo av \u00abPenisatlaset\u00bb p\u00e5 et nachspiel i studietiden. S\u00e5 kan man jo stille seg sp\u00f8rsm\u00e5let, om denne boka n\u00e5r frem til dem som trenger \u00e5 lese den. Den burde egentlig v\u00e6rt pensum, tenker jeg, eller i alle fall utgangspunkt for et prosjekt p\u00e5 skolen. \u00c5 sette seg ned med en bok, som attp\u00e5til handler om puberteten, st\u00e5r vel ikke h\u00f8yest p\u00e5 lista over hva ten\u00e5ringsgutter flest vil bruke fritiden sin p\u00e5. Pr\u00f8v likevel.  Jeg vet ikke, kanskje betale gutten noen kroner for \u00e5 lese den, om det er det som skal til. Jeg f\u00f8ler meg sikker p\u00e5 at det vil v\u00e6re verdt det. For hvis de unge guttene v\u00e5re leser denne boka, er jeg sikker p\u00e5 at livet blir lettere \u00e5 leve og verden et morsommere sted. Anmeldt av: Trine Saugestad Hatlen',\n  \"question\": 'Hvem st\u00e5r for illustrasjonene i \u00abGutteboka\u00bb?',\n  \"answers\": {\n    \"answer_start\": 2321,\n    \"text\": array(['illustrasjonene til Magnhild Wisnes'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": ' Regjeringen lanserer ny handlingsplan for \u00e5 beskytte den truede villaksen. \u2013 Altfor slapt, sier SV-politiker.Regjeringen lanserer n\u00e5 en handlingsplan for \u00e5 bevare den truede villaksen.\u2013 Villaksen kan n\u00e5 bli r\u00f8dlistet i Norge for f\u00f8rste gong. Det er helt klart at det trengs konkrete tiltak for \u00e5 snu denne utviklingen, sier Sveinung Rotevatn i pressemeldingen fra regjeringen.Handlingsplanen inneholder tiltak mot blant annet lakselus, r\u00f8mt oppdrettsfisk, lakseparasitten Gyro, vannkraftregulering, forsuring, overbeskatning og fremmende fiskearter som pukkellaks.Regjeringen viser til at lakselus utgj\u00f8r den st\u00f8rste risikoen for \u00e5 gj\u00f8re ytterligere skade p\u00e5 vill atlantisk laks, if\u00f8lge Vitenskapelig r\u00e5d for lakseforvaltning.\u2013 Lakselus utgj\u00f8r en stor risiko for villaksen. Regjeringen vil blant annet utrede krav om nullutslipp av lakselus fra oppdrettsanlegg fra og med 2030, sier Rotevatn.Det vil i s\u00e5 fall inneb\u00e6re krav om lukkede anlegg.Lakselus finnes naturlig i alle havomr\u00e5der p\u00e5 den nordlige halvkule, og er den vanligste parasitten p\u00e5 laksefisk.Blir forekomsten av lus h\u00f8y, kan det v\u00e6re en utfordring b\u00e5de for oppdrettsfisk og vill laksefisk.Havbruk medf\u00f8rer at antall fisk i sj\u00f8en \u00f8ker, og dermed \u00f8ker ogs\u00e5 antall verter for lakselus. Niv\u00e5ene med lakselus i anleggene m\u00e5 derfor holdes lavest mulig, slik at de samlede lusemengdene i sj\u00f8en ikke blir for store.Som f\u00f8lge av omfattende resistens hos lusen mot kjemiske behandlingsmidler, har n\u00e6ringen de siste \u00e5rene v\u00e6rt tvunget til \u00e5 ta i bruk mekaniske metoder for \u00e5 fjerne lusen, med negative konsekvenser for fiskens velferd.Kilde: Lusedata, MattilsynetDagens trafikklyssystem som regulerer veksten i n\u00e6ringen i forhold til luseutviklingen, skal ogs\u00e5 utvikles og forbedres.Planen inneholder ogs\u00e5 tiltak mot en rekke andre p\u00e5virkningsfaktorer. Utfisking av r\u00f8mt oppdrettslaks skal \u00f8kes, og det skal vurderes nye metoder for \u00e5 spore og merke oppdrettslaks og hindre at r\u00f8mt oppdrettslaks gyter.Hele 80 prosent av villaksbestandene i Norge n\u00e5r for tiden ikke minstem\u00e5let for god kvalitet. R\u00f8mt oppdrettslaks og lakselus er regnet som de to st\u00f8rste truslene, skriver regjeringen.Fremmende fiskearter utgj\u00f8r ogs\u00e5 en risiko for b\u00e5de biologisk mangfold, produktiviteten til lokal laksefisk og akvakultur.I \u00e5r har Norge hatt den st\u00f8rste invasjonen av pukkellaks noensinne, og regjeringen vil derfor opprette en nasjonal kompetansegruppe for \u00e5 koordinere arbeidet med dette.SVs nestleder Torgeir Knag Fylkesnes er ikke forn\u00f8yd med tiltakene.\u2013 Dette er altfor, altfor slapt. Regjeringen tar ikke tak i elefanten i rommet, nemlig den lite b\u00e6rekraftige forvaltningen av oppdrettsn\u00e6ringa. Vi m\u00e5 stille strengere milj\u00f8krav til alle nye oppdrettstillatelser, og fase inn disse kravene hos de med eksisterende tillatelser, skriver han i en kommentar til E24.Han p\u00e5peker at det i dag tildeles oppdrettstillatelser til den h\u00f8ystbydende, og ikke til de med den mest milj\u00f8vennlige teknologien. \u2013 Skal vi redde villaksen og sikre en b\u00e6rekraftig vekst for oppdrettsn\u00e6ringen, m\u00e5 vi legge om systemet slik at vi gjennom \u00e5 gi billigere tillatelser, men med krav om nullutslipp, null r\u00f8mming og null ressurser p\u00e5 avveie.Fylkesnes understreker videre at teknologien finnes, og at n\u00e6ringen har god r\u00e5d.\u2013 N\u00e5r man for eksempel ser p\u00e5 Salmars investeringsaktivitet de siste ukene, s\u00e5 ser vi at n\u00e6ringen b\u00e5de kan betale for ny teknologi og skatt p\u00e5 formue og grunnrente.Fylkesnes gikk tidligere denne uken hardt ut mot Salmar-eier Gustav Witz\u00f8e, etter at laksemilliard\u00e6ren uttalte seg kritisk mot \u00f8kning i formuesskatten tidligere i sommer.',\n  \"question\": 'Hva inneholder regjeringens nye handlingsplan for villaksen?',\n  \"answers\": {\n    \"answer_start\": 377,\n    \"text\": array(['Handlingsplanen inneholder tiltak mot blant annet'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 2</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Tekst: {text}\nSp\u00f8rsm\u00e5l: {question}\nSvar p\u00e5 maks 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Tekst: {text}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor med maks 3 ord.\n\nSp\u00f8rsm\u00e5l: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norglm-multi-qa\n</code></pre>"},{"location":"datasets/norwegian/#knowledge","title":"Knowledge","text":""},{"location":"datasets/norwegian/#mmlu-no","title":"MMLU-no","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Norwegian was conducted using the DeepL translation API.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvorfor er Mahavira en viktig person i jainatradisjonene?\\nSvaralternativer:\\na. Han er den siste av de asketiske profetene.\\nb. Han er den f\u00f8rste av de asketiske profetene\\nc. Han er den mest l\u00e6rde av de asketiske profetene\\nd. Han er den helligste av de asketiske profetene\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En enfaset fullbroomformer kan drives i lastkommuteringsmodus hvis belastningen best\u00e5r av\\nSvaralternativer:\\na. RL.\\nb. RLC underdempet.\\nc. RLC overdempet.\\nd. RLC kritisk dempet.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En professor, som var eneeier av en boligblokk, skrev et skj\u00f8te med f\u00f8lgende ordlyd: \\\"Jeg overdrar herved min boligblokk til min s\u00f8nn og datter som leietakere i fellesskap.\\\" I skj\u00f8tet, som var korrekt utferdiget, forbeholdt professoren seg en livsvarig eiendomsrett. Professoren fortalte deretter barna sine om overdragelsen og la den i familiehvelvet i biblioteket for oppbevaring. Deretter giftet s\u00f8nnen seg med en lege. Professoren, som mislikte legen, utferdiget deretter et nytt skj\u00f8te som han kalte \\\"et korreksjonsskj\u00f8te\\\". I \\\"korreksjonsskj\u00f8tet\\\" overf\u00f8rte professoren byg\u00e5rden \\\"til min s\u00f8nn og datter som sameiere med overlevelsesrett.\\\" If\u00f8lge det nye skj\u00f8tet forbeholdt professoren seg igjen livsvarig eiendomsrett. Begge barna aksepterte overdragelsen av \\\"korreksjonsskj\u00f8tet.\\\" Et halvt \u00e5r senere d\u00f8de s\u00f8nnen, og etterlot seg legen som eneste arving. Eiendomsretten til boligblokken er i datterens og\\nSvaralternativer:\\na. datteren og legen som sameiere.\\nb. datteren med forbehold om professorens livstidsarv.\\nc. datteren og legen som sameiere, med forbehold om professorens livsarvinger.\\nd. datteren og legen som sameiere med overlevelsesrett, med forbehold for professorens livsarvinger.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-no\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-arc-no","title":"Unofficial: ARC-no","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Norwegian was conducted using the DeepL translation API.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Hvorfor er det tryggere \u00e5 se p\u00e5 m\u00e5nen enn p\u00e5 solen?\\nSvaralternativer:\\na. M\u00e5nen er mindre lyssterk.\\nb. M\u00e5nen er n\u00e6rmere jorden.\\nc. M\u00e5nen skinner mest om natten.\\nd. M\u00e5nen er full bare \u00e9n gang i m\u00e5neden.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Hvilket av f\u00f8lgende er et biprodukt av celle\u00e5nding hos dyr?\\nSvaralternativer:\\na. oksygen\\nb. varme\\nc. sukker\\nd. protein\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Big Bang-teorien sier at universet\\nSvaralternativer:\\na. trekker seg sammen.\\nb. ikke har noen begynnelse.\\nc. startet som \u00e9n enkelt masse.\\nd. hele tiden danner hydrogen.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-no\n</code></pre>"},{"location":"datasets/norwegian/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/norwegian/#hellaswag-no","title":"HellaSwag-no","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated to Norwegian using the DeepL translation API.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Slik holder du deg kj\u00f8lig og f\u00f8ler deg frisk om sommeren [title] Dusj hver dag. [step] Bruk en eksfolierende dusjs\u00e5pe for \u00e5 fjerne smuss. Sett vannet p\u00e5 varmt i starten av dusjen (fordi det rengj\u00f8r deg mer effektivt), men mot slutten av dusjen setter du vannet p\u00e5 lunkent eller kj\u00f8lig.\\nSvaralternativer:\\na. Dette senker kroppstemperaturen slik at du f\u00f8ler deg kj\u00f8ligere (og v\u00e5kner opp om morgenen!). [Sm\u00f8r deg med fuktighetskrem rett etter at du har g\u00e5tt ut av dusjen.\\nb. P\u00e5f\u00f8r denne gelen p\u00e5 svetten under armene eller p\u00e5 kroppen. Tenk p\u00e5 det som \u00e5 spyle den ene armhulen med vann (du kan lage din egen dusjs\u00e5pe med armene eller bena, og du kan vaske av deg litt med en gang).\\nc. Alternativt kan du \u00e5pne d\u00f8ren og la kj\u00f8lig vann str\u00f8mme gjennom det \u00e5pne vinduet i minst en time. [Bruk en ansiktsmaske mens du dusjer.\\nd. Vannet skal v\u00e6re varmt nok til \u00e5 skylle ut smuss og d\u00f8d hud som henger over ansiktet. P\u00e5f\u00f8r kroppss\u00e5pe (eller la den v\u00e6re \u00e5pen for lufting) p\u00e5 hudoverflaten i korte riller.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En l\u00f8per l\u00f8per p\u00e5 en bane foran en folkemengde. en mann\\nSvaralternativer:\\na. kaster en ball som hunden skal fange.\\nb. snakker til kameraet.\\nc. l\u00f8per ikke n\u00e5r han hopper ned i en sandkasse.\\nd. gir en kort introduksjon f\u00f8r han fortsetter og konkurrerer mot mannen i svart.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Slik vet du om hunden din liker deg best [title] Legg merke til at hunden din f\u00f8lger mye etter deg. [En m\u00e5te \u00e5 bevise at en hund liker deg best, er n\u00e5r den er mye sammen med deg. S\u00e5 hold \u00f8ye med om hunden din liker \u00e5 v\u00e6re i n\u00e6rheten av deg.\\nSvaralternativer:\\na. [Hold \u00f8ye med eventuell fysisk atferd. [Et godt eksempel p\u00e5 denne atferden er hvis den presser rumpa opp mot l\u00e5ret ditt og sjekker hva du har p\u00e5 deg.\\nb. [Se etter tegn p\u00e5 at hunden din kan v\u00e6re fl\u00f8rtende. [Et godt tegn p\u00e5 at hunden din liker deg er at den klapper deg mye eller stirrer p\u00e5 deg i intime \u00f8yeblikk.\\nc. [Finn ut om hunden din liker \u00e5 leke med deg. [Hvis det er en hund som elsker leker, kan du leke med dem, og hvis den er veldig glad i \u00e5 leke, s\u00e5 liker den at du leker med den.\\nd. Legg merke til at hunden din f\u00f8lger deg rundt i huset hver dag n\u00e5r du er ute og g\u00e5r. Selv om du kanskje ikke har lyst til det, kan det \u00e5 tilbringe mye tid sammen med en hund f\u00e5 den til \u00e5 f\u00f8le seg komfortabel med deg.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Sp\u00f8rsm\u00e5l: {text}\nSvaralternativer:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', 'c' eller 'd', og engu \u00f6\u00f0ru.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-no\n</code></pre>"},{"location":"datasets/norwegian/#summarization","title":"Summarization","text":""},{"location":"datasets/norwegian/#nosammendrag","title":"NoSammendrag","text":"<p>This dataset is a combination of the SNL and VG summarisation datasets as well as a translated version of the English XSum dataset, based on British BBC news articles. The SNL dataset is based on the Norwegian encyclopedia Store Norske Leksikon, while the VG dataset is based on the Norwegian articles from the newspaper VG. The translation of the XSum dataset was done using the NLLB model.</p> <p>The original full dataset consists of 472,000 samples, and we use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"P\u00e5 Akvariet i Bergen har pingvinene f\u00e5tt et ekstra fristende sommertilbud denne uken. \u2013 Vi fikk en litt artig id\u00e9, og bestemte oss for \u00e5 gi pingvinene v\u00e5re en slags \u00abslush-is\u00bb i g\u00e5r. Det ble til en morsom aktivisering for pingvinene, og det falt virkelig i god smak hos dem, sier dyrepasser Jannicke Johannessen. Hun forteller at de eldre pingvinene f\u00f8rst var litt skeptiske, og at det var de yngste som ledet an i isleken. \u2013 Ett- og to\u00e5ringene var veldig interesserte da vi kom ut med isen, og hoppet opp p\u00e5 den og storkoste seg. En av pingvinene ble faktisk liggende opp\u00e5 isen helt til den smeltet, ler hun. Hun forteller at isen falt i s\u00e5 god smak, at de skal gjenta suksessen l\u00f8rdag, slik at flere gjester i parken ogs\u00e5 kan f\u00e5 med seg aktiviteten.Selv om sommeren har satt flere varmerekorder i hele landet, forteller Johannessen at dyrene i Akvariet slettes ikke har lidd noen n\u00f8d. \u2013 Vi har California-sj\u00f8l\u00f8ver, som overhodet ikke har hatt noen problemer med varmen. Tvert imot, de elsker \u00e5 ligge \u00e5 sole seg. Vi har ogs\u00e5 europeiske otere, som takler klimaet godt, da det er dyr man finner naturlig i s\u00f8rlige deler av Europa. Dessuten er vi ekstremt heldige her p\u00e5 Akvariet, og pumper opp nytt saltvann hele tiden, og dyrene har mange muligheter til \u00e5 kj\u00f8le seg ned p\u00e5. Hun gir imidlertid et viktig r\u00e5d til dyreeiere som vil kj\u00f8le ned dyrene sine: \u2013 Jeg har f\u00e5tt med meg at folk gir is som hundene kan spise for eksempel, og det er ikke akkurat et sjakktrekk. N\u00e5r man kj\u00f8ler ned dyrene fra innsiden samtidig som det er veldig varmt ute, tuller det med kroppstemperaturen. Kroppen jobber for \u00e5 varme opp innsiden samtidig som de f\u00e5r varme utenfra. Du gir dem egentlig et heteslag, sier hun. \u2013 Det beste er \u00e5 kj\u00f8le dem ned p\u00e5 utsiden. Dusj dem under \u00abarmhulene\u00bb, eller generelt der de har tynn hud.Ogs\u00e5 i Tyskland har det v\u00e6rt h\u00f8ye temperaturer i sommer, og dyrepassere har m\u00e5ttet ta grep for \u00e5 avkj\u00f8le dyrene i varmen. I Osnabr\u00fcck, nord i landet, ble det registrert rundt 35 varmegrader onsdag. For tapirene i dyrehagen ble maten strategisk servert i skyggen, slik at dyrene ikke blir solbrent. Dyrepasser Daniel Chirico bestemte seg dessuten for \u00e5 spyle tapirene med en hageslange, for \u00e5 kj\u00f8le dem ned ytterligere. \u2013 Spesielt de nordiske artene i dyreparken har merket heteb\u00f8lgen, og tilbringer mesteparten av dagen i skyggen, sier Tobias Klumpe, biolog i Osnabr\u00fcck Zoo til den tyske avisen Osnabr\u00fccker Zeitung . Svartbj\u00f8rnene tar mer enn gjerne en kald dukkert i sola, samtidig som de nyter kalde forfriskninger med frukt og b\u00e6r.I Finland har ogs\u00e5 sommervarmen sl\u00e5tt inn for fullt. I Korkeasaari Zoo i Helsinki ble det torsdag registrert 30 varmegrader. L\u00f8sningen har blant annet v\u00e6rt \u00e5 installere en \u00abregnskog\u00bb for kenguruene, mens papeg\u00f8yene har f\u00e5tt egne dusjer de kan bruke. Bj\u00f8rnene har f\u00e5tt iskald vannmelon, som de nyter i det kalde vannet, og tigerne f\u00e5r frosne kaniner \u2013 s\u00e5fremt de faktisk \u00f8nsker \u00e5 spise. \u2013 Appetitten deres blir mindre i varmen. For eksempel spiser hunnene i snitt bare annenhver dag, sier dyrepasser Jonne Stenroth til den finske avisen MTV . Ellers tilbringer tigrene mesteparten av dagen i skyggen mens de slapper av i bassenget, skriver avisen.\",\n  \"target_text\": \"Mens solen skinner og temperaturene er som h\u00f8yest, tar dyreparker rundt om i Europa i bruk kreative l\u00f8sninger for \u00e5 holde dyrene avkj\u00f8lte.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Nick Corsellis, advokat for Carl Wood, sa at en \\\"innend\u00f8rs mann\\\" m\u00e5 ha v\u00e6rt involvert i razzia, men hans klient manglet ekspertise til \u00e5 v\u00e6re den personen. Mr Wood og tre andre menn nekter \u00e5 ha deltatt i \u00a3 14m r\u00f8veriet. Fire andre har allerede erkl\u00e6rt seg skyldig for deres roller i r\u00f8veriet. \\\"Og dette er en av grunnene til at Mr. Wood ikke er skyldig. Hva tok han med seg til bordet?\\\" sa han. Mr. Corsellis sa at det ikke fulgte at hans klient var mannen som ble identifisert av anklagemyndigheten som \\\"Man F\\\" i CCTV-opptak av razzia. \\\"Male F var faktisk en spiller. En innsider, eller knyttet til innsiden, som var fullt kjent med det indre arbeidet i Hatton Garden Safe Deposit\\\". Mr. Wood manglet slik kunnskap og ville bare ha v\u00e6rt i stand til \u00e5 fungere som en \\\"generell hundekrop\\\", sa advokaten. Corsellis spurte juryen om profesjonelle kriminelle ville v\u00e6rt forberedt p\u00e5 \u00e5 gi opp en del av sine millioner til en person som bare ville ha v\u00e6rt et \\\"ekstrapar hender (EPH)\\\". Han kalte det \\\"ilogisk\\\" og \\\"utrolig\\\" at en slik person var involvert da \\\"kriminelle ikke er veldedig folk\\\". \\\"Men hvem ville spille Carl Wood - EPH? Tror du at Mr. Tom Hardy eller Mr. Vinnie Jones vil haste \u00e5 ta rollen som... EPH?\\\" spurte han.\",\n  \"target_text\": \"En av mennene som er anklaget for \u00e5 v\u00e6re en del av Hatton Garden-raiden, kunne ikke ha v\u00e6rt involvert fordi han manglet noen ferdigheter \u00e5 tilby gjengen, har en domstol h\u00f8rt.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Verdenshjelpen forlot klubben i fjor p\u00e5 grunn av arbeids- og studietilbud, pluss behovet for \u00e5 komme seg fra en ryggskade. Manager Jamie Sherwood sa til klubbens nettside: \\\"Jeg er virkelig glad for \u00e5 ha brakt Natalie tilbake til klubben. \\\"Hennes erfaring, lederskap og \u00e5penbare evne blir et utmerket tillegg til v\u00e5r tropp for 2017\\\". Haigh la til: \\\"Etter skaden jeg fikk p\u00e5 ryggen for nesten 15 m\u00e5neder siden, trodde jeg aldri at jeg ville spille igjen, enn si p\u00e5 dette niv\u00e5et. \\\"Det er flott \u00e5 v\u00e6re tilbake i og rundt klubben - det er en ekte buzz etter den suksessen de oppn\u00e5dde i fjor\\\".\",\n  \"target_text\": \"Yeovil Town Ladies har gjenforenet tidligere kaptein Natalie Haigh f\u00f8r damer Super League One klubbens f\u00f8rste sesong i toppklassen.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset no-sammendrag\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-norglm-multi-sum","title":"Unofficial: NorGLM Multi Sum","text":"<p>This dataset was released in this paper and features a manually annotated summarisation dataset based on Norwegian news articles.</p> <p>The original dataset contains 467 samples, which we split into 147 / 64 / 256 samples for training, validation and test, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \" En sel i England ble fanget i plast. Det kunne g\u00e5tt galt. Hver dag blir ogs\u00e5 dyr i Norge fanget i plast. Et vondt syn m\u00f8tte nylig dyrevernere p\u00e5 en strand i England. Der l\u00e5 en sel som hadde tuklet seg inn i plast. Det kunne g\u00e5tt veldig galt.\u2013 Det var tydelig at selen hadde det vondt, forteller en kvinne som s\u00e5 selen p\u00e5 stranden, til kanalen BBC.Men dyrlegene fra den britiske dyrevernsorganisasjonen BDMLR kom heldigvis i tide. De klarte \u00e5 fri selen fra plasten. Selen ble sluppet tilbake i sj\u00f8en.Heldigvis ble ikke selen skadet denne gangen, forklarte dyrevernsorganisasjonen til BBC.Men mange dyr er ikke s\u00e5 heldige n\u00e5r de blir fanget i plast. Dyr setter seg fast i plast over hele verden. Norske sj\u00f8dyr setter seg fast i plast hver eneste dag, forteller Per-Erik Schulze. Han jobber i Naturvernforbundet og er ekspert p\u00e5 plast og forurensing i havet. \u2013 Mange av dyrene st\u00e5r fast i mange dager eller m\u00e5neder uten \u00e5 slippe l\u00f8s. Det er helt grusomt, sier Schulze.Han forteller at disse dyrene ofte setter seg fast i plast: Sj\u00f8fuglerFiskSelerSm\u00e5hvalerHummerSkilpadderDet er ogs\u00e5 dyr p\u00e5 land som setter seg fast i plast, for eksempel sauer og reinsdyr. Hvert \u00e5r havner over \u00e5tte millioner tonn plast i havet, if\u00f8lge Verdens naturfond (WWF). Det meste synker til havbunnen, resten skyller inn p\u00e5 strender eller flyter p\u00e5 havoverflaten.Det er farlig for dyr som lever i og rundt havet, fordi de kan sette seg fast i plasten eller f\u00e5 den i magen.Hva skjer med dyrene som setter seg fast i plast?\u2013 Det er det st\u00f8rste dyreplageriet i verden. Det er veldig vondt \u00e5 hekte seg fast. Mange d\u00f8r kanskje ikke av plasten, men av sult, fordi de ikke kommer seg l\u00f8s s\u00e5 de kan dra og spise, sier han.Derfor er det viktig ikke \u00e5 kaste plast som fors\u00f8pler naturen, mener Schulze.\u2013 En fin tanke er at hver plastbit vi rydder opp, kanskje kan redde et dyr. For det finnes ogs\u00e5 en god nyhet: De siste \u00e5rene har mange ryddet s\u00f8ppel i naturen og langs kysten i Norge. Har det hjulpet? \u2013 Ja, det har v\u00e6rt en kjempe-ryddedugnad i Norge de siste fem \u00e5rene. Noen steder er det s\u00e5 rent n\u00e5 at det er vanskelig \u00e5 finne noe plast. Det er et godt tegn, sier Schulze.\",\n  \"target_text\": \" En sel i England som var fanget i plast ble reddet av dyrevernere. Dette er en vanlig situasjon, b\u00e5de i Norge og andre steder i verden, da mange dyr setter seg fast og lider lenge fordi de ikke kan komme seg l\u00f8s. Per-Erik Schulze, en ekspert fra Naturvernforbundet, oppfordrer folk til \u00e5 fortsette ryddearbeidet for \u00e5 minimere risikoen for dyr \u00e5 komme til skade assosiert med plastfors\u00f8pling. Han bekrefter at ryddedugnadene i Norge har v\u00e6rt en suksess.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \" Det drar seg til mot sommer, ferietid, og ikke minst helg. Usikker p\u00e5 hva du skal vie den til? Her har du et lite knippe velmente tips.Denne guiden gjelder fra fredag 10. juni til s\u00f8ndag 12. juni.Fredag og l\u00f8rdag er det duket for folkefest og musikkbonanza p\u00e5 Viking stadion i J\u00e5tt\u00e5v\u00e5gen.Anledningen er to konserter fra det folkekj\u00e6re Stavangerbandet Mods, som er tilbake igjen p\u00e5 arenaen hvor de i 2012 og i 2017 spilte foran flere titalls tusen elleville fans. Ogs\u00e5 Kvelertak er med p\u00e5 \u00e5 innramme en meget sterk musikkhelg i regionen. P\u00e5 fredag g\u00e5r de nemlig opp p\u00e5 scenen p\u00e5 Folken i Stavanger, og skal by p\u00e5 de herligste toner med b\u00e5de hardrock og metall. Ogs\u00e5 i utelivets verden skjer det ting i helgen. Fredag kveld gj\u00f8r et nytt nattklubb- og cocktailbar-konsept sitt inntog i Stavanger n\u00e5r LouLou \u00e5pner d\u00f8rene i de gamle Hot-lokalene i Skagen. \u2013 Vi har sett at Stavanger manglet en annen og kanskje litt mer eksklusiv plass, hvor man kan feire bursdager og andre store begivenheter, sa daglig leder i Rekom, Frederik Mygind til Byas i forrige uke.Ogs\u00e5 p\u00e5 Show Bar, nysatsingen til duoen Dennis Poppe og \u00d8yvind S\u00f8rensen, blir det \u00e5pning til helgen. \u00abEin liden (ein) pre-opening i morgen (l\u00f8rdag) og s\u00f8ndag p\u00e5 Show Bar! Sees kl. 20:00\u00bb, skriver Poppe p\u00e5 sin Instagram-konto. Etter seieren borte mot Sverige sist s\u00f8ndag, er det en revansjelysten \u00abs\u00f6ta bror\u00bb som gjester Ullevaal kommende s\u00f8ndag. Flere rogalendinger figurerer i viktige roller p\u00e5 landslaget, med Erling Braut Haaland, Veton Berisha, Kristian Thorstvedt og Birger Meling som navnene. Kampen kan sees p\u00e5 flere utesteder i Stavanger, men kan ogs\u00e5 nytes fra sofaen fra klokken 20:45. I det Aftenbladet omtaler som \u00absuperdagene\u00bb, med en hel rekke arrangementer den kommende uken, finner flere av de sted denne helgen. Det 91 kilometer lange sykkell\u00f8pet, Nordsj\u00f8rittet, fra Egersund til Sandnes g\u00e5r av stabelen l\u00f8rdag, og kan la svettekjertlene f\u00e5 fri utfoldelse. Rittet s\u00e5 dagens lys tilbake i 1998 og er et samarbeid mellom flere lokale sykkelklubber. Og p\u00e5 Sola blir det moro for b\u00e5de store og sm\u00e5 n\u00e5r Sola Airshow 2022, flystevnet som har vist fram gamle og nye luftmaskiner i en \u00e5rrekke, holdes p\u00e5 l\u00f8rdagen og s\u00f8ndagen. Er du derimot mer opptatt av folkelivet, s\u00e5 kan enten Tanangerdagene, eller Solafestivalen v\u00e6re for deg. I Sola kulturhus er det p\u00e5 fredag og l\u00f8rdag duket for ungdomsfestival.Arrangementet er gratis, for de mellom 13 og 20 \u00e5r, og byr blant annet p\u00e5 musikk fra den norske rapperen Hkeem, samt Stavanger-bandet Kriminell Kunst. Og et lite stykke unna, fra onsdag denne uken og fram til og med s\u00f8ndag, blir det folkeliv i Tananger, n\u00e5r Tanagerdagene g\u00e5r av stabelen. Arrangementet holdes i regi av Lions Club Tananger, og lover fem dager fulle av aktiviteter for familier, barn, ungdom og voksne. \u2013 Her er noe for alle og mye for mange. Hjertelig velkommen, skriver arrang\u00f8ren p\u00e5 Facebook-arrangementet sitt. Fra 10. til 12. juni holder fem kunstnere pop up-utstilling i Pedersgata.Kunstnerne det er snakk om er ragnhild.kristine, pryl.art, hwks.art, corneliussen.art og Rosa Ottestad.Det hele finner sted i Pedersgata 43, og det er ventet flere bes\u00f8kende til arrangementet. Utstillingen \u00e5pner kl. 18 p\u00e5 fredag, og holder \u00e5pent gjennom helga. Vet du bedre enn oss hva skjer neste helg? Send en e-post til\u00a0helga@byas.no!\",\n  \"target_text\": \" Artikkelen handler om hvilke arrangementer som skal holdes i perioden fra 10. juni til 12. juni. Blant arrangementene er konserter med bandene Mods og Kvelertak, landskamp i fotball p\u00e5 Ullevaal, og flystevnet Sola Airshow 2022 p\u00e5 Sola der det skal vises fram gamle og nye luftmaskiner. I tillegg arrangeres Tanangerdagene og Solafestivalen.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \" Regjeringen foresl\u00e5r \u00e5 \u00e5pne nye omr\u00e5der for oppdrettsn\u00e6ringen, men med strenge milj\u00f8krav. \u2013 Gir betydelige muligheter for \u00e5 \u00f8ke produksjonen, sier fiskeriministeren.N\u00e6rings- og fiskeridepartementet foresl\u00e5r n\u00e5 en ny tillatelsesordning for oppdrett med milj\u00f8krav.Det f\u00f8rste \u00e5ret kan det tildeles tillatelser p\u00e5 maksimalt 15.000 tonn biomasse (fisk). Hver enkelt s\u00f8ker kan maksimalt f\u00e5 tildelt ti tillatelser, og det vil stilles strenge milj\u00f8krav til s\u00f8kerne, heter det i meldingen fra departementet.\u2013 Dagens produksjon i \u00e5pne merder vil fortsatt v\u00e6re grunnstammen i norsk oppdrett. I tillegg har vi lagt til rette for landbasert oppdrett og havbruk til havs. Med denne ordningen peker vi ut en ny retning som gir oppdrettsn\u00e6ringen mulighet til \u00e5 ta i bruk nye arealer langs kysten, sier fiskeri- og sj\u00f8matminister Odd Emil Ingebrigtsen (H).Til sammenligning ble det produsert rundt 1,4 millioner tonn laks i Norge i 2019, if\u00f8lge SSB.Tillatelsene i den nye milj\u00f8teknologiordningen kommer i tillegg til veksten som blir tilbudt p\u00e5 ordin\u00e6r m\u00e5te gjennom trafikklyssystemet.\u2013 Samlet sett gir dette norsk havbruksn\u00e6ring betydelige muligheter for \u00e5 \u00f8ke produksjonen fremover, sier ministeren.Forslaget inneb\u00e6rer f\u00f8lgende milj\u00f8krav: Null utslipp av egg og frittsv\u00f8mmende stadier av lakselus, minimum 60 prosent oppsamling av slam, samt krav til r\u00f8mningssikkerhet.Prisen for tillatelsene vil bli satt med utgangspunkt i auksjonsprisene som er oppn\u00e5dd i forbindelse med ordin\u00e6re kapasitetsjusteringer, men med et rimelig fradrag.\u2013 Havbruksn\u00e6ringen skaper store verdier for Norge. Men videre vekst m\u00e5 skje innenfor b\u00e6rekraftige rammer. Hensynet til natur generelt, og villaksen spesielt, er av avgj\u00f8rende betydning, sier klima- og milj\u00f8minister Sveinung Rotevatn (V).Til tross for bedring p\u00e5 viktige omr\u00e5der, er antallet norsk laks i havet mer enn halvert siden 1980-tallet, if\u00f8lge\u00a0Vitenskapelig r\u00e5d for lakseforvaltning.Det er flere grunner til det, ogs\u00e5 overfiske, men r\u00e5det sl\u00e5r fast at r\u00f8mt oppdrettslaks og lakselus n\u00e5 er de st\u00f8rste truslene mot villaks.Forslaget skal p\u00e5 kort tid ut p\u00e5 h\u00f8ring.E24 skrev tidligere at siste sitat i saken var fra Ingebrigtsen, mens det egentlig var fra Rotevatn. E24 beklager og har n\u00e5 rettet feilen.\",\n  \"target_text\": \" Regjeringen foresl\u00e5r en ny tillatelsesordning for oppdrett med strenge milj\u00f8krav for \u00e5 muliggj\u00f8re b\u00e6rekraftig vekst i havbruksn\u00e6ringen. Denne ordningen vil \u00e5pne nye omr\u00e5der for oppdrett, tillate hver s\u00f8ker \u00e5 f\u00e5 maksimalt ti tillatelser, og krever null utslipp av egg og frittsv\u00f8mmende stadier av lakselus, minimum 60 prosent oppsamling av slam, samt krav til r\u00f8mningssikkerhet. Dette skal gi n\u00e6ringen mulighet til \u00e5 \u00f8ke produksjonen p\u00e5 b\u00e6rekraftig m\u00e5te.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset norglm-multi-sum\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-schibsted-no","title":"Unofficial: Schibsted-no","text":"<p>This dataset was released here and features summaries of news articles from Schibsted Medias Norwegian newsrooms.</p> <p>The original dataset contains 1,240 / 347 / 374 samples for training, validation and testing, respectively. We use these splits as-is.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Klubblegenden med innr\u00f8mmelse under VAR-debatten: \u2013 Vanskelig \u00e5 st\u00e5 her : VAR-oppr\u00f8ret tok en knusende seier i Trondheim. Til og med styremedlem Ola By Rise m\u00e5tte innr\u00f8mme at det var mange gode argumenter imot videod\u00f8mmingen.  Den gamle keeperhelten talte RBK-styrets sak for VAR sammen med medstyremedlem Tore Reginiussen:  \u2013 Det er en veldig vanskelig sak. Det er ikke to VAR-tilhengere som st\u00e5r her, sa en engasjert By Rise fra talerstolen.  VAR-debatten hadde kommet til Rosenborgs medlemmer torsdag, som skulle stemme for at Rosenborg aktivt skulle arbeide for \u00e5 fjerne VAR eller ikke.  489 stemte for \u00e5 avvikle VAR. 157 stemte for \u00e5 beholde VAR. Stemmene ble lest opp til enorm applaus fra salen.  Forslaget om at RBK-styret skulle f\u00e5 \u00abutrede ulike modeller for \u00e5 f\u00e5 kapital inn i klubben\u00bb ble ogs\u00e5 stemt ned med god margin. \u2013 Medlemmene har definitivt makta i Rosenborg og de bruker den. Dette er et gedigent nederlag for det sittende styret og leder Cecilie Gotaas Johnsen, sier Adresseavisens kommentator Birger L\u00f8faldli til VG.  \u2013 S\u00e6rlig investorsaken tror jeg er tung \u00e5 svelge, der det forel\u00f8pig kun var snakk om en utredning. Jeg er spent p\u00e5 hvordan Gotaas Johnsen vil reagere p\u00e5 dette og hvordan hun vurderer arbeidsbetingelsene det kommende \u00e5ret, sier L\u00f8faldli.  VAR-debatten var den som tok lengst tid:  \u2013 Jeg har forst\u00e5else for klubbens posisjon og forst\u00e5r at m\u00e5ten oppleves som uvanlig detaljstyrende. Men for mange er dette en ekstraordin\u00e6r sak. Det er viktig at styret forst\u00e5r: VAR m\u00e5 ikke forbedres, VAR m\u00e5 fjernes! sa forslagsstiller Ole Christian Gullv\u00e5g.  \u2013 Talelista begynner \u00e5 bli lang, var meldingen fra ordstyrer etter at et par stykker hadde snakket sin side i VAR-saken.  Styremedlem By Rise argumenterte med at det ville bli vanskelig \u00e5 \u00absette tannkremen tilbake p\u00e5 tuben\u00bb. Forslagsstiller Gullv\u00e5g svarte:  \u2013 For oss oppleves det som at noen har spr\u00f8ytet tannkrem p\u00e5 stua midt under fredagstacoen. Vi har ikke bedt om det, vil ikke ha det.  Ola By Rise har tidligere v\u00e6rt ute p\u00e5 Twitter og v\u00e6rt kritisk til VAR. Han innr\u00f8mmet ogs\u00e5 sin tvil rundt temaet.  \u2013 Det er vanskelig \u00e5 st\u00e5 her. Man m\u00e5 ikke st\u00e5 hver kamp p\u00e5 \u00d8vre \u00d8st for \u00e5 reagere p\u00e5 hvordan VAR praktiseres i dag. S\u00e5 er det ikke sikkert den blir god nok. Involveringen av supporterne burde definitivt blitt bedre. Men det er ikke sikkert det er verkt\u00f8yet som er problemet, men gjennomf\u00f8ringen, sa By Rise.  Han og Reginiussen listet opp b\u00e5de negative og positive sider ved VAR, og pekte som flere andre klubber p\u00e5 det potensielle \u00f8konomiske tapet ved \u00e5 fjerne VAR.  Styret argumenterte for at Rosenborg skulle v\u00e6re en kritisk meningsb\u00e6rer rundt videod\u00f8mming. Et titalls medlemmer tok ordet og sa seg sv\u00e6rt uenige, og til slutt var det forslaget fra medlemmene som vant frem.  RBK-medlem Emil Alm\u00e5s var forslagsstiller sammen med Gullv\u00e5rg. Han sier f\u00f8lgende til VG: \u2013 Det vi har f\u00e5tt til i norsk toppfotball de siste dagene er en seier for fotballen og en seier for medlemsdemokratiet. Ved \u00e5 takke nei til VAR, har norske supportere startet et jordskred, som kommer til \u00e5 rase gjennom fotballeuropa i \u00e5rene som kommer! Den dagen VAR er historie, skal jeg med stolthet si at jeg, og mange andre norske fotballsupportere var med p\u00e5 \u00e5 trille de f\u00f8rste steinene nedover dalsiden, sier Alm\u00e5s.  PS. En r\u00f8rt Rune Bratseth mottok tittelen som \u00e6resmedlem i Rosenborg, etter en lang karriere som spiller, sportssjef og styremedlem. - Det er veldig spesielt for meg, sa Bratseth. \",\n  \"target_text\": \"489 RBK-medlemmer stemte for \u00e5 avvikle VAR ved et m\u00f8te torsdag, med 157 mot Styremedlem Ola By Rise innr\u00f8mmet gode argumenter mot videod\u00f8mming, men argumenterte for at Rosenborg skulle v\u00e6re en kritisk stemme imot. RBK-medlem Emil Alm\u00e5s hevder \\\"norske supportere starter et jordskred\\\" mot VAR i Europa Medlemmene ga ogs\u00e5 sitt nei til at RBK-styret skulle f\u00e5 \u00abutrede ulike modeller for \u00e5 f\u00e5 kapital inn i klubben\u00bb.  \u2013 Et gedigent nederlag for det sittende styret, mener Adresseavisens kommentator Birger L\u00f8faldli \"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Gazas befolkning sultes med vilje, sier FN-ekspert: Krigen har \u00f8delagt matproduksjonen. Samtidig slippes det ikke inn nok n\u00f8dhjelp. Israel driver en aktiv politikk for \u00e5 sulte ut Gazas befolkning, mener FNs spesialrapport\u00f8r. Israel har som m\u00e5l \u00e5 begrense Gazas sivilbefolkning tilgang til mat. Det hevder FNs spesialrapport\u00f8r for retten til mat, Michael Fakhri, til The Guardian. \u2013 Det finnes ingen grunn til \u00e5 med vilje stoppe leveringen av humanit\u00e6r hjelp eller \u00f8delegger sm\u00e5 fiskeb\u00e5ter, drivhus og frukt\u00e5kere, bortsett fra \u00e5 nekte folk tilgang til mat, sier Fakhri til den britiske avisen. Han mener at Israel med dette gj\u00f8r seg skyldig i b\u00e5de krigsforbrytelser og folkemord. Jan Egeland: \u2013 Fullstendig galskap Sentrale israelske politikere er flere ganger blitt anklaget for \u00e5 ha brukt retorikk som oppfordrer til folkemord. Dette ble blant annet lagt til grunn da S\u00f8r-Afrika klaget Israel inn til ICJ. \u2013 Som en menneskerettighetsekspert ved FN mener jeg at dette n\u00e5 er en folkemord-situasjon, understreker Fakhri. Fakhri er ikke den eneste som har advart om konsekvensene av hungersn\u00f8den i Gaza. En FN-rapport konkluderte nylig: Flyktninghjelpens generalsekret\u00e6r, Jan Egeland, reiste tirsdag inn i Gaza. Han beskriver rystende scener med desperate mennesker som gj\u00f8r alt i sin makt for \u00e5 kare til seg mat. \u2013 Jeg er fullstendig sjokkert over forholdene her. Folk sl\u00e5ss som ville og gale over madrasser og sekker med mat, sier Egeland til VG. \u2013 Det er fullstendig galskap at verden har latt en befolkning best\u00e5ende av stort sett helt uskyldige kvinner og barn bli utsatt for bombardement og utsulting siden midten av oktober. Hevder Israel trosser FN-domstol Situasjonen er ikke blitt bedre de siste ukene. Det sier bistandsorganisasjoner. Det til tross for at Den internasjonale domstolen (ICJ), FNs viktigste domstol, for \u00e9n m\u00e5ned siden bestemte at Israel m\u00e5 gj\u00f8re alt i sin makt for \u00e5 s\u00f8rge for \u00e5 stoppe et folkemord og s\u00f8rge for at palestinere har tilgang til bistand. Human Rights Watch (HRW) og Amnesty International p\u00e5peker at det slippes inn 30 prosent f\u00e6rre lastebiler med n\u00f8dhjelp hver dag n\u00e5 sammenlignet med f\u00f8r ICJs p\u00e5legg 26. januar. I februar slapp det inn halvparten s\u00e5 mye n\u00f8dhjelp i Gaza som m\u00e5neden f\u00f8r, if\u00f8lge FNs organisasjon for palestinske flyktninger (Unrwa). \u2013 Den israelske regjeringen sulter 2,4 millioner palestinere i Gaza.  Det sier Omar Shakir, som er lederen for HRWs virksomhet i Israel og Palestina. \u2013 Den israelske regjeringen har ganske enkelt oversett domstolens p\u00e5legg, f\u00f8yer han til. Tirsdag redegjorde Ramesh Rajasingham ved FNs kontor for koordinering av humanit\u00e6r innsats (UNOCHA) om situasjonen for FNs sikkerhetsr\u00e5d. Han advarte om at jordbruket i Gaza vil kollapse innen mai hvis situasjonen ikke blir bedre, og hvis det ikke blir pause i krigshandlingene. \u2013 Vi understreker derfor nok en gang v\u00e5rt krav om en v\u00e5penhvile, sa han. USA blokkerte i februar enda en gang en resolusjon i Sikkerhetsr\u00e5det om v\u00e5penhvile. Begrunnelsen var at resolusjonen kunne \u00f8delegge forhandlinger om v\u00e5penhvile og fangeutveksling som p\u00e5g\u00e5r mellom Egypt, Israel og Qatar. \u2013 Hvis ingenting skjer, frykter vi at storskala sult i Gaza nesten er uunng\u00e5elig, og det vil f\u00f8re til mange flere ofre, sa Rajasingham til Sikkerhetsr\u00e5det.\",\n  \"target_text\": \"FN mener Israel pr\u00f8ver \u00e5 sulte ut befolkningen p\u00e5 Gazastripen. M\u00e5lrettede angrep hindrer matproduksjon og levering av n\u00f8dhjelp.  Akutt underern\u00e6ring truer hele befolkningen. Barn og kvinner i Nord-Gaza og Rafah er mest utsatt.  Israel overser FN-domstolens p\u00e5legg om \u00e5 gi palestinere tilgang til bistand. Hjelpeorganisasjoner ser mindre n\u00f8dhjelp komme inn.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Marokkanske og albanske mafianettverk dominerer. Svenskene blir en stadig st\u00f8rre trussel.: Flere er bygd p\u00e5 lojalitet til familie og klan, if\u00f8lge ny rapport fra Kripos. Om kort tid legger politiet frem sin trusselvurdering. Der vil Politi-Norge peke p\u00e5 de st\u00f8rste truslene mot det norske samfunnet. En av truslene som vil bli viet mye plass, er organiserte kriminelle nettverk. I Norge er det rundt hundre slike nettverk. Kripos mener politiet har kapasitet til \u00e5 f\u00f8lge med p\u00e5 40 av dem. Nettverkene smugler og selger enorme mengder narkotika. De st\u00e5r bak skyteepisoder, eksplosjoner, menneskesmugling og bedragerier. M\u00e5let er profitt. Midlene er vold og hard indre justis. Noen av de mektigste nettverkene er bygd p\u00e5 lojalitet til familie og klan. N\u00e5 letter Kripos p\u00e5 sl\u00f8ret. For f\u00f8rste gang g\u00e5r politiet ut med en egen rapport om nettverkene som dominerer i den kriminelle underverdenen: I rapporten trekker Kripos frem fem store trusler: 1. Marokkanske narkonettverk En av de aller st\u00f8rste truslene er marokkanske narkonettverk. \u2013 De er utrolig sentrale, ikke bare i Norge og Norden, sier Eivind Borge fra Kripos. Norskmarokkanere dukker ogs\u00e5 opp i etterforskninger i andre europeiske land. Aftenposten har tidligere omtalt Zakariya Rahali, som har v\u00e6rt p\u00e5 r\u00f8mmen siden 2017. Rahali er pekt ut som lederen av Norges st\u00f8rste narkonettverk. 2. Albanske narkonettverk Etter marokkanerne, er det albanske nettverk som utgj\u00f8r den st\u00f8rste trusselen. Disse regnes for \u00e5 v\u00e6re blant de st\u00f8rste nettverkene som driver med kokain i hele Europa.  3. Svenske narkonettverk Borges skrekkscenario er at Norge kommer dit Sverige er i dag. Der har gjengkrigen herjet og deler av samfunnet er i ferd med \u00e5 bli infiltrert av kriminelle. I Norge har samtlige politidistrikt st\u00f8tt p\u00e5 svenske kriminelle nettverk. Og trusselen er \u00f8kende, vurderer Kripos. 4. Litauiske kriminelle nettverk For \u00e5 frakte narkotika, trengs det logistikk. For \u00e5 gj\u00f8re dette, tar mange kriminelle i bruk litauiske nettverk.  5. Norge som transittland I fjor opplevde Europa en \u00abkokaintsunami\u00bb. Enorme mengder kokain ble tatt av politi og tollere, ogs\u00e5 i Norge. Men prisene gikk ikke opp. Et tegn p\u00e5 at store mengder kokain er i oml\u00f8p.  I flere \u00e5r har havnene i Rotterdam og Antwerpen v\u00e6rt stedet hvor kokain er blitt smuglet inn til Europa. Men der har myndighetene kastet seg rundt. Dermed m\u00e5 de kriminelle se seg om etter nye havner for \u00e5 f\u00e5 det hvite pulveret til kundene. De store beslagene i fjor, kan peke mot at Norge i st\u00f8rre grad er i ferd med \u00e5 bli et av disse stedene. Enn s\u00e5 lenge er det for tidlig \u00e5 konkludere om Norge er blitt en del av kokainruten til Europa, mener Borge og Ole J\u00f8rgen Arvesen, avdelingsleder med ansvar for etterretning i Kripos. G\u00e5r sammen med kartellene Hvordan kan Kripos v\u00e6re s\u00e5 sikre i sin sak? Mye kommer fra p\u00e5g\u00e5ende etterforskninger, men de siste \u00e5rene har de ogs\u00e5 f\u00e5tt et unikt innblikk i hvordan de kriminelle jobber og samarbeider. De har f\u00e5tt meldinger og bilder fra Encrochat, Sky ECC og Anom. Det har ledet til flere store saker, men likevel er trusselen fra de kriminelle nettverkene blitt st\u00f8rre. \u2013 Den er betydelig og \u00f8kende for hele Europa, ogs\u00e5 Norge, sier Arvesen. Nettverkene er blitt mer profesjonelle og samarbeider mer med kriminelle i andre land.  \u2013 Vi ser tydelig at norske nettverk har direkte kontakt med karteller i S\u00f8r-Amerika, sier Eivind Borge fra Kripos. Han sier bakmennene de jobber for \u00e5 ta, ikke lar seg stoppe med forebygging. Det krever mye etterforskning og samarbeid med politi i andre land.\",\n  \"target_text\": \"For f\u00f8rste gang g\u00e5r politiet ut med en egen rapport om kriminelle nettverk. Rapporten peker p\u00e5 fem store trusler: marokkanske og albanske narkonettverk, svenske narkonettverk, litauiske kriminelle nettverk og at Norge blir et transittland for kokain. Nettverkene i Norge er blitt mer profesjonelle, har direkte kontakt med karteller i S\u00f8r-Amerika. Dette krever mer etterforskning og internasjonalt samarbeid.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset schibsted-no\n</code></pre>"},{"location":"datasets/norwegian/#unofficial-personal-sum","title":"Unofficial: Personal Sum","text":"<p>This dataset was released here and contains human annotated summaries that reflect individual user preferences.</p> <p>The original dataset contains 1,099 summaries based on 441 unique articles. The dataset has been restructured into 441 samples, where each sample represents a unique article paired with all of its corresponding summaries (1 or more). The dataset has been split such that we have 121 / 64 / 256 samples for training, validation and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n    \"text\": \"I en ny bok forteller Abid Rajas s\u00f8ster Abida Raja (49) at hun over lengre tid levde i et voldelig forhold. I en pressemelding avviser eksmannen anklagene. \u2013 Min klient \u00f8nsker \u00e5 p\u00e5peke at han nekter straffeskyld for partnervold og\\nvoldtektsanklager. Han vedkjenner at ekteskapet har hatt sine utfordringer, og at de derfor skilte seg i 2015, skriver eksmannens advokat Javeed H. Shah i en pressemelding. I boken \u00abFrihetens \u00d8yeblikk\u00bb, beskriver Raja at eksmannen hennes var voldelig, og at hun flere ganger fors\u00f8kte \u00e5 unnslippe mannen. I boken skriver forfatter H\u00e5kon F. H\u00f8ydal:\u00abDe siste tjue \u00e5rene hadde v\u00e6rt en kamp mot seg selv: Hun \u00f8nsket \u00e5 g\u00e5 fra mannen. Men hun m\u00e5tte bli. P\u00e5 grunn av barna, og p\u00e5 grunn av familien, p\u00e5 grunn av frykten for fattigdom og skam. N\u00e5 hadde hun verken barna, penger eller hus.\u00bbVG har tidligere v\u00e6rt i kontakt med Abida Rajas eksmann i forbindelse med bokutgivelsen, som tirsdag ikke hadde lest boken.\u2013 Jeg er i utlandet og har ikke lest boken, s\u00e5 kan ikke kommentere uten \u00e5 lese det, skriver han i en SMS til VG.I boka skriver forfatteren at Abida etter stort press fra familien, skal ha m\u00f8tt \u00e9n av ektemannkandidatene, en 23 \u00e5r gammel inngiftet onkel i Pakistan. Hun var 18 \u00e5r og skulle g\u00e5tt i andre klasse p\u00e5 videreg\u00e5ende hjemme i Norge.\u00abAbida husker ikke om hun sa ja. Men hun sa heller ikke nei. Hun ville bare bort\u00bb, heter det i boken.Onsdag svarer eksmannen via sin advokat, at han har levd i god tro om at Abida giftet seg av fri vilje slik hun selv uttrykte ovenfor han. \u2013 Derfor er opplysningene om tvangsekteskap noe han ble kjent med f\u00f8rst i 2020. Boken kommer ett \u00e5r etter at venstrepolitiker og tidligere statsr\u00e5d Abid Raja kom med sin bok\\xa0\u00abMin skyld\u00bb. Boken er skrevet av VG-journalist H\u00e5kon F. H\u00f8ydal og ble lansert tirsdag morgen\\xa0etter mye hemmelighold. VG har ikke hatt noe med utgivelsen \u00e5 gj\u00f8re.\",\n    \"target_text\": [\"I en ny bok forteller Abid Rajas s\u00f8ster Abida Raja om hennes erfaringer med et voldelig ekteskap, hvor hun beskriver flere fors\u00f8k p\u00e5 \u00e5 unnslippe. Eksmannen avviser anklagene og hevder at han levde i god tro om at ekteskapet var av fri vilje, noe han f\u00f8rst ble klar over i 2020.\",\n    \"Abida Raja beskriver i en ny bok et voldelig forhold med sin eksmann, som avviser anklagene om partnervold og voldtektsanklager. Boken avsl\u00f8rer ogs\u00e5 at Abida ble presset til \u00e5 m\u00f8te en ektemannkandidat i en tvangssituasjon, noe eksmannen hevder han ikke var klar over f\u00f8r i 2020.\",\n    \"I boken \u00abFrihetens \u00f8yeblikk\u00bb forteller forfatteren H\u00e5kon F. H\u00f8ydal at Rajas eksmann var voldelig og hun \u00f8nsket \u00e5 forlate ham. Hun ble v\u00e6rende fordi hun var redd for barnas lidelser, redd for fattigdom og hun skammet seg.\"]\n}\n</code></pre> <pre><code>{\n    \"text\": \"Flere lakseaksjer falt igjen tungt, dagen etter at skatteforslag ga b\u00f8rsras for sj\u00f8matselskaper. Samtidig steg Norwegian etter anbefaling fra storbank.Det Ble en noe vinglete dag p\u00e5 Oslo B\u00f8rs torsdag.Etter en positiv start vendte B\u00f8rsen snuten nedover i tidlig handel, f\u00f8r den hentet seg inn igjen til forsiktig oppgang omtrent halvveis ut i handelsdagen. Utover ettermiddagen snudde B\u00f8rsen s\u00e5 nedover igjen.Hovedindeksen endte til slutt dagen ned 1,58 prosent.Nedgangen tiltok den siste timen med handel, samtidig som Wall Street falt kraftig.Oljeprisen steg solid gjennom g\u00e5rsdagen, og handles rundt \u00e9n dollar h\u00f8yere enn da B\u00f8rsen stengte onsdag. Et fat Nordsj\u00f8olje (brent spot) koster ved stengetid torsdag 88,4 dollar, ned rundt 0,9 prosentsiden midnatt.Oljeselskapene Equinor og Aker BP falt i overkant av \u00e9n prosent, mens V\u00e5r Energi endte ned 3,82 prosent.Onsdag falt Hovedindeksen 2,76 prosent etter at lakseselskapene fikk gjennomg\u00e5 etter regjeringens foresl\u00e5tte grunnrenteskatt p\u00e5 havbruk. Verst gikk det for Salmar som stupte 30 prosent, samtidig som Ler\u00f8y Seafood falt 27,5 prosent. Torsdag fortsetter nedgangen for lakseaksjene. Sj\u00f8matindeksen endte ned 5,05 prosent.Slik s\u00e5 det ut for lakseaksjene ved stengetid (utvikling onsdag i parentes): Salmar falt 1,05 prosent (stupte 30,3 prosent)Grieg Seafood falt 2,75 prosent (falt 26,6 prosent)Mowi falt 3,15 prosent (falt 18,9 prosent) Ler\u00f8y Seafood falt 8,10 prosent (raste 27,5 prosent)Austevoll Seafood falt 6,28 prosent (falt 21,7 prosentNorway Royal Salmon falt 8,94 prosent (endte ned 22,9 prosent)Bakkafrost-aksjen falt samtidig 12,83 prosent.Selskapet har virksomhet p\u00e5 F\u00e6r\u00f8yene og understreket onsdag at de ikke p\u00e5virkes av det nye norske skatteforslaget. Samtidig understreket de at det arbeides med et forslag om justeringer av skattesatsen p\u00e5 F\u00e6r\u00f8yene.I USA peker pilene solid nedover p\u00e5 b\u00f8rsene torsdag ettermiddag.Det er kraftig nedgang p\u00e5 Wall Street, der den brede S&amp;P 500-indeksen faller godt over to prosent. Teknologiindeksen Nasdaq faller samtidig mer enn tre prosent.I Europa er det ogs\u00e5 bred, kraftig nedgang p\u00e5 de viktigste b\u00f8rsene. London-b\u00f8rsen, Frankfurt-b\u00f8rsen og Paris-b\u00f8rsen er alle ned i overkant av to prosent rundt stengetid i Oslo.Storbanken HSBC har gjenopptatt dekning p\u00e5 flyselskapet Norwegian, if\u00f8lge Bloomberg. Banken anbefaler kj\u00f8p og har satt et kursm\u00e5l p\u00e5 14,50 kroner. Dermed ser banken for seg en oppside p\u00e5 hele 119 prosent i aksjen, skriver nyhetsbyr\u00e5et. Norwegian-aksjen steg 6,81 prosent.\u2013 Nye Norwegian er en annen forretning enn den f\u00f8r pandemien, som har omstrukturert operasjonelt og \u00f8konomisk, skriver HSBC i analysen.\u2013 Den nye ledelsen har en solid strategi, en enkel og kostnadseffektiv\\nforretningsmodell med en enkelt type fly, et sterkt fokus p\u00e5 sine n\u00f8kkelmarkeder i Norden og en solid balanse og likviditet, alt innenfor et gunstig konkurranselandskap som b\u00f8r tillate ny NAS \u00e5 ta markedsandeler fra sine konkurrenter, heter det videre i analysen.Storbanken begrunner ogs\u00e5 sin nye dekning p\u00e5 flyselskapet ved at dets konkurrenter venter mye motvind og ny ettersp\u00f8rsel for Norwegian kan komme ut av det. I tillegg nevnes Norges sikkerhetsnett rundt h\u00f8ye energi- og str\u00f8mpriser.- Mens Europa st\u00e5r overfor h\u00f8y inflasjon og lav forbrukertillit, har Norge betydelig lysere utsikter med sine omfattende energiressurser, statlig finansiering og h\u00f8y inntekt per innbygger.HSBC viser ogs\u00e5 til h\u00f8y reiseettersp\u00f8rsel blant nordmenn.Fornybarselskapet Scatec er i fokus i forbindelse med at selskapet har kommet med nye m\u00e5lsetninger. Selskapet vil investere 10 milliarder kroner av egenkapitalen i nye kraftverk frem mot 2027. Investeringene har som m\u00e5l \u00e5 utvide kapasiteten med 1,5 gigawatt hvert \u00e5r i perioden. Scatec-aksjen endte dagen ned 2,93 prosentXXL er samtidig blant b\u00f8rstaperne torsdag. Aksjen til sportsbutikk-kjeden falt 11,66 prosent.\",\n    \"target_text\": [\"Lakseaksjer opplever fortsatt betydelig nedgang p\u00e5 Oslo B\u00f8rs etter regjeringens foresl\u00e5tte grunnrenteskatt p\u00e5 havbruk. Hovedindeksen endte ned 1,58 prosent, og sj\u00f8matindeksen falt ytterligere 5,05 prosent. Samtidig steg Norwegian-aksjen etter anbefaling fra HSBC, som gjenopptok dekning p\u00e5 selskapet og anbefalte kj\u00f8p med et kursm\u00e5l p\u00e5 14,50 kroner, med en forventet oppside p\u00e5 119 prosent.\"]\n}\n</code></pre> <pre><code>{\n    \"text\": \"(Minnesota Wild \u2013 St. Louis Blues 4\u20136) Mats Zuccarello (34) var sv\u00e6rt kritisk til seg selv og lagkameratene i Minnesota Wild etter nattens tap mot St. Louis Blues i 23 minusgrader foran 38.000 tilskuere.\u2013 Jeg har egentlig ikke ord. Det er pinlig n\u00e5r du har 40.000 mennesker som kommer og fryser r\u00e6va av seg, og s\u00e5 spiller vi s\u00e5nn, sa Zuccarello p\u00e5 pressekonferansen etter \u00abWinter Classic\u00bb-oppgj\u00f8ret p\u00e5 Target Field \u2013 et baseballstadion i Minneapolis. F\u00f8r siste periode ledet Blues 6\u20132, og Zuccarello beskriver de to f\u00f8rste periodene som at de ble \u00ablett utspilt\u00bb av Blues. Zuccarello hadde \u00e9n assist \u2013 da Ryan Hartman scoret lagets tredje m\u00e5l . Wild reduserte to ganger i siste periode og fastsatte sluttresultatet til 4\u20136. 34-\u00e5ringen mener det ikke nytter \u00e5 forklare tapet med kulden, vanskelige forhold og det faktum at de ikke har spilt kamp siden 20. desember: \u2013 Det er ingen unnskyldninger ... Det er kaldt for begge lag, isen er humpete for begge lag. Vi spilte ikke smart hockey som vi har gjort i store deler av sesongen. Det var Wilds femte strake tap i en sesong der Zuccarello og laget jevnt over har levert meget bra. \u2013 Dessverre skjedde det p\u00e5 en stor kveld som dette. Folk forlater hjemmene sine i kulden for \u00e5 st\u00f8tte oss, og s\u00e5 serverer vi dem dette. Vi har skuffet oss selv og alle andre. Det var p\u00e5 forh\u00e5nd varslet sprengkulde, og m\u00e5lingene viste 23 minusgrader. Zuccarello beskriver opplevelsen slik:\u2013 Jeg var skikkelig kald under oppvarmingen, men n\u00e5r kampen starter sl\u00e5r adrenalinet inn. Men jeg tror aldri jeg har v\u00e6rt s\u00e5 kald i hele mitt liv f\u00f8r sisteperioden da vi l\u00e5 under 6\u20132, eller hva det var. Det var ingen god f\u00f8lelse. \u2013 Det store bildet n\u00e5 er at vi har fem strake tap, og vi m\u00e5 finne tilbake til m\u00e5ten \u00e5 vinne p\u00e5 og hvordan vi skal spille som et lag, sier Zuccarello. Zuccarello har scoret \u00e5tte m\u00e5l og lagt 17 m\u00e5lgivende pasninger i l\u00f8pet av 25 kamper denne sesongen. Det vil si ett m\u00e5lpoeng per kamp i snitt. I sine beste m\u00e5lpoengsesonger for New York Rangers \u2013 2013/14, 2015/16 og 2016/17 \u2013 oppn\u00e5dde han henholdsvis 59 m\u00e5lpoeng p\u00e5 77 kamper, 61 m\u00e5lpoeng p\u00e5 81 kamper og 59 p\u00e5 80 kamper.PS! Natt til fredag spiller Minnesota Wild borte mot Boston Bruins. To dager senere er det hjemmekamp mot Washington Capitals.\",\n    \"target_text\": [\"Minnesota Wild led et nederlag mot St. Louis Blues under ekstreme v\u00e6rforhold p\u00e5 Target Field. Mats Zuccarello uttrykte sin skuffelse over lagets ytelse foran 38 000 tilskuere, og tilskrev tapet til d\u00e5rlig spill heller enn kulden. Til tross for Zuccarellos bidrag med en assist, endte Wild med sitt femte strake tap, noe som f\u00f8rte til et press for \u00e5 finne tilbake til seiersformen f\u00f8r kommende kamper mot Boston Bruins og Washington Capitals.\",\n    \"Det er ingen unnskyldninger for Wilds femte strake tap, til tross for at b\u00e5de Zuccarello og resten av laget generelt har spilt bra denne sesongen. Forholdene var like for begge lag, men laget spilte ikke smart hockey slik de har gjort tidligere i sesongen.\"]\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\n</code></pre></li> <li>Base prompt template:   <pre><code>Nyhetsartikkel: {text}\nSammendrag: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Nyhetsartikkel: {text}\n\nSkriv et sammendrag av den ovennevnte artikkelen.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset personal-sum\n</code></pre>"},{"location":"datasets/swedish/","title":"\ud83c\uddf8\ud83c\uddea Swedish","text":"<p>This is an overview of all the datasets used in the Swedish part of EuroEval. The datasets are grouped by their task - see the task overview for more information about what these constitute.</p>"},{"location":"datasets/swedish/#sentiment-classification","title":"Sentiment Classification","text":""},{"location":"datasets/swedish/#swerec","title":"SweReC","text":"<p>This dataset was published in this B.Sc. thesis and is a manually annotated dataset of Swedish reviews from both Trustpilot and Reco.se.</p> <p>The original dataset contains 10,757 reviews. We use a split of 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"J\u00e4ttebra och rekommenderas till alla\",\n  \"label\": \"positive\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Lugnt och trevlig st\u00e4mning, inte f\u00f6r bullrigt. god mat, lite mer variation hade \u00f6nskats p\u00e5 de varma r\u00e4tterna. trevlig personal, dock missade de att ta dryckesbest\u00e4llningar fr\u00e5n oss vilket var ett litet minus. \u00f6verlag trevlig st\u00e4lle.\",\n  \"label\": \"neutral\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Extremt d\u00e5lig mottagning - b\u00e5de gsm och 3g? samtalen bryts hela tiden och s\u00e5 tar dom betalt f\u00f6r en ny uppkopplingsavgift varje g\u00e5ng.\",\n  \"label\": \"negative\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Base prompt template:   <pre><code>Recension: {text}\nSentiment: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Recension: {text}\n\nKlassificera sentimentet i recensionen. Svara med 'positiv', 'neutral' eller 'negativ'.\n</code></pre></li> <li>Label mapping:<ul> <li><code>positive</code> \u27a1\ufe0f <code>positiv</code></li> <li><code>neutral</code> \u27a1\ufe0f <code>neutral</code></li> <li><code>negative</code> \u27a1\ufe0f <code>negativ</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset swerec\n</code></pre>"},{"location":"datasets/swedish/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"datasets/swedish/#suc-30","title":"SUC 3.0","text":"<p>This dataset, also known as the Stockholm-Ume\u00e5 Corpus 3.0, was published here and is a manually NER-annotated dataset, based on Swedish texts from the 1990s. The dataset does not follow the CONLL format, so we convert it into that format using the following mapping:</p> <ul> <li><code>animal</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>event</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>inst</code> \u27a1\ufe0f <code>ORG</code></li> <li><code>myth</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>other</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>person</code> \u27a1\ufe0f <code>PER</code></li> <li><code>place</code> \u27a1\ufe0f <code>LOC</code></li> <li><code>product</code> \u27a1\ufe0f <code>MISC</code></li> <li><code>work</code> \u27a1\ufe0f <code>MISC</code></li> </ul> <p>The dataset consists of 74,245 samples, which we split into 1,024 / 256 / 2,048 samples for training, validation, and testing, respectively.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"tokens\": array(['Det', 'l\u00e5ter', 'som', 'en', 'v\u00e4stanfl\u00e4kt', 'j\u00e4mf\u00f6rt', 'med', 'den', 'i', 'filmen', 'f\u00f6rk\u00e4ttrade', 'bilj\u00e4tten', 'General', 'Motors', ',', 'som', 'frist\u00e4llt', '35000', 'jobbare', 'i', 'staden', 'Flint', ',', 'Michigan', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['En', 'liknande', 'kunskapsteoretisk', 'grundfr\u00e5ga', ',', 'fast', 'i', 'mer', 'modernt', 'sofistikerad', 'form', ',', 'n\u00e5r', 'oss', 'nu', 'fr\u00e5n', 'Paris', ':'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], dtype=object)\n}\n</code></pre> <pre><code>{\n  \"tokens\": array(['-', 'Dessv\u00e4rre', ',', 'sa', 'man', ',', 'vi', 'har', 'ingen', 'Bj\u00f6rn', 'Eriksson', 'p\u00e5', 'passagerarlistan', '.'], dtype=object),\n  \"labels\": array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'], dtype=object)\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 8</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter som f\u00f6rekommer i den givna meningen.\n</code></pre></li> <li>Base prompt template:   <pre><code>Mening: {text}\nNamngivna entiteter: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Mening: {text}\n\nIdentifiera de namngivna enheterna i meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', 'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna enheter av den typen, precis som de f\u00f6rekommer i meningen.\n</code></pre></li> <li>Label mapping:<ul> <li><code>B-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>I-PER</code> \u27a1\ufe0f <code>person</code></li> <li><code>B-LOC</code> \u27a1\ufe0f <code>plats</code></li> <li><code>I-LOC</code> \u27a1\ufe0f <code>plats</code></li> <li><code>B-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>I-ORG</code> \u27a1\ufe0f <code>organisation</code></li> <li><code>B-MISC</code> \u27a1\ufe0f <code>diverse</code></li> <li><code>I-MISC</code> \u27a1\ufe0f <code>diverse</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset suc3\n</code></pre>"},{"location":"datasets/swedish/#linguistic-acceptability","title":"Linguistic Acceptability","text":""},{"location":"datasets/swedish/#scala-sv","title":"ScaLA-sv","text":"<p>This dataset was published in this paper and was automatically created from the Swedish Universal Dependencies treebank by assuming that the documents in the treebank are correct, and corrupting the samples to create grammatically incorrect samples. The corruptions were done by either removing a word from a sentence, or by swapping two neighbouring words in a sentence. To ensure that this does indeed break the grammaticality of the sentence, a set of rules were used on the part-of-speech tags of the words in the sentence.</p> <p>The original dataset consists of 6,026 samples, from which we use 1,024 / 256 / 2,048 samples for training, validation and testing, respectively (so 3,328 samples used in total). These splits are used as-is in the framework.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"U-l\u00e4nderna m\u00e5ste ta en genv\u00e4g f\u00f6r att komma i fatt.\",\n  \"label\": \"correct\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Undra att vi blev lite undandragna.\",\n  \"label\": \"incorrect\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det \u00e4r ocks\u00e5 att viktigt ha tillr\u00e4ckligt korta dubbar.\",\n  \"label\": \"incorrect\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 12</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\n</code></pre></li> <li>Base prompt template:   <pre><code>Mening: {text}\nGrammatisk korrekt: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Mening: {text}\n\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r.\n</code></pre></li> <li>Label mapping:<ul> <li><code>correct</code> \u27a1\ufe0f <code>ja</code></li> <li><code>incorrect</code> \u27a1\ufe0f <code>nej</code></li> </ul> </li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scala-sv\n</code></pre>"},{"location":"datasets/swedish/#reading-comprehension","title":"Reading Comprehension","text":""},{"location":"datasets/swedish/#scandiqa-sv","title":"ScandiQA-sv","text":"<p>This dataset was published in this paper and was automatically created from the Swedish part of the MKQA dataset. The MKQA dataset is based on the English Natural Questions dataset, based on search queries from the Google search engine. The questions and answers were manually translated to Swedish (and other languages) as part of MKQA, and the contexts were in ScandiQA-sv machine translated using the DeepL translation API. A rule-based approach was used to ensure that the translated contexts still contained the answer to the question, potentially by changing the answers slightly.</p> <p>The original full dataset consists of 6,810 / 500 / 500 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All validation samples in our version also belong to the original validation set, and all original test samples are included in our test set. The remaining 1,548 test samples in our version was sampled from the original training set.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"context\": \"I Freedom Cry f\u00e5r spelaren ta rollen som Ad\u00e9wal\u00e9, en frigiven slav fr\u00e5n Trinidad som blev Edward Kenways kvarterm\u00e4stare och senare medlem i Assassin Order. Ber\u00e4ttelsel\u00e4get utspelar sig 15 \u00e5r efter h\u00e4ndelserna i Assassin's Creed IV: Black Flag d\u00e4r Ad\u00e9wal\u00e9 har blivit en tr\u00e4nad l\u00f6nnm\u00f6rdare och finner sig sj\u00e4lv skeppsbruten i Saint-Domingue, d\u00e4r han st\u00e4lls \u00f6ga mot \u00f6ga med n\u00e5got av det mest brutala slaveriet i V\u00e4stindien. DLC:n \u00e4r skriven av Jill Murray, som skrev Liberation och Aveline-inneh\u00e5llet f\u00f6r Black Flag. I februari 2014 meddelades att Freedom Cry skulle sl\u00e4ppas som en frist\u00e5ende titel till PlayStation 4 och PlayStation 3 den 18 februari 2014 f\u00f6r Nordamerika och den 19 februari 2014 f\u00f6r Europa. Det sl\u00e4pptes f\u00f6r PC den 25 februari 2014.\",\n  \"question\": \"N\u00e4r sl\u00e4pptes assassin's creed freedom cry?\",\n  \"answers\": {\n    \"answer_start\": array([637]),\n    \"text\": array(['18 februari 2014'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Political history of the United Kingdom (1945\u2013present)\\n\u00c5r 1950 orsakade Koreakriget ett nytt tungt tryck p\u00e5 statskassan f\u00f6r milit\u00e4ra utgifter. Detta orsakade en bitter splittring inom Labourpartiet.  De konservativa gjorde \u00e5tstramningspolitiken till en viktig fr\u00e5ga i parlamentsvalet 1950. Labour f\u00f6rlorade det mesta av sin stora majoritet. Sv\u00e4ngningen var 3,6 % mot dem och de f\u00f6rlorade 78 platser, vilket gav Attlee en knapp majoritet i parlamentet. Ett \u00e5r senare f\u00f6rlorade Labour dock parlamentsvalet 1951 trots att det fick fler r\u00f6ster \u00e4n i valet 1945, och faktiskt fler r\u00f6ster \u00e4n det konservativa partiet.',\n  \"question\": 'Hur m\u00e5nga \u00e5r har det varit sen 1940?',\n  \"answers\": {\n    \"answer_start\": array([388]),\n    \"text\": array(['78'], dtype=object)\n  }\n}\n</code></pre> <pre><code>{\n  \"context\": 'Data link layer\\nOSI-modellen\\nper skikt\\n\\n\\n\\n\\n7.  Applikationslager[visa]\\n\\n\\nNNTP\\nSIP\\nSSI\\nDNS\\nFTP\\nGopher\\nHTTP\\nNFS\\nNTP\\nSMPP\\nSMTP\\nSNMP\\nTelnet\\nDHCP\\nNetconf\\nmer....\\n\\n\\n\\n\\n\\n\\n\\n\\n6.  Presentationslager[visa]\\n\\n\\nMIME\\nXDR\\n\\n\\n\\n\\n\\n\\n\\n\\n5.  Sessionsskikt[visa]\\n\\n\\nNamngiven pipe\\nNetBIOS\\nSAP\\nPPTP\\nRTP\\nSOCKS\\nSPDY\\n\\n\\n\\n\\n\\n\\n\\n\\n4.  Transportlager[visa]\\n\\n\\nTCP\\nUDP\\nSCTP\\nDCCP\\nSPX\\n\\n\\n\\n\\n\\n\\n\\n\\n3.  N\u00e4tverksskikt[visa]\\n\\n\\nIP\\n\\nIPv4\\nIPv6\\n\\n\\nICMP\\nIPsec\\nIGMP\\nIPX\\nAppleTalk\\nX.25 PLP\\n\\n\\n\\n\\n\\n\\n\\n\\n2.  Datal\u00e4nkskiktet[visa]\\n\\n\\nATM\\nARP\\nIS-IS\\nSDLC\\nHDLC\\nCSLIP\\nSLIP\\nGFP\\nPLIP\\nIEEE 802.2\\nLLC\\nMAC\\nL2TP\\nIEEE 802.3\\nFrame Relay\\nITU-T G.hn DLL\\nPPP\\nX.25 LAPB\\nQ.921 LAPD\\nQ.922 LAPF\\n\\n\\n\\n\\n\\n\\n\\n\\n1.  Fysiskt lager[visa]\\n\\n\\nEIA/TIA-232\\nEIA/TIA-449\\nITU-T V-serien\\nI.430\\nI.431\\nPDH\\nSONET/SDH\\nPON\\nOTN\\nDSL\\nIEEE 802.3\\nIEEE 802.11\\nIEEE 802.15\\nIEEE 802.16\\nIEEE 1394\\nITU-T G.hn PHY\\nUSB\\nBluetooth\\nRS-232\\nRS-449\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nv\\nt\\ne',\n  \"question\": 'Vilket lager av osi-modellen \u00e4r uppdelad i tv\u00e5 delskikt?',\n  \"answers\": {\n    \"answer_start\": array([0]),\n    \"text\": array(['Data link layer'], dtype=object)\n  }\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 4</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Text: {text}\nFr\u00e5ga: {question}\nSvar p\u00e5 max 3 ord: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Text: {text}\n\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med h\u00f6gst 3 ord.\n\nFr\u00e5ga: {question}\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset scandiqa-sv\n</code></pre>"},{"location":"datasets/swedish/#knowledge","title":"Knowledge","text":""},{"location":"datasets/swedish/#mmlu-sv","title":"MMLU-sv","text":"<p>This dataset is a machine translated version of the English MMLU dataset and features questions within 57 different topics, such as elementary mathematics, US history and law. The translation to Swedish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 269 / 1,410 / 13,200 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). These splits are new and there can thus be some overlap between the original validation and test sets and our validation and test sets.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Varf\u00f6r \u00e4r tidpunkten f\u00f6r monumental byggnation vid Ceibal signifikant?\\nSvarsalternativ:\\na. Det mots\u00e4ger hypotesen att den monumental byggnationen av Maya i huvudsak inspirerades av Olmekerna.\\nb. Det bekr\u00e4ftar att inv\u00e5narna i Ceibal inspirerades av Olmekerna f\u00f6r att bygga stora plattformar.\\nc. Det mots\u00e4ger hypotesen att utvecklingen av monumental byggnation bland Maya var en intern process.\\nd. Det bekr\u00e4ftar att Olmekerna, som byggde de flesta Maya-monumenten, inspirerades av egyptierna.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Vilken populationsstatistik visar f\u00f6delsetalet vid vilket en befolkning precis f\u00e5r tillr\u00e4ckligt med f\u00f6dslar f\u00f6r att ers\u00e4tta f\u00f6r\u00e4ldrarna och kompensera f\u00f6r tidiga d\u00f6dsfall?\\nSvarsalternativ:\\na. R\u00e5 f\u00f6delsetal\\nb. Ers\u00e4ttningstal\\nc. D\u00f6dlighetstal\\nd. Total fertilitetstal\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En subenhet av DNA och protein som best\u00e5r av 134-baspar l\u00e5nga str\u00e4ckor av DNA som omger en proteinoktomer kallas (a)\\nSvarsalternativ:\\na. histon\\nb. kromatin\\nc. nukleosom\\nd. solenoid\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset mmlu-sv\n</code></pre>"},{"location":"datasets/swedish/#unofficial-arc-sv","title":"Unofficial: ARC-sv","text":"<p>This dataset is a machine translated version of the English ARC dataset and features US grade-school science questions. The translation to Swedish was done by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 1,110 / 297 / 1,170 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 1,024 split for training, validation and testing, respectively (so 2,304 samples used in total). All new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"En typ av f\u00e5gel i Afrika \u00e4ter blodsugande insekter fr\u00e5n stora d\u00e4ggdjur. Vilket ord beskriver b\u00e4st relationen mellan f\u00e5geln och d\u00e4ggdjuren?\\nSvarsalternativ:\\na. mutualism\\nb. parasitism\\nc. neutralism\\nd. kommensalism\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Mr. Pratt g\u00f6r en vetenskaplig demonstration. Han bl\u00e5ser upp en ballong, placerar den i en frys och tar sedan ut den efter 10 minuter. Vilket alternativ beskriver b\u00e4st ballongens volym n\u00e4r den \u00e4r i frysen och efter att den har tagits ut och \u00e5ter till\u00e5tits att v\u00e4rmas upp?\\nSvarsalternativ:\\na. expanderar i frysen och kontraherar sedan n\u00e4r den blir varmare igen\\nb. kontraherar i frysen och expanderar sedan n\u00e4r den blir varmare igen\\nc. expanderar i frysen och h\u00e5ller sedan den volymen n\u00e4r den v\u00e4rms upp\\nd. kontraherar i frysen och h\u00e5ller sedan den volymen n\u00e4r den v\u00e4rms upp\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"En elev tills\u00e4tter vatten och reng\u00f6ringsmedel till en kopp med jord. Blandningen skakas och till\u00e5ts s\u00e4tta sig. Eleven observerar att silt-partiklar f\u00f6rblir uppsuspenderade l\u00e5ngt efter att de andra partiklarna bildar lager p\u00e5 botten av beh\u00e5llaren. Den mest troliga f\u00f6rklaringen \u00e4r att silt-partiklarna \u00e4r\\nSvarsalternativ:\\na. organiska.\\nb. uppl\u00f6sta.\\nc. mindre t\u00e4tt packade.\\nd. r\u00f6r sig snabbare.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset arc-sv\n</code></pre>"},{"location":"datasets/swedish/#common-sense-reasoning","title":"Common-sense Reasoning","text":""},{"location":"datasets/swedish/#hellaswag-sv","title":"HellaSwag-sv","text":"<p>This dataset is a machine translated version of the English HellaSwag dataset. The original dataset was based on both video descriptions from ActivityNet as well as how-to articles from WikiHow. The dataset was translated by the University of Oregon as part of this paper, using GPT-3.5-turbo.</p> <p>The original full dataset consists of 9,310 samples. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total).</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"[header] Hur man hittar de perfekta brudt\u00e4rnekl\u00e4nningarna [title] Internet \u00e4r en underbar resurs f\u00f6r att hitta brudt\u00e4rnekl\u00e4nningar. [step] Vi rekommenderar ocks\u00e5 att bl\u00e4ddra genom popul\u00e4ra br\u00f6llopstidningar, s\u00e5som brudens och moderna brudt\u00e4rnets tidningar. Rekommenderat \u00e4r att bruden g\u00e5r och handlar med en eller tv\u00e5 av sina brudt\u00e4rnor och ser vilka stilar de gillar.\\nSvarsalternativ:\\na. N\u00e4r du har begr\u00e4nsat urvalet kan du sedan f\u00e5 input fr\u00e5n dina andra brudt\u00e4rnor om du \u00f6nskar det. [title] Vilka \u00e4r de senaste trenderna i brudt\u00e4rnekl\u00e4nningar? [title] A-linje kl\u00e4nningar som ser bra ut p\u00e5 alla olika kroppsformer och storlekar \u00e4r mycket popul\u00e4ra.\\nb. Tyv\u00e4rr kan du inte handla lika ofta som om du letade efter matchade brudt\u00e4rnor. [title] N\u00e4r du v\u00e4ljer din brud, v\u00e4lj tre olika stilar: [step] Klipp l\u00e4ngd, klipp tjocklek och fr\u00e5n de flesta \\\"f\u00f6r-skjutna\\\" stilarna till de grundl\u00e4ggande.\\nc. Medan varje brud \u00e4r annorlunda, alla \u00e4r b\u00e5de olika och har olika smaker. [title] Se om bruden har en favoritlook f\u00f6r sin br\u00f6llopskl\u00e4nning.\\nd. [title] B\u00f6rja s\u00f6ka efter id\u00e9er eller allm\u00e4nna \u00e5sikter om s\u00e4rskilda br\u00f6llopskl\u00e4nningar. [step] F\u00f6rs\u00f6k att inte bli f\u00f6r stel och s\u00f6k bara efter n\u00e5gra kl\u00e4nningar som du tror kan fungera bra tillsammans.\",\n  \"label\": \"a\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"[header] Hur man g\u00f6r en pedikyr [title] Ta bort all befintlig f\u00e4rg med nagellacksborttagare. [step] T\u00e4ck toppen p\u00e5 din nagellacksborttagare med en bomullstuss, v\u00e4nd snabbt upp och ner den och omedelbart upp och ner igen f\u00f6r att applicera lite av produkten. Gnugga sedan nagellacksborttagaren \u00f6ver dina t\u00e5naglar f\u00f6r att ta bort f\u00e4rgen.\\nSvarsalternativ:\\na. [title] L\u00e5t dina t\u00e5naglar bl\u00f6tl\u00e4ggas i vatten i 10 till 20 minuter. [step] Vatten kan g\u00f6ra dina naglar vitare genom att l\u00f6sa upp andra f\u00f6reningar, s\u00e4rskilt syror.\\nb. [substeps] Flytta bomullstussen i sm\u00e5, cirkul\u00e4ra r\u00f6relser om du har sv\u00e5rt att ta bort f\u00e4rgen. [title] Fyll en fotspa eller en balja med varmt vatten.\\nc. [substeps] Om du inte har nagellacksborttagare kan du \u00f6verv\u00e4ga att anv\u00e4nda den vita nagellacksborttagaren fr\u00e5n f\u00f6reg\u00e5ende steg f\u00f6r en enklare applikation. [title] T\u00e4ck dina h\u00e4nder med bandage eller tejp med canvas-lining.\\nd. [title] Anv\u00e4nd aceton p\u00e5 dina t\u00e5naglar. [step] Aceton kan verkligen hj\u00e4lpa till att ta bort gammalt nagellack fr\u00e5n dina naglar.\",\n  \"label\": \"b\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Han forts\u00e4tter att klippa gr\u00e4set. Kameran fokuserar p\u00e5 det rinnande vattnet igen. Den g\u00e5r tillbaka till mannen som klipper gr\u00e4set. sedan\\nSvarsalternativ:\\na. den g\u00e5r tillbaka till filmen av mannen som klipper jord.\\nb. \u00e5terv\u00e4nder till honom och dem som pratar igen.\\nc. v\u00e4xlar tillbaka till det rinnande vattnet.\\nd. m\u00f6rk himmel igen.\",\n  \"label\": \"c\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 5</li> <li>Prefix prompt:   <pre><code>F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\n</code></pre></li> <li>Base prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\nSvar: {label}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Fr\u00e5ga: {text}\nSvarsalternativ:\na. {option_a}\nb. {option_b}\nc. {option_c}\nd. {option_d}\n\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' eller 'd', och inget annat.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset hellaswag-sv\n</code></pre>"},{"location":"datasets/swedish/#summarization","title":"Summarization","text":""},{"location":"datasets/swedish/#swedn","title":"SweDN","text":"<p>This dataset was published in this paper and are based on news articles from the Swedish newspaper Dagens Nyheter, with the summaries being the first paragraph of the article (and that paragraph being removed from the article).</p> <p>The original dataset consists of 29,800 / 4,530 / 3,750 samples for training, validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training, validation and testing, respectively (so 3,328 samples used in total). All the new splits are subsets of the original splits.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Ett \u00f6verraskande ras p\u00e5 den ryska lastbilsmarknaden har gjort att Scania blivit fr\u00e5nsprunget av konkurrenten Volvo som \u00f6kat sina leveranser, skriver Dagens Industri. Bakom Scanias tapp p\u00e5 24 procent ligger bland annat problem med tillst\u00e5nden f\u00f6r att producera Euro-3 lastbilar i fabriken i S:t Petersburg. Men det r\u00e4knar Scanias Rysslandschef Hans Tardell med att ta tillbaka under \u00e5ret. Konkurrenten Volvo, som \u00f6kat leveranserna med 40 procent och ordering\u00e5ngen med 68 procent j\u00e4mf\u00f6rt mot f\u00f6rsta kvartalet 2011, hoppas kunna v\u00e4xa ytterligare.  \",\n  \"target_text\": \"Ett \u00f6verraskande ras p\u00e5 den ryska lastbilsmarknaden har gjort att Scania blivit fr\u00e5nsprunget av konkurrenten Volvo som \u00f6kat sina leveranser, skriver Dagens Industri.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Scenen som beskrivs i \u00e5talet kunde vara h\u00e4mtad ur en skr\u00e4ckfilm. Den d\u00e5 tolv\u00e5riga flickan har ber\u00e4ttat hur hon f\u00f6rs\u00e5gs med handbojor och kedjades vid en krok i taket. Enligt \u00e5talet ska hon \u00e4ven ha f\u00e5tt ett koppel kring halsen och piskats. \u00c5klagaren menar att det handlar om ett utdraget f\u00f6rlopp. \u2013 En tolv\u00e5rig flicka ska inte sitta fastsatt i en krok i taket, s\u00e4ger \u00e5klagare Daniel Veivo Pettersson, som nu har \u00e5talat en 25-\u00e5rig man f\u00f6r grov v\u00e5ldt\u00e4kt mot barn. I veckan ber\u00e4ttade TT att sju m\u00e4n d\u00f6mts f\u00f6r att vid olika tillf\u00e4llen ha utsatt samma flicka f\u00f6r sexuella \u00f6vergrepp. M\u00e4nnen fick kontakt med flickan via forum p\u00e5 n\u00e4tet och tjatade sig till tr\u00e4ffar med henne. En av m\u00e4nnen band och v\u00e5ldtog henne i en skog. 25-\u00e5ringen blir nu den \u00e5ttonde mannen som \u00e5talas f\u00f6r \u00f6vergrepp. \u2013 Man h\u00e4pnar n\u00e4r man h\u00f6r hennes ber\u00e4ttelse. Hon \u00e4r mycket trov\u00e4rdig och vi har \u00e4ven kunnat styrka \u00e5talen mot m\u00e4nnen genom teknisk bevisning som chattkonversationer och i n\u00e5got fall fanns dna p\u00e5 en kondom och p\u00e5 en bh, s\u00e4ger Daniel Veivo Pettersson. Vid en husrannsakan i 25-\u00e5ringens hem i Stockholm, d\u00e4r v\u00e5ldt\u00e4kten ska ha beg\u00e5tts under h\u00f6sten 2013, hittades kedjor, handbojor, koppel och en piska. Enligt flickan hade delar av \u00f6vergreppen filmats. Polisen misst\u00e4nkte att filmerna kunde ha sparats i en s\u00e5 kallad molntj\u00e4nst, och \u00e5klagaren fick ta hj\u00e4lp av Microsoft i USA. \u2013 Det drog ut p\u00e5 tiden, men tyv\u00e4rr hittade vi inte det vi letade efter. Han har raderat en hel del information i sin dator, s\u00e4ger Daniel Veivo Pettersson. 25-\u00e5ringen \u00e5talas dessutom f\u00f6r ytterligare en v\u00e5ldt\u00e4kt p\u00e5 flickan, eftersom han misst\u00e4nks ha v\u00e5ldtagit henne p\u00e5 en toalett. Mannen \u00e4r tidigare d\u00f6md f\u00f6r \u00f6vergrepp p\u00e5 en annan minder\u00e5rig flicka, och \u00e5klagaren har nu beg\u00e4rt honom h\u00e4ktad i sin fr\u00e5nvaro. \u2013 Han kan vara hemma, men han kan \u00e4ven vara utomlands. Om han h\u00e4ktas i sin utevaro kommer han att efterlysas, s\u00e4ger Daniel Veivo Pettersson. 25-\u00e5ringen f\u00f6rsvaras av advokat Thomas Bodstr\u00f6m. Han vill inte ber\u00e4tta om 25-\u00e5ringen kommer n\u00e4rvara vid h\u00e4ktningsf\u00f6rhandlingen, men han s\u00e4ger: \u2013 Han nekar till samtliga brott, \u00e4r helt oskyldig och det finns ingen grund f\u00f6r h\u00e4ktning. Enligt \u00e5klagaren misst\u00e4nks flickan ha utsatts av ytterligare minst en man som polisen inte har lyckats identifiera. M\u00e4nnen i h\u00e4rvan 37-\u00e5ring, \u00d6sterg\u00f6tland: V\u00e5ldt\u00e4kt mot barn och barnpornografibrott \u2013 fem \u00e5rs f\u00e4ngelse. 26-\u00e5ring, Dalarna: Sexuellt ofredande \u2013 skyddstillsyn. 29-\u00e5ring, Stockholmstrakten: V\u00e5ldt\u00e4kt mot barn (tv\u00e5 tillf\u00e4llen) \u2013 tre \u00e5rs f\u00e4ngelse. 26-\u00e5ring, Stockholmstrakten: V\u00e5ldt\u00e4kt mot barn \u2013 tv\u00e5 och ett halvt \u00e5rs f\u00e4ngelse. 27-\u00e5ring, Stockholmstrakten: Grov v\u00e5ldt\u00e4kt mot barn och v\u00e5ldt\u00e4kt mot barn (fyra tillf\u00e4llen) \u2013 sju \u00e5rs f\u00e4ngelse. 55-\u00e5ring, \u00d6sterg\u00f6tland: Utnyttjande av barn f\u00f6r sexuell posering (elva tillf\u00e4llen) och sexuellt ofredande (tv\u00e5 tillf\u00e4llen) \u2013 \u00e5tta m\u00e5naders f\u00e4ngelse. 19-\u00e5ring, V\u00e4stra G\u00f6taland: V\u00e5ldt\u00e4kt mot barn \u2013 \u00e5tta m\u00e5naders f\u00e4ngelse (domen \u00e4r \u00f6verklagad). 25-\u00e5ring, Stockholmstrakten: \u00c5talad f\u00f6r grov v\u00e5ldt\u00e4kt mot barn och v\u00e5ldt\u00e4kt mot barn. \",\n  \"target_text\": \"Den tolv\u00e5riga flickan kedjades vid en krok i taket och v\u00e5ldtogs. En 25-\u00e5rig man har nu \u00e5talats f\u00f6r grov v\u00e5ldt\u00e4kt mot barn, men det \u00e4r oklart var han \u00e4r. Sju m\u00e4n d\u00f6mdes nyss f\u00f6r \u00f6vergrepp p\u00e5 samma flicka.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Det \u00e4r Gr\u00f6na partiets ledare Jill Stein som har uppmanat valkommissionen i delstaten Wisconsin att r\u00e4kna om r\u00f6sterna, det skriver Reuters och Wisconsins valkommission. Valkommissionen skriver att man \u201dr\u00e4knar med att omr\u00e4kningen b\u00f6rjar inom en vecka efter det att Steins kampanj har betalat avgiften omr\u00e4kningen, som vi fortfarande h\u00e5ller p\u00e5 att ber\u00e4kna\u201d. En omr\u00e4kning ska vara genomf\u00f6rd f\u00f6re den 13 december. Delstaten vanns av Donald Trump med 47,9 procent av r\u00f6sterna mot Hillary Clintons 46,9 procent och gav honom 10 elektorsr\u00f6ster. Skillnaden mellan de tv\u00e5 kandidaterna var 23.000 r\u00f6ster. Jill Stein har tidigare sagt att hon \u00e4r beredd att \u00e4ven f\u00f6rs\u00f6ka f\u00e5 r\u00f6sterna i Michigan och Pennsylvania omr\u00e4knade. Om hon ska beg\u00e4ra en omr\u00e4kning ocks\u00e5 i dessa tv\u00e5 delstater m\u00e5ste den beg\u00e4ran inkomma under n\u00e4sta vecka, skriver NBC News. Jill Stein. Foto: AP F\u00f6r att f\u00e5 till st\u00e5nd en omr\u00e4kning m\u00e5ste Gr\u00f6na partiet ha pengar nog att driva en s\u00e5dan. Enligt Washington Post har partiet lyckats samla in 4,5 miljoner dollar som ska t\u00e4cka juridiska omkostnader och annat som har med en eventuell omr\u00e4kning att g\u00f6ra i de tre delstaterna. Enligt tidningen kommer det sannolikt att beh\u00f6vas sammanlagt mellan 6 och 7 miljoner f\u00f6r att genomf\u00f6ra en omr\u00e4kning. Om Clinton skulle g\u00e5 segrande ur en omr\u00e4kning i Wisconsin skulle detta \u00e4nd\u00e5 inte inneb\u00e4ra n\u00e5gon skillnad n\u00e4r det g\u00e4ller utg\u00e5ngen av presidentvalet. Skulle Clinton vinna \u00e4ven i Michigan och Pennsylvania skulle det d\u00e4remot betyda en annan utg\u00e5ng av valet. \u00c4ven om f\u00e5 tror att en omr\u00e4kning skulle betyda n\u00e5got i praktiken, Hillary Clinton har redan erk\u00e4nt sig besegrad, s\u00e5 skulle en omr\u00e4kning i hennes fav\u00f6r i Wisconsin och Pennsylvania ge henne 30 elektorsr\u00f6ster medan Trump f\u00f6rlorar lika m\u00e5nga. Om s\u00e5, rent hypotetiskt, skulle bli fallet, skiljer bara 10 elektorsr\u00f6ster till Trumps f\u00f6rdel \u2013 och d\u00e5 \u00e5terst\u00e5r \u00e4nnu Michigans r\u00f6ster att slutr\u00e4knas. Skulle Clinton vinna \u00e4ven dem s\u00e5 har hon flest antal elektorsr\u00f6ster. Jill Stein har i en intervju sj\u00e4lv sagt att hon inte beg\u00e4r en omr\u00e4kning f\u00f6r att gynna n\u00e5gon av kandidaterna utan f\u00f6r att \u201damerikanerna inte blev s\u00e4rskilt glada \u00f6ver utg\u00e5ngen av valet\u201d. Sett till enbart r\u00f6sterna, och inte till elektorerna, leder just nu Hillary Clinton med 48,1 procent av r\u00f6sterna mot Donald Trumps 46,6 procent. I antal r\u00f6ster leder Clinton med 2.012.331 r\u00f6ster. \",\n  \"target_text\": \"Valkommissionen i Wisconsin i har f\u00e5tt en uppmaning om att r\u00f6sterna i presidentvalet ska r\u00e4knas om. Wisconsin har nu b\u00f6rjat f\u00f6rbereda en omr\u00e4kning. Och det kan bli fler.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSammanfattning: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSkriv en sammanfattning av artikeln ovan.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset swedn\n</code></pre>"},{"location":"datasets/swedish/#unofficial-schibsted-sv","title":"Unofficial: Schibsted-sv","text":"<p>This dataset was published here and features summaries of news articles from Schibsted Medias Swedish newsroom, from Aftonbladet.</p> <p>The original dataset has 528 / 96 / 89 samples for training, validation and testing, respectively. We use these splits as-is.</p> <p>Here are a few examples from the training split:</p> <p><pre><code>{\n  \"text\": \"Richard Jomshof blir uppr\u00f6rd och v\u00e4grar svara p\u00e5 fr\u00e5gor: SD-toppen Richard Jomshof v\u00e4grar kommentera kritiken efter p\u00e5hoppet p\u00e5 Daniel Riazat (V).  N\u00e4r Aftonbladet m\u00f6ter honom i riksdagen blir han uppr\u00f6rd och g\u00e5r iv\u00e4g. \u2013 Jag uppskattar inte skjutj\u00e4rnsjournalistik, det \u00e4r ett oseri\u00f6st s\u00e4tt att jobba, s\u00e4ger han.  Justitieutskottets ordf\u00f6rande Richard Jomshof (SD) f\u00e5r h\u00e5rd kritik f\u00f6r sitt uttalande att V-ledamoten Daniel Riazat borde flytta fr\u00e5n Sverige.  Flera i den politiska oppositionen d\u00f6mer ut det som rasistiskt. \u00c4ven i Tid\u00f6partierna h\u00f6rs protester.  \u201d\u00c4r man svensk medborgare s\u00e5 \u00e4r man. Skamligt var ordet!\u201d skriver L-politikern Jan J\u00f6nsson i ett uttalande p\u00e5 X.  \u201dTa det med pressavdelningen\u201d Aftonbladet var p\u00e5 plats utanf\u00f6r justitieutskottets m\u00f6te i riksdagen vid lunchtid p\u00e5 tisdagen. Jomshof anl\u00e4nde f\u00f6rst av alla ledam\u00f6ter, tio minuter innan m\u00f6tet inleddes, men ville inte svara p\u00e5 fr\u00e5gor.  \u2013 Du f\u00e5r ta det med pressavdelningen. Varf\u00f6r vill du inte svara, det \u00e4r ju du som har skrivit de h\u00e4r tweetsen? \u2013 Du f\u00e5r ta det med pressavdelningen. Du kan l\u00e4sa min senaste tweet f\u00f6rresten, s\u00e5 kan vi utg\u00e5 fr\u00e5n den. Varf\u00f6r tycker du att han borde l\u00e4mna Sverige? \u2013 B\u00f6rja med att l\u00e4sa min tweet, det framg\u00e5r v\u00e4ldigt tydligt d\u00e4r. \u201dUppskattar inte skjutj\u00e4rnsjournalistik\u201d Inl\u00e4gget som Jomshof syftar p\u00e5 lades upp kort innan justitieutskottets m\u00f6te. Jomshof g\u00e5r d\u00e4r till nytt angrepp mot Riazat. Han anklagar honom f\u00f6r att ha ett \u201dsunkigt\u201d beteende, att vara of\u00f6rsk\u00e4md och komma med aggressiva p\u00e5hopp p\u00e5 politiska motst\u00e5ndare.  M\u00f6tet med justitieutskottet varade en timme, n\u00e4r Richard Jomshof kom ut fr\u00e5n salen var uppr\u00f6rd \u00f6ver Aftonbladets n\u00e4rvaro. Detta trots att media brukar bevaka m\u00f6tena och att ledam\u00f6terna i utskottet ofta tar tillf\u00e4lle att ge intervjuer efter\u00e5t.  \u2013 F\u00f6r det f\u00f6rsta, vill ni prata med mig s\u00e5 g\u00e5r ni till pressavdelningen. Jag uppskattar inte skjutj\u00e4rnsjournalistik, det \u00e4r ett oseri\u00f6st s\u00e4tt att jobba. Tv\u00e5, jag har inget mer att till\u00e4gga \u00e4n det jag lagt ut p\u00e5 plattformen X. D\u00e4r framg\u00e5r det tydligt vad det h\u00e4r handlar om. Tre, ett tips i all v\u00e4nlighet, ni kan ju prata med Riazat sj\u00e4lv, om hans of\u00f6rsk\u00e4mdheter och aggressiva beteende, om varf\u00f6r han inte vill ta politiska motst\u00e5ndare och kvinnor i hand. Nu t\u00e4nker jag g\u00e5 och \u00e4ta lunch, s\u00e4ger Jomshof.  Busch: Jag \u00e4r ganska osugen Daniel Riazat kallade ig\u00e5r Richard Jomshofs uttalande f\u00f6r rasistiskt och uppmanar statsminister Ulf Kristersson (M) att ta avst\u00e5nd. Aftonbladet har s\u00f6kt Kristersson, hans pressekreterare ber att f\u00e5 \u00e5terkomma om statsministern har m\u00f6jlighet att uttala sig. Vice statsminister Ebba Busch (KD) var f\u00e5ordig n\u00e4r hon fick fr\u00e5gor om det p\u00e5 tisdagen.  \u2013 Jag \u00e4r ganska osugen p\u00e5 att bidra till det rubrikspelet, sa hon i samband med en utfr\u00e5gning i riksdagen.  Vice ordf\u00f6rande i justitieutskottet, Ardalan Shekarabi (S), har tidigare kr\u00e4vt Jomshofs avg\u00e5ng. Han uppmanar f\u00f6retr\u00e4dare f\u00f6r regeringen att sluta ge Jomshof st\u00f6d.  \u2013 Tyv\u00e4rr \u00e4r det ett konsekvent beteende han har. Han verkar f\u00f6r splittring, mots\u00e4ttningar och i vissa fall hat mot folkgrupper. Han anv\u00e4nder den plattform som ordf\u00f6rande i justitieutskottet medf\u00f6r till att bedriva den typen av agitation, s\u00e4ger han.  Aftonbladet har s\u00f6kt Sverigedemokraternas pressavdelning. De ber om att f\u00e5 fr\u00e5gorna till Richard Jomshof p\u00e5 mejl och att f\u00e5 \u00e5terkomma senare. Aftonbladet har s\u00f6kt Daniel Riazat. V\u00e4nsterpartiets pressavdelning ber att f\u00e5 \u00e5terkomma. \",\n  \"target_text\": \"SD-toppen Richard Jomshof v\u00e4grar kommentera kritiken f\u00f6r sitt p\u00e5st\u00e5ende att V\u00e4nsterpartiets riksdagsledamot Daniel Riazat borde l\u00e4mna Sverige. M\u00e5nga inom den politiska oppositionen kallar uttalandet rasistiskt N\u00e4r Jomshof konfronteras med fr\u00e5gor fr\u00e5n Aftonbladet vid ett utskottsm\u00f6te i riksdagen, blir han uppr\u00f6rd och g\u00e5r iv\u00e4g utan att svara p\u00e5 fr\u00e5gorna. Han h\u00e4nvisar till SD:s pressavdelning.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Fredrik Bolanders uttalande i \u201dRobinson\u201d f\u00e5r kritik: \u201dSkriver att jag \u00e4r en mansgris\u201d: Kvinnor \u00e4r bra p\u00e5 att st\u00e4da, laga mat och h\u00e5lla ordning.  Killar vill \u00e4ta mat, \u00e4r starkare och b\u00e4ttre. Fredrik Bolanders uttalande i \u201dRobinson\u201d har f\u00e5tt m\u00e5nga att reagera. \u2013 Jag vet att folk st\u00f6r sig p\u00e5 s\u00e5dana uttalanden, det \u00e4r ju ett s\u00e5dan samh\u00e4lle vi lever vi, s\u00e4ger han. \u2013 Om jag hade f\u00e5tt best\u00e4mma hade det varit en kvinna i laget f\u00f6r de \u00e4r ju bra p\u00e5 att laga mat, de \u00e4r bra p\u00e5 att h\u00e5lla ordning och st\u00e4da. D\u00e4r har vi det negativa med att inte ha en kvinna i laget. Vi m\u00e4n vill ju \u00e4ta s\u00e5klart. Uttalandet fr\u00e5n \u201dRobinson\u201d-deltagaren Fredrik Bolander, 40, har f\u00e5tt m\u00e5nga att reagera, bland annat p\u00e5 \u201dRobinsons\u201d sociala medier.  \u00c4ndringen i \u201dRobinson\u201d 2024 I \u00e5rets s\u00e4song delas kvinnor och m\u00e4n upp i olika lag.  N\u00e4r programledaren Anders Lundin, 65, fr\u00e5gar Bolander om han tror att det ger kvinnorna en st\u00f6rre chans att vinna i \u00e5r f\u00e5r han ett snabbt svar.  \u2013 Nej, det blir en kille som vinner i \u00e5r. Killar \u00e4r ofta lite starkare och b\u00e4ttre \u00e4n tjejer. Flera deltagare reagerar p\u00e5 uttalandet i programmet. Tjejerna protesterar h\u00f6gljutt och Gustav Jacobson, 27, g\u00f6r en f\u00f6rskr\u00e4ckt min.  Bolander s\u00e4ger \u00e4ven i programmet att han inte g\u00e5r s\u00e5 bra ihop med kvinnor och feminister. \u2013 Jag \u00e4r v\u00e4ldigt manlig i mig sj\u00e4lv, och jag har en v\u00e4ldigt manlig jargong, och tycker att det ska vara j\u00e4mlikt men man ska ocks\u00e5 f\u00f6rst\u00e5 vem som \u00e4r mannen i huset. \u201dSkriver att jag \u00e4r en mansgris\u201d N\u00e4r Aftonbladet pratar med Bolander samma dag som \u201dRobinson\u201d har premi\u00e4r ber\u00e4ttar han att han redan f\u00e5tt reaktioner och meddelanden fr\u00e5n tittare.  \u2013 De skriver att jag \u00e4r en mansgris och att jag har fel kvinnosyn. Samtidigt \u00e4r han medveten om att det han s\u00e4ger om kvinnor triggar folk.  \u2013 Jag \u00e4lskar att provocera. Det \u00e4r klart att jag gillar att se reaktioner, det vill jag ju, s\u00e4ger Bolander.  Han forts\u00e4tter:  \u2013 Jag vet att folk st\u00f6r sig p\u00e5 s\u00e5dana uttalanden, det \u00e4r ju ett s\u00e5dan samh\u00e4lle vi lever vi. S\u00e5 det var roligt att k\u00f6ra lite tv\u00e4rtom t\u00e4nkte jag. Fredrik Bolander om reaktionerna Just uttalandet om att det beh\u00f6vs en kvinna f\u00f6r att st\u00e4da och laga mat i killarnas lag \u00e4r det han f\u00e5tt mest reaktioner p\u00e5.  \u2013 M\u00e5nga som skrivit \u00e4r ju inte j\u00e4tteglada. Vad skriver folk? \u2013 Att vi lever i 2024 och man ska inte vara s\u00e5 och alla ska vara lika och allt det d\u00e4r. Men samtidigt s\u00e5, man g\u00f6r ju det man \u00e4r bra p\u00e5? Men m\u00e4n kan v\u00e4l ocks\u00e5 vara bra p\u00e5 att laga mat och st\u00e4da? \u2013 Jo men vi har ju mycket annat att g\u00f6ra? Som att tr\u00e4na med stenar? \u2013 Exakt. Pumpa muskler och tr\u00e4na, vi m\u00e5ste t\u00e4nka p\u00e5 hur vi ser ut, vi m\u00e5ste se solbr\u00e4nda ut och det tar tid. Det h\u00e4r \u00e4r ju ett uttalande som uppr\u00f6r m\u00e5nga. K\u00e4nner du att du kan st\u00e5 f\u00f6r det uttalandet? \u2013 Det d\u00e4r \u00e4r en sv\u00e5r fr\u00e5ga. Jag s\u00e4ger s\u00e5 h\u00e4r; man f\u00e5r se lite under programmets g\u00e5ng om det \u00e4r n\u00e5got jag st\u00e5r f\u00f6r eller inte. S\u00e5 kan jag s\u00e4ga. M\u00e5nga undrar ocks\u00e5 om du \u00e4r seri\u00f6s eller skojar? \u2013 Det \u00e4r det som \u00e4r fr\u00e5gan, skojar jag eller \u00e4r jag seri\u00f6s? Det svarar jag inte p\u00e5. Varf\u00f6r inte? \u2013 Antingen kanske jag st\u00e5r f\u00f6r det senare eller s\u00e5 g\u00f6r jag inte det. Det f\u00e5r ni se. \u201dRobinson\u201d s\u00e4nds s\u00f6ndagar klockan 21.00 samt m\u00e5ndag till torsdag klockan 19.30 p\u00e5 TV4 och p\u00e5 TV4 play. \",\n  \"target_text\": \"\\\"Robinson\\\"-deltagaren Fredrik Bolander har hamnat i bl\u00e5sv\u00e4der efter sina uttalanden om kvinnor och m\u00e4n, och f\u00e5r kritik p\u00e5 sociala medier. Han p\u00e5st\u00e5r att kvinnor \u00e4r bra p\u00e5 att laga mat och st\u00e4dning medan m\u00e4n \u00e4r starkare och b\u00e4ttre, och detta uppr\u00f6rde andra deltagare och tittare. Bolander s\u00e4ger att han \u00e4lskar att provocera, men v\u00e4grar svara p\u00e5 fr\u00e5gan om han sk\u00e4mtar eller \u00e4r seri\u00f6s.\"\n}\n</code></pre> <pre><code>{\n  \"text\": \"Polisen om den \u00f6vergivna diplomatbilen: \u201dVi unders\u00f6ker immunitetsfr\u00e5gan\u201d: En diplomatbil l\u00e4mnades \u00f6vergiven p\u00e5 ett t\u00e5gsp\u00e5r i centrala Stockholm i helgen. Fordonet tillh\u00f6r Etiopiens ambassad som har bett om urs\u00e4kt f\u00f6r vansinnesf\u00e4rden. Men n\u00e4r Aftonbladet knackar p\u00e5 \u00e4r de f\u00e5ordiga.  \u2013 Vi \u00e5terkommer s\u00e5 fort det g\u00e5r, s\u00e4ger en anst\u00e4lld p\u00e5 ambassaden. Det var natten till s\u00f6ndag som minibussen krockade p\u00e5 tv\u00e4rbanans sp\u00e5r vid Alviks strand i Stockholm. \u201dV\u00e5r ambassad ber om urs\u00e4kt f\u00f6r olyckan och besv\u00e4ren den orsakat. Vi har startat en internutredning f\u00f6r att ta reda p\u00e5 hur olyckan ska ha skett\u201d, skriver Etiopiens ambassad i Stockholm i ett mail till Aftonbladet. I \u00f6vrigt har de inte kommenterat h\u00e4ndelsen och n\u00e4r Aftonbladet knackar p\u00e5 hos ambassaden \u00e4r svaret kort. \u2013 Vi h\u00e5ller p\u00e5 att jobba med det. Vi \u00e5terkommer s\u00e5 fort det g\u00e5r, s\u00e4ger en anst\u00e4lld p\u00e5 ambassaden. Men n\u00e4r vill de inte svara p\u00e5. 17 300 kronor i obetalda b\u00f6ter T\u00e5gtrafiken var tillf\u00e4lligt avst\u00e4ngd under s\u00f6ndagsmorgonen och bilen fick b\u00e4rgas med hj\u00e4lp av en sp\u00e5rtraktor. Den har troligtvis k\u00f6rt upp p\u00e5 sp\u00e5ret vid Gr\u00f6ndal, enligt SL. D\u00e4r k\u00f6r bilar och sp\u00e5rvagnar p\u00e5 gatan innan r\u00e4lsen viker av p\u00e5 en egen banvall. \u2013 D\u00e4refter ska den i s\u00e5 fall ha k\u00f6rt tv\u00e5 kilometer p\u00e5 kross och makadam innan den krockat med en stolpe, s\u00e4ger Claes Keisu, pressansvarig p\u00e5 SL. Minibussen har ocks\u00e5 obetalda b\u00f6ter p\u00e5 17\u00a0300 kronor, enligt Transportstyrelsen.  \u201dHar skett en g\u00e5ng tidigare\u201d Den h\u00e4r typen av felk\u00f6rning sker cirka tio g\u00e5nger om \u00e5ret. Under februari skedde det tv\u00e5 g\u00e5nger, just vid Gr\u00f6ndal. Vanligtvis uppt\u00e4cks misstaget tidigt och d\u00e5 brukar f\u00f6raren kunna backa tillbaka p\u00e5 v\u00e4gen. \u2013 Det h\u00e4r fordonet har lite h\u00f6gre markfrig\u00e5ng s\u00e5 det kan f\u00f6rklara att den kunnat ta sig l\u00e4ngre, s\u00e4ger Claes Keisu. Men att bilen lyckats ta sig s\u00e5 l\u00e5ngt \u00e4r v\u00e4ldigt ovanligt. \u2013 Vad vi vet har det bara skett en g\u00e5ng tidigare. 2012 var det en \u00c5l\u00e4nning med sin familj som kom upp p\u00e5 banan i Hammarby sj\u00f6stad och k\u00f6rde hela v\u00e4gen till Gullmarsplan, s\u00e4ger Keisu. F\u00f6raren ska d\u00e5 ha k\u00f6rt uppemot en kilometer p\u00e5 sp\u00e5ret. \u201dVi unders\u00f6ker immunitetsfr\u00e5gan\u201d Polisen har inlett en f\u00f6runders\u00f6kning om v\u00e5rdsl\u00f6shet i trafik. Det \u00e4r fortfarande oklart om n\u00e5gon kan \u00e5talas.  \u2013 Vi unders\u00f6ker immunitetsfr\u00e5gan, s\u00e4ger Nadya Norton, presstalesperson vid Stockholmspolisen. \u201dUtredningen f\u00e5r visa om personen som k\u00f6rde bilen hade immunitet eller inte. Om en person har immunitet kan denne inte lagf\u00f6ras i Sverige\u201d, skriver f\u00f6runders\u00f6kningsledaren, Timmy Malmgren, i ett mail till Aftonbladet. Diplomater f\u00e5r inte straffas i landet de arbetar i, enligt internationella \u00f6verrenskommelser. \u2013 Jag har inga uppgifter om n\u00e5gon \u00e4r misst\u00e4nkt i \u00e4rendet, s\u00e4ger Nadya Norton. Hade fest under kv\u00e4llen Kv\u00e4llen innan bilen hittades p\u00e5 t\u00e5gsp\u00e5ret ska Ambassaden anordnat en fest i sina lokaler. \u201dVi p\u00e5 Ambassaden f\u00f6r Demokratiska f\u00f6rbundsrepubliken Etiopien p\u00e5 v\u00e5ning 3 kommer att ha ett event p\u00e5 l\u00f6rdag den 2. Observera att vi kommer ha g\u00e4ster. Vi hoppas att vi inte st\u00f6r er, k\u00e4ra grannar. Tack f\u00f6r er f\u00f6rst\u00e5else\u201d, skriver de p\u00e5 en lapp som sitter i fastighetens hiss.\",\n  \"target_text\": \"En bil fr\u00e5n Etiopiens ambassad l\u00e4mnades \u00f6vergiven p\u00e5 ett t\u00e5gsp\u00e5r i centrala Stockholm under helgen, vilket ledde till tillf\u00e4lligt avst\u00e4ngd t\u00e5gtrafik. Ambassaden har bett om urs\u00e4kt och p\u00e5b\u00f6rjat en intern utredning f\u00f6r att ta reda p\u00e5 h\u00e4ndelsef\u00f6rloppet. En polisutredning \u00e4r ig\u00e5ng f\u00f6r v\u00e5rdsl\u00f6shet i trafik, men det \u00e4r oklart om n\u00e5gon kan \u00e5talas p\u00e5 grund av diplomatisk immunitet.\"\n}\n</code></pre></p> <p>When evaluating generative models, we use the following setup (see the methodology for more information on how these are used):</p> <ul> <li>Number of few-shot examples: 1</li> <li>Prefix prompt:   <pre><code>Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\n</code></pre></li> <li>Base prompt template:   <pre><code>Artikel: {text}\nSammanfattning: {target_text}\n</code></pre></li> <li>Instruction-tuned prompt template:   <pre><code>Artikel: {text}\n\nSkriv en sammanfattning av artikeln ovan.\n</code></pre></li> </ul> <p>You can evaluate this dataset directly as follows:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --dataset schibsted-sv\n</code></pre>"},{"location":"extras/radial_plotter/","title":"Radial Plot","text":"<p>This tool can be used to compare individual models across each task in the benchmark, using a so-called radial plot (also known as a spider plot). You can access the underlying Hugging Face Space here.</p> <p> </p> <p></p>"},{"location":"leaderboards/","title":"Leaderboards","text":"<p>\ud83d\udc48 Choose a leaderboard on the left to see the results.</p>"},{"location":"leaderboards/#types-of-leaderboards","title":"\ud83c\udff7\ufe0f Types of Leaderboards","text":"<p>Each language has two leaderboards:</p> <ul> <li>Generative Leaderboard: This leaderboard shows the performance of models that can   generate text. These models have been evaluated on all tasks, both NLU and   NLG.</li> <li>NLU Leaderboard: This leaderboard shows the performance of models that can only   understand text, and not generate text themselves. These models have been evaluated on   the NLU tasks only.</li> </ul>"},{"location":"leaderboards/#how-to-read-the-leaderboards","title":"\ud83d\udcca How to Read the Leaderboards","text":"<p>The main score column is the <code>Rank</code>, showing the mean rank score of the model across all the tasks in the leaderboard. The lower the rank, the better the model.</p> <p>The columns that follow the rank columns are metadata about the model:</p> <ul> <li><code>Parameters</code>: The total number of parameters in the model, in millions.</li> <li><code>Vocabulary</code>: The size of the model's vocabulary, in thousands.</li> <li><code>Context</code>: The maximum number of tokens that the model can process at a time.</li> <li><code>Speed</code>: The inference time of the model - see more here.</li> <li><code>Type</code>: The type of model:<ul> <li>\ud83d\udd0d indicates that it is an encoder model (e.g., BERT)</li> <li>\ud83e\udde0 indicates that it is a base generative model (e.g., GPT-2)</li> <li>\ud83d\udcdd indicates that it is an instruction-tuned model (e.g., ChatGPT)</li> <li>\ud83e\udd14 indicates that it is a reasoning model (e.g., o1)</li> </ul> </li> <li><code>Commercial</code>: Whether the model can be used for commercial purposes. See here   for more information.</li> <li><code>Merge</code>: Whether the model is a merge of other models.</li> </ul> <p>After these metadata columns, the individual scores for each dataset is shown. Each dataset has a primary and secondary score - see what these are on the task page. Lastly, the final columns show the EuroEval version used to benchmark the given model on each of the datasets.</p> <p>To read more about the individual datasets, see the datasets page. Uf you're interested in the methodology behind the benchmark, see the methodology page.</p>"},{"location":"leaderboards/Monolingual/danish/","title":"\ud83c\udde9\ud83c\uddf0 Danish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/dutch/","title":"\ud83c\uddf3\ud83c\uddf1 Dutch","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/english/","title":"\ud83c\uddec\ud83c\udde7 English","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/faroese/","title":"\ud83c\uddeb\ud83c\uddf4 Faroese","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard <p>   A generative leaderboard for Faroese is not available yet, as the necessary datasets   are not yet available. </p>"},{"location":"leaderboards/Monolingual/french/","title":"\ud83c\uddeb\ud83c\uddf7 French","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/german/","title":"\ud83c\udde9\ud83c\uddea German","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/icelandic/","title":"\ud83c\uddee\ud83c\uddf8 Icelandic","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/norwegian/","title":"\ud83c\uddf3\ud83c\uddf4 Norwegian","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Monolingual/swedish/","title":"\ud83c\uddf8\ud83c\uddea Swedish","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Multilingual/european/","title":"\ud83c\uddea\ud83c\uddfa European","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Multilingual/germanic/","title":"\ud83c\udde9\ud83c\uddf0\ud83c\uddf3\ud83c\uddf1\ud83c\uddec\ud83c\udde7\ud83c\uddeb\ud83c\uddf4\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddf8\ud83c\uddf3\ud83c\uddf4\ud83c\uddf8\ud83c\uddea Germanic","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"leaderboards/Multilingual/mainland-scandinavian/","title":"\ud83c\udde9\ud83c\uddf0\ud83c\uddf3\ud83c\uddf4\ud83c\uddf8\ud83c\uddea Mainland Scandinavian","text":"<p>See the leaderboard page for more information about all the columns.</p> Generative LeaderboardNLU Leaderboard"},{"location":"tasks/","title":"Tasks","text":"<p>\ud83d\udc48 Choose a task on the left to see detailed information about that task.</p>"},{"location":"tasks/#overview","title":"\ud83d\udcda Overview","text":"<p>This page covers all the evaluation tasks used in EuroEval. These tasks fall under two categories, corresponding to whether the models should merely understand the input documents (NLU), or rather they are also required to generate new text (NLG).</p>"},{"location":"tasks/#nlu-tasks","title":"NLU Tasks","text":"<p>NLU tasks are tasks where the model is required to understand the natural language input and provide an output based on this understanding. The outputs are typically very short, often just a single label or a couple of words. The performance on these tasks is thus relevant to you if you primarily aim to use the language models for processing documents rather than generating entirely new documents. Both encoder and decoder models can be evaluated on these tasks, enabling you to compare the performance across all language models out there. The tasks in this category are:</p> <ol> <li>Sentiment Classification</li> <li>Named Entity Recognition</li> <li>Linguistic Acceptability</li> <li>Reading Comprehension</li> </ol>"},{"location":"tasks/#nlg-tasks","title":"NLG Tasks","text":"<p>NLG tasks are tasks where the model is required to generate natural language output based on some input. The outputs are typically longer than in NLU tasks, often multiple paragraphs. The performance on these tasks is thus relevant to you if you aim to use the language models for generating new documents. Only decoder models can be evaluated on these tasks, as encoder models do not have the capability to generate text. The tasks in this category are:</p> <ol> <li>Summarization</li> <li>Knowledge \uff0a</li> <li>Common-sense Reasoning \uff0a</li> </ol> <p>\uff0a These tasks should be considered as NLU tasks, but currently encoder models have not been set up to be evaluated on them. This will be added in a future version of EuroEval - see the progress in this issue.</p>"},{"location":"tasks/common-sense-reasoning/","title":"Common-sense Reasoning","text":""},{"location":"tasks/common-sense-reasoning/#overview","title":"\ud83d\udcda Overview","text":"<p>Common-sense reasoning is testing whether a model is able to understand basic deduction about the world. For instance, if the model is given the statement \"It is raining outside, and Peter is in his garden without an umbrella\", it should be able to deduce that Peter is getting wet. The task is set up as a multiple-choice question answering task, where the model is given a question and a set of possible answers, and it has to choose the correct answer.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/common-sense-reasoning/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the common-sense reasoning task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the accuracy score, as this is the most common metric used for this task, enabling comparisons with other benchmarks.</p>"},{"location":"tasks/common-sense-reasoning/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the common-sense reasoning task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task common-sense-reasoning\n</code></pre>"},{"location":"tasks/knowledge/","title":"Knowledge","text":""},{"location":"tasks/knowledge/#overview","title":"\ud83d\udcda Overview","text":"<p>The knowledge task is testing how much factual knowledge a model has. The task is set up as a multiple-choice question answering task, where the model is given a question and a set of possible answers, and it has to choose the correct answer. Crucially, it is not given any context in which the answer appears, so it has to answer purely based on its knowledge of the world.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/knowledge/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the knowledge task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the accuracy score, as this is the most common metric used for this task, enabling comparisons with other benchmarks.</p>"},{"location":"tasks/knowledge/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the knowledge task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task knowledge\n</code></pre>"},{"location":"tasks/linguistic-acceptability/","title":"Linguistic Acceptability","text":""},{"location":"tasks/linguistic-acceptability/#overview","title":"\ud83d\udcda Overview","text":"<p>Linguistic acceptability is a task of determining whether a given text is grammatically correct or not. It thus tests whether the model is able to understand the detailed syntax of a given document, and not just understand the overall gist of it. It roughly corresponds to when a native speaker would say \"this sentence sounds weird\".</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/linguistic-acceptability/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the linguistic acceptability task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the macro-average F1-score, being the average of the F1-score for each class, thus again weighing each class equally.</p>"},{"location":"tasks/linguistic-acceptability/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the linguistic acceptability task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task linguistic-acceptability\n</code></pre>"},{"location":"tasks/named-entity-recognition/","title":"Named Entity Recognition","text":""},{"location":"tasks/named-entity-recognition/#overview","title":"\ud83d\udcda Overview","text":"<p>Named entity recognition is a task of determining the named entities in a given text, such as named of persons, organisations, or locations. It thus both tests the knowledge the model has about these things, as well as being able to extract multiple pieces of information from a document at once.</p> <p>When evaluating generative models, we allow the model to generate 128 tokens on this task.</p>"},{"location":"tasks/named-entity-recognition/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the named entity recognition task, we use the micro-average F1-score without MISC, computed as the total number of true positives for all (non-trivial) entities except <code>MISC</code>, divided by the total number of predicted positives for all entities except <code>MISC</code>.</p> <p>We also report the micro-average F1-score, computed the same way, but where we include the <code>MISC</code> entity as well. This is useful for comparing with other benchmarks, as it is the most common metric used for this task. We find that excluding <code>MISC</code> gives a more accurate picture of the model's performance, however, as the the <code>MISC</code> entity is not well-defined and varies across datasets.</p>"},{"location":"tasks/named-entity-recognition/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the named entity recognition task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task named-entity-recognition\n</code></pre>"},{"location":"tasks/reading-comprehension/","title":"Reading Comprenhension","text":""},{"location":"tasks/reading-comprehension/#overview","title":"\ud83d\udcda Overview","text":"<p>Reading comprehension is a task of determining whether a model is able to understand a given text and answer questions about it. The model receives a text passage and a question about the text, and it has to provide the answer as it is stated in the text. This is very related to Retrieval-augmented Generation (RAG) applications, where a generative model is used to answer a question based on one or more retrieved documents.</p> <p>When evaluating generative models, we allow the model to generate 32 tokens on this task.</p>"},{"location":"tasks/reading-comprehension/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the reading comprehension task is the exact match (EM) score, which is the percentage of questions for which the model provides the exact answer.</p> <p>We also report the F1-score on a character-basis, which is more lenient than the EM score, as it allows for small differences in the answer.</p>"},{"location":"tasks/reading-comprehension/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the reading comprehension task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task reading-comprehension\n</code></pre>"},{"location":"tasks/sentiment-classification/","title":"Sentiment Classification","text":""},{"location":"tasks/sentiment-classification/#overview","title":"\ud83d\udcda Overview","text":"<p>Sentiment classification is a classical task of determining the sentiment of a given text, which can be positive, negative, or neutral. It thus tests whether the model is able to understand the overall semantics of a given document.</p> <p>When evaluating generative models, we allow the model to generate 5 tokens on this task.</p>"},{"location":"tasks/sentiment-classification/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric we use when evaluating the performance of a model on the sentiment classification task, we use Matthews correlation coefficient (MCC), which has a value between -100% and +100%, where 0% reflects a random guess. The primary benefit of MCC is that it is balanced even if the classes are imbalanced.</p> <p>We also report the macro-average F1-score, being the average of the F1-score for each class, thus again weighing each class equally.</p>"},{"location":"tasks/sentiment-classification/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the sentiment classification task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task sentiment-classification\n</code></pre>"},{"location":"tasks/speed/","title":"Speed","text":""},{"location":"tasks/speed/#overview","title":"\ud83d\udcda Overview","text":"<p>Speed is a task of measuring how quickly a model can process a given input. The model receives text passages of varying lengths, and it has to process the documents as quickly as possible, which includes tokenisation of the input. We let the model process the input repeatedly for 3 seconds, and then we measure how quick it was. We use the <code>pyinfer</code> package to perform the speed measurement.</p> <p>The speed is of course very dependent on available hardware, and for APIs it also fluctuates depending on the number of requests in the queue, so the speed benchmark should be taken as only a rough estimate of the model's speed, rather than an exact measurement.</p>"},{"location":"tasks/speed/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric used to evaluate the performance of a model on the speed task is the average number of GPT-2 tokens processed per second on GPUs, when the model is processing documents with roughly 100, 200, ..., 1,000 tokens. If the model is only accessible through an API then the speed is measured on the API. The GPUs used here vary, depending on the size of the model - we preferably use an NVIDIA RTX 3090 Ti GPU, if the model has less than ~8B parameters, and one or more NVIDIA A100 GPUs is larger.</p> <p>The secondary metric is the same, but where the documents are shorter, with roughly 12.5, 15, ..., 125 tokens.</p>"},{"location":"tasks/speed/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the speed task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task speed\n</code></pre>"},{"location":"tasks/summarization/","title":"Summarization","text":""},{"location":"tasks/summarization/#overview","title":"\ud83d\udcda Overview","text":"<p>Summarization is a task of generating a shorter version of a given text, while preserving the main points of the original text. The model receives a long text and has to generate a shorter version of it, typically a handful of sentences long. This is abstractive summarization, meaning that the summary typically do not appear verbatim in the original text, but that the model has to generate new text based on the input.</p> <p>When evaluating generative models, we allow the model to generate 256 tokens on this task.</p>"},{"location":"tasks/summarization/#metrics","title":"\ud83d\udcca Metrics","text":"<p>The primary metric used to evaluate the performance of a model on the summarization task is the BERTScore, which uses a pretrained encoder model to encode each token in both the reference summary and the generated summary, and then uses cosine similarity to measure how the tokens match up. Using an encoder model allows for the model to phrase a summary differently than the reference, while still being rewarded for capturing the same meaning. We use the <code>microsoft/mdeberta-v3-base</code> encoder model for all languages, as it is the best performing encoder model consistently across all languages in the framework.</p> <p>We also report the ROUGE-L score, which measures the longest sequence of words that the generated summary and the reference summary have in common. This is a more traditional metric for summarization, which is why we report it as well, but it correlates less well with human judgments than BERTScore.</p>"},{"location":"tasks/summarization/#how-to-run","title":"\ud83d\udee0\ufe0f How to run","text":"<p>In the command line interface of the EuroEval Python package, you can benchmark your favorite model on the summarization task like so:</p> <pre><code>$ euroeval --model &lt;model-id&gt; --task summarization\n</code></pre>"},{"location":"api/euroeval/","title":"euroeval","text":"euroeval<p> source package euroeval </p> <p>EuroEval - A benchmarking framework for language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> </ul> <p> source class Benchmarker(progress_bar: bool = True, save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.euroeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, use_flash_attention: bool | None = None, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False, only_allow_safetensors: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.euroeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014</p> <p>Whether to only allow models that use the safetensors format. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None, only_allow_safetensors: bool | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_allow_safetensors :  bool | None \u2014</p> <p>Whether to only allow models that use the safetensors format. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> <li> <p>e</p> </li> </ul> <p> source block_terminal_output() \u2192 None </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p>"},{"location":"src/euroeval/","title":"euroeval","text":"euroeval<p> docs package euroeval </p> <pre><code>\"\"\"EuroEval - A benchmarking framework for language models.\"\"\"\n\n### STAGE 1 ###\n### Block unwanted terminal output that happens on importing external modules ###\n\nimport logging\nimport sys\nimport warnings\n\nfrom termcolor import colored\n\n# Block specific warnings before importing anything else, as they can be noisy\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nlogging.getLogger(\"httpx\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"datasets\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n\n# Set up logging\nfmt = colored(\"%(asctime)s\", \"light_blue\") + \" \u22c5 \" + colored(\"%(message)s\", \"green\")\nlogging.basicConfig(\n    level=logging.CRITICAL if hasattr(sys, \"_called_from_test\") else logging.INFO,\n    format=fmt,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\n\n\n### STAGE 2 ###\n### Set the rest up ###\n\nimport importlib.metadata  # noqa: E402\nimport os  # noqa: E402\n\nfrom dotenv import load_dotenv  # noqa: E402\n\nfrom .benchmarker import Benchmarker  # noqa: E402\nfrom .utils import block_terminal_output  # noqa: E402\n\n# Block unwanted terminal outputs. This blocks way more than the above, but since it\n# relies on importing from the `utils` module, external modules are already imported\n# before this is run, necessitating the above block as well\nblock_terminal_output()\n\n\n# Fetches the version of the package as defined in pyproject.toml\n__version__ = importlib.metadata.version(\"euroeval\")\n\n\n# Loads environment variables\nload_dotenv()\n\n\n# Disable parallelisation when tokenizing, as that can lead to errors\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Enable MPS fallback\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n\n# Set amount of threads per GPU - this is the default and is only set to prevent a\n# warning from showing\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n\n# Disable a warning from Ray regarding the detection of the number of CPUs\nos.environ[\"RAY_DISABLE_DOCKER_CPU_WARNING\"] = \"1\"\n\n\n# Set the HF_TOKEN env var to copy the HUGGINGFACE_API_KEY env var, as vLLM uses the\n# former and LiteLLM uses the latter\nif os.getenv(\"HUGGINGFACE_API_KEY\"):\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_API_KEY\"]\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/","title":"euroeval.benchmark_modules","text":"euroeval.benchmark_modules<p> source package euroeval.benchmark_modules </p> <p>The different types of modules that can be benchmarked.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> source class BenchmarkModule(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : ABC</p></p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>buffer :  dict[str, t.Any] \u2014</p> <p>A buffer to store temporary data.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property BenchmarkModule.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if the model is not generative.</p> </li> </ul> <p> source method BenchmarkModule.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method BenchmarkModule.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property BenchmarkModule.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property BenchmarkModule.compute_metrics: ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> source property BenchmarkModule.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property BenchmarkModule.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class FreshEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A freshly initialised encoder model.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method FreshEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : BenchmarkModule</p></p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source class LiteLLMModel() </p> <p><p>Bases : BenchmarkModule</p></p> <p>A generative model from LiteLLM.</p> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source property LiteLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source method LiteLLMModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method LiteLLMModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method LiteLLMModel.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property LiteLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property LiteLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property LiteLLMModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source class VLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property VLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property VLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source property VLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property VLLMModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/","title":"euroeval.benchmark_modules","text":"euroeval.benchmark_modules<p> docs package euroeval.benchmark_modules </p> <pre><code>\"\"\"The different types of modules that can be benchmarked.\"\"\"\n\nfrom .base import BenchmarkModule\nfrom .fresh import FreshEncoderModel\nfrom .hf import HuggingFaceEncoderModel\nfrom .litellm import LiteLLMModel\nfrom .vllm import VLLMModel\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/base/","title":"euroeval.benchmark_modules.base","text":"euroeval.benchmark_modules.base<p> source module euroeval.benchmark_modules.base </p> <p>Abstract benchmark module class that the model classes inherit from.</p> <p> Classes </p> <ul> <li> <p>BenchmarkModule \u2014 Abstract class for a benchmark module.</p> </li> </ul> <p> source class BenchmarkModule(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : ABC</p></p> <p>Abstract class for a benchmark module.</p> <p>Initialise the benchmark module.</p> <p> Attributes </p> <ul> <li> <p>model_config \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>buffer :  dict[str, t.Any] \u2014</p> <p>A buffer to store temporary data.</p> </li> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>get_pytorch_module \u2014 Get the underlying PyTorch module.</p> </li> <li> <p>get_tokenizer \u2014 Get the underlying tokenizer.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>prepare_datasets \u2014 Prepare the datasets for the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method BenchmarkModule.get_pytorch_module() \u2192 nn.Module </p> <p>Get the underlying PyTorch module.</p> <p> Returns </p> <ul> <li> <p>nn.Module \u2014 The PyTorch module.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.get_tokenizer() \u2192 PreTrainedTokenizer </p> <p>Get the underlying tokenizer.</p> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BenchmarkModule.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source property BenchmarkModule.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if the model is not generative.</p> </li> </ul> <p> source method BenchmarkModule.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method BenchmarkModule.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property BenchmarkModule.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property BenchmarkModule.compute_metrics: ComputeMetricsFunction </p> <p>The function used to compute the metrics.</p> <p> Returns </p> <ul> <li> <p>ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> </ul> <p> source property BenchmarkModule.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property BenchmarkModule.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method BenchmarkModule.prepare_datasets(datasets: list[DatasetDict], task: Task) \u2192 list[DatasetDict] </p> <p>Prepare the datasets for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the datasets for.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 The prepared datasets.</p> </li> </ul> <p> source method BenchmarkModule.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method BenchmarkModule.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod BenchmarkModule.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod BenchmarkModule.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/base/","title":"euroeval.benchmark_modules.base","text":"euroeval.benchmark_modules.base<p> docs module euroeval.benchmark_modules.base </p> <pre><code>\"\"\"Abstract benchmark module class that the model classes inherit from.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport sys\nimport typing as t\nfrom abc import ABC, abstractmethod\nfrom functools import cached_property, partial\n\nfrom datasets import DatasetDict\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizer, Trainer\n\nfrom ..data_models import (\n    BenchmarkConfig,\n    DatasetConfig,\n    GenerativeModelOutput,\n    ModelConfig,\n    Task,\n)\nfrom ..enums import BatchingPreference, GenerativeType, TaskGroup\nfrom ..exceptions import NeedsEnvironmentVariable, NeedsExtraInstalled\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ComputeMetricsFunction, ExtractLabelsFunction\nfrom ..utils import log_once\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass BenchmarkModule(ABC):docs\n    \"\"\"Abstract class for a benchmark module.\n\n    Attributes:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n        buffer:\n            A buffer to store temporary data.\n    \"\"\"\n\n    fresh_model: bool\n    batching_preference: BatchingPreference\n    high_priority: bool\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the benchmark module.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        self.model_config = model_config\n        self.dataset_config = dataset_config\n        self.benchmark_config = benchmark_config\n        self.buffer: dict[str, t.Any] = dict()\n        self._log_metadata()\n\n    def _log_metadata(self) -&gt; None:\n        \"\"\"Log the metadata of the model.\"\"\"\n        # Set logging level based on verbosity\n        if hasattr(sys, \"_called_from_test\"):\n            logging_level = logging.CRITICAL\n        elif self.benchmark_config.verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        logger.setLevel(logging_level)\n\n        logging_msg: str = \"\"\n        if self.num_params &lt; 0:\n            logging_msg += \"The model has an unknown number of parameters, \"\n        else:\n            logging_msg += f\"The model has {self.num_params:,} parameters, \"\n        if self.vocab_size &lt; 0:\n            logging_msg += \"an unknown vocabulary size, \"\n        else:\n            logging_msg += f\"a vocabulary size of {self.vocab_size:,}, \"\n        if self.model_max_length &lt; 0:\n            logging_msg += \"and an unknown maximum sequence length.\"\n        else:\n            logging_msg += f\"and a maximum context length of {self.model_max_length:,}.\"\n        log_once(message=logging_msg, level=logging.INFO)\n\n    def get_pytorch_module(self) -&gt; \"nn.Module\":docs\n        \"\"\"Get the underlying PyTorch module.\n\n        Returns:\n            The PyTorch module.\n        \"\"\"\n        if hasattr(self, \"_model\"):\n            return self._model\n        raise NotImplementedError(\n            \"The `get_pytorch_module` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    def get_tokenizer(self) -&gt; \"PreTrainedTokenizer\":docs\n        \"\"\"Get the underlying tokenizer.\n\n        Returns:\n            The tokenizer.\n        \"\"\"\n        if hasattr(self, \"_tokenizer\"):\n            return self._tokenizer\n        raise NotImplementedError(\n            \"The `get_tokenizer` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    @cached_property\n    @abstractmethod\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def generative_type(self) -&gt; GenerativeType | None:docs\n        \"\"\"Get the generative type of the model.\n\n        Returns:\n            The generative type of the model, or None if the model is not generative.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethod\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        ...\n\n    @cached_property\n    @abstractmethod\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum length of the model.\n\n        Returns:\n            The maximum length of the model.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethoddocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        ...\n\n    @property\n    def compute_metrics(self) -&gt; ComputeMetricsFunction:docs\n        \"\"\"The function used to compute the metrics.\n\n        Returns:\n            The function used to compute the metrics.\n        \"\"\"\n        match self.dataset_config.task.task_group:\n            case TaskGroup.SEQUENCE_CLASSIFICATION:\n                return partial(\n                    sequence_classification.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION:\n                return partial(\n                    sequence_classification.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case TaskGroup.TEXT_TO_TEXT:\n                return partial(\n                    text_to_text.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                return partial(\n                    token_classification.compute_metrics,\n                    has_misc_tags=self.buffer.get(\"has_misc_tags\", True),\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case TaskGroup.QUESTION_ANSWERING:\n                return partial(\n                    question_answering.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=self.benchmark_config,\n                )\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task group: {self.dataset_config.task.task_group}.\"\n                )\n\n    @property\n    @abstractmethod\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        ...\n\n    def prepare_datasets(docs\n        self, datasets: list[DatasetDict], task: Task\n    ) -&gt; list[DatasetDict]:\n        \"\"\"Prepare the datasets for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            datasets:\n                The datasets to prepare.\n            task:\n                The task to prepare the datasets for.\n\n        Returns:\n            The prepared datasets.\n        \"\"\"\n        for idx, dataset in enumerate(\n            tqdm(iterable=datasets, desc=\"Preparing datasets\")\n        ):\n            prepared_dataset = self.prepare_dataset(\n                dataset=dataset, task=task, itr_idx=idx\n            )\n            if self.dataset_config.task.task_group == TaskGroup.TOKEN_CLASSIFICATION:\n                labels_in_train: set[str] = {\n                    tag for tag_list in dataset[\"train\"][\"labels\"] for tag in tag_list\n                }\n                self.buffer[\"has_misc_tags\"] = (\n                    \"B-MISC\" in labels_in_train or \"I-MISC\" in labels_in_train\n                )\n            datasets[idx] = DatasetDict(\n                dict(\n                    train=prepared_dataset[\"train\"],\n                    val=prepared_dataset[\"val\"],\n                    test=prepared_dataset[\"test\"],\n                    original_train=dataset[\"train\"],\n                    original_val=dataset[\"val\"],\n                    original_test=dataset[\"test\"],\n                )\n            )\n        return datasets\n\n    @abstractmethod\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        ...\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `generate` method has not been implemented for \"\n            f\"{self.__class__.__name__}.\"\n        )\n\n    @classmethod\n    @abstractmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        ...\n\n    @classmethod\n    @abstractmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/fresh/","title":"euroeval.benchmark_modules.fresh","text":"euroeval.benchmark_modules.fresh<p> source module euroeval.benchmark_modules.fresh </p> <p>Freshly initialised encoder models.</p> <p> Classes </p> <ul> <li> <p>FreshEncoderModel \u2014 A freshly initialised encoder model.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> </ul> <p> source class FreshEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A freshly initialised encoder model.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method FreshEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method FreshEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source classmethod FreshEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod FreshEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, model_max_length: int) \u2192 tuple[PreTrainedModel, PreTrainedTokenizer] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>model_max_length :  int \u2014</p> <p>The maximum context length of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel, PreTrainedTokenizer] \u2014 The loaded model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>InvalidModel</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/fresh/","title":"euroeval.benchmark_modules.fresh","text":"euroeval.benchmark_modules.fresh<p> docs module euroeval.benchmark_modules.fresh </p> <pre><code>\"\"\"Freshly initialised encoder models.\"\"\"\n\nimport os\nfrom functools import cached_property\nfrom json import JSONDecodeError\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    ElectraForQuestionAnswering,\n    ElectraForSequenceClassification,\n    ElectraForTokenClassification,\n    PretrainedConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    XLMRobertaForQuestionAnswering,\n    XLMRobertaForSequenceClassification,\n    XLMRobertaForTokenClassification,\n)\n\nfrom ..data_models import BenchmarkConfig, DatasetConfig, ModelConfig\nfrom ..enums import InferenceBackend, ModelType, TaskGroup\nfrom ..exceptions import (\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..utils import block_terminal_output, create_model_cache_dir\nfrom .hf import (\n    HuggingFaceEncoderModel,\n    align_model_and_tokenizer,\n    setup_model_for_question_answering,\n)\n\n\nclass FreshEncoderModel(HuggingFaceEncoderModel):docs\n    \"\"\"A freshly initialised encoder model.\"\"\"\n\n    fresh_model = True\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        # This is already set when calling `super.__init__`, but we need it to get a\n        # value from `self.model_max_length`, so we set it here as well.\n        self.model_config = model_config\n\n        model, tokenizer = load_model_and_tokenizer(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n            model_max_length=self.model_max_length,\n        )\n        self._model: PreTrainedModel = model\n        self._tokenizer: PreTrainedTokenizer = tokenizer\n\n        self._model, self._tokenizer = align_model_and_tokenizer(\n            model=self._model,\n            tokenizer=self._tokenizer,\n            model_max_length=self.model_max_length,\n            raise_errors=benchmark_config.raise_errors,\n        )\n\n        # We specify `HuggingFaceEncoderModel` here instead of `VLLMModel`, as we want\n        # to call the `__init__` method of the `BenchmarkModule` class.\n        super(HuggingFaceEncoderModel, self).__init__(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n    @cached_property\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 278_885_778\n            case \"fresh-electra-small\":\n                return 13_738_755\n            case _:\n                raise NotImplementedError(\n                    f\"Number of parameters for model {self.model_config.model_id} is \"\n                    \"not implemented.\"\n                )\n\n    @cached_property\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 250_002\n            case \"fresh-electra-small\":\n                return 32_000\n            case _:\n                raise NotImplementedError(\n                    f\"Vocabulary size for model {self.model_config.model_id} is not \"\n                    \"implemented.\"\n                )\n\n    @cached_property\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum context length of the model.\n\n        Returns:\n            The maximum context length of the model.\n        \"\"\"\n        match self.model_config.model_id:\n            case \"fresh-xlm-roberta-base\":\n                return 512\n            case \"fresh-electra-small\":\n                return 128\n            case _:\n                raise NotImplementedError(\n                    f\"Maximum context length for model {self.model_config.model_id} is \"\n                    \"not implemented.\"\n                )\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        valid_models = [\"fresh-electra-small\", \"fresh-xlm-roberta-base\"]\n        return model_id in valid_models\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=model_id,\n            task=\"fill-mask\",\n            languages=list(),\n            revision=\"main\",\n            merge=False,\n            inference_backend=InferenceBackend.TRANSFORMERS,\n            model_type=ModelType.ENCODER,\n            fresh=True,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n\ndef load_model_and_tokenizer(docs\n    model_config: ModelConfig,\n    dataset_config: DatasetConfig,\n    benchmark_config: BenchmarkConfig,\n    model_max_length: int,\n) -&gt; tuple[PreTrainedModel, PreTrainedTokenizer]:\n    \"\"\"Load the model and tokenizer.\n\n    Args:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n        model_max_length:\n            The maximum context length of the model.\n\n    Returns:\n        The loaded model and tokenizer.\n    \"\"\"\n    config: \"PretrainedConfig\"\n    block_terminal_output()\n\n    # Get the fresh model ID and the corresponding real model ID\n    model_id = model_config.model_id.replace(\"-\", \"_\")\n    fresh_to_real_model_id_mapping = dict(\n        fresh_xlm_roberta_base=\"FacebookAI/xlm-roberta-base\",\n        fresh_electra_small=\"google/electra-small-discriminator\",\n    )\n    real_model_id = fresh_to_real_model_id_mapping[model_id]\n\n    match dataset_config.task.task_group:\n        case (\n            TaskGroup.SEQUENCE_CLASSIFICATION | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n        ):\n            model_cls_mapping = dict(\n                fresh_xlm_roberta_base=XLMRobertaForSequenceClassification,\n                fresh_electra_small=ElectraForSequenceClassification,\n            )\n        case TaskGroup.TOKEN_CLASSIFICATION:\n            model_cls_mapping = dict(\n                fresh_xlm_roberta_base=XLMRobertaForTokenClassification,\n                fresh_electra_small=ElectraForTokenClassification,\n            )\n        case TaskGroup.QUESTION_ANSWERING:\n            model_cls_mapping = dict(\n                fresh_xlm_roberta_base=XLMRobertaForQuestionAnswering,\n                fresh_electra_small=ElectraForQuestionAnswering,\n            )\n        case _:\n            raise InvalidBenchmark(\n                f\"Task group {dataset_config.task.task_group} is not \"\n                f\"supported for model {model_config.model_id}.\"\n            )\n    model_cls = model_cls_mapping[model_id]\n\n    config = AutoConfig.from_pretrained(\n        real_model_id,\n        token=benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True,\n        num_labels=dataset_config.num_labels,\n        id2label=dataset_config.id2label,\n        label2id=dataset_config.label2id,\n        cache_dir=model_config.model_cache_dir,\n        trust_remote_code=benchmark_config.trust_remote_code,\n    )\n    model = model_cls(config)\n\n    if dataset_config.task.task_group == TaskGroup.QUESTION_ANSWERING:\n        model = setup_model_for_question_answering(model=model)\n\n    # Load the tokenizer. If the model is a subclass of a RoBERTa model then we\n    # have to add a prefix space to the tokens, by the way the model is constructed\n    prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n    prefix = any(model_type in type(model).__name__ for model_type in prefix_models)\n    try:\n        tokenizer: \"PreTrainedTokenizer\" = AutoTokenizer.from_pretrained(\n            real_model_id,\n            revision=model_config.revision,\n            token=benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True,\n            add_prefix_space=prefix,\n            cache_dir=model_config.model_cache_dir,\n            use_fast=True,\n            verbose=False,\n            trust_remote_code=benchmark_config.trust_remote_code,\n        )\n    except (JSONDecodeError, OSError):\n        raise InvalidModel(f\"Could not load tokenizer for model {real_model_id!r}.\")\n\n    model, tokenizer = align_model_and_tokenizer(\n        model=model,\n        tokenizer=tokenizer,\n        model_max_length=model_max_length,\n        raise_errors=benchmark_config.raise_errors,\n    )\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/hf/","title":"euroeval.benchmark_modules.hf","text":"euroeval.benchmark_modules.hf<p> source module euroeval.benchmark_modules.hf </p> <p>Encoder models from the Hugging Face Hub.</p> <p> Classes </p> <ul> <li> <p>HuggingFaceEncoderModel \u2014 An encoder model from the Hugging Face Hub.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> <li> <p>get_model_repo_info \u2014 Get the information about the model from the HF Hub or a local directory.</p> </li> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>get_torch_dtype \u2014 Get the torch dtype, used for loading the model.</p> </li> <li> <p>load_hf_model_config \u2014 Load the Hugging Face model configuration.</p> </li> <li> <p>setup_model_for_question_answering \u2014 Setup a model for question answering.</p> </li> <li> <p>get_children_of_module \u2014 Get the children of a module.</p> </li> <li> <p>align_model_and_tokenizer \u2014 Aligns the model and the tokenizer.</p> </li> <li> <p>task_group_to_class_name \u2014 Convert a task group to a class name.</p> </li> </ul> <p> source class HuggingFaceEncoderModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : BenchmarkModule</p></p> <p>An encoder model from the Hugging Face Hub.</p> <p>Initialise the model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum context length of the model.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.model_max_length() \u2192 int </p> <p>The maximum context length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum context length of the model.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property HuggingFaceEncoderModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source method HuggingFaceEncoderModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod HuggingFaceEncoderModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 tuple[PreTrainedModel, PreTrainedTokenizer] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel, PreTrainedTokenizer] \u2014 The loaded model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source get_model_repo_info(model_id: str, revision: str, benchmark_config: BenchmarkConfig) \u2192 HFModelInfo | None </p> <p>Get the information about the model from the HF Hub or a local directory.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>HFModelInfo | None \u2014 The information about the model, or None if the model could not be found.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>HuggingFaceHubDown</p> </li> <li> <p>NoInternetConnection</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source load_tokenizer(model: PreTrainedModel | None, model_id: str, trust_remote_code: bool) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel | None \u2014</p> <p>The model, which is used to determine whether to add a prefix space to the tokens. Can be None.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The model identifier. Used for logging.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_torch_dtype(device: torch.device, torch_dtype_is_set: bool, bf16_available: bool) \u2192 str | torch.dtype </p> <p>Get the torch dtype, used for loading the model.</p> <p> Parameters </p> <ul> <li> <p>device :  torch.device \u2014</p> <p>The device to use.</p> </li> <li> <p>torch_dtype_is_set :  bool \u2014</p> <p>Whether the torch data type is set in the model configuration.</p> </li> <li> <p>bf16_available :  bool \u2014</p> <p>Whether bfloat16 is available.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str | torch.dtype \u2014 The torch dtype.</p> </li> </ul> <p> source load_hf_model_config(model_id: str, num_labels: int, id2label: dict[int, str], label2id: dict[str, int], revision: str, model_cache_dir: str | None, api_key: str | None, trust_remote_code: bool, run_with_cli: bool) \u2192 PretrainedConfig </p> <p>Load the Hugging Face model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The Hugging Face model ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from label IDs to labels.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from labels to label IDs.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>model_cache_dir :  str | None \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The Hugging Face API key.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the script is being run with the CLI.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PretrainedConfig \u2014 The Hugging Face model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source setup_model_for_question_answering(model: PreTrainedModel) \u2192 PreTrainedModel </p> <p>Setup a model for question answering.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to setup.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedModel \u2014 The setup model.</p> </li> </ul> <p> source get_children_of_module(name: str, module: nn.Module) \u2192 nn.Module | dict[str, t.Any] | None </p> <p>Get the children of a module.</p> <p> Parameters </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the module.</p> </li> <li> <p>module :  nn.Module \u2014</p> <p>The module to get the children of.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>nn.Module | dict[str, t.Any] | None \u2014 The children of the module, or None if the module has no children.</p> </li> </ul> <p> source align_model_and_tokenizer(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, model_max_length: int, raise_errors: bool = False) \u2192 tuple[PreTrainedModel, PreTrainedTokenizer] </p> <p>Aligns the model and the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model :  PreTrainedModel \u2014</p> <p>The model to fix.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer to fix.</p> </li> <li> <p>model_max_length :  int \u2014</p> <p>The maximum length of the model.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of trying to fix them silently.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[PreTrainedModel, PreTrainedTokenizer] \u2014 The fixed model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source task_group_to_class_name(task_group: TaskGroup) \u2192 str </p> <p>Convert a task group to a class name.</p> <p> Parameters </p> <ul> <li> <p>task_group :  TaskGroup \u2014</p> <p>The task group.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The class name.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/hf/","title":"euroeval.benchmark_modules.hf","text":"euroeval.benchmark_modules.hf<p> docs module euroeval.benchmark_modules.hf </p> <pre><code>\"\"\"Encoder models from the Hugging Face Hub.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport os\nimport typing as t\nfrom functools import cached_property, partial\nfrom json import JSONDecodeError\nfrom pathlib import Path\nfrom time import sleep\n\nimport torch\nfrom datasets import DatasetDict\nfrom huggingface_hub import HfApi\nfrom huggingface_hub import whoami as hf_whoami\nfrom huggingface_hub.hf_api import ModelInfo as HfApiModelInfo\nfrom huggingface_hub.hf_api import RepositoryNotFoundError, RevisionNotFoundError\nfrom huggingface_hub.utils import (\n    GatedRepoError,\n    HFValidationError,\n    LocalTokenNotFoundError,\n)\nfrom requests.exceptions import RequestException\nfrom torch import nn\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    BatchEncoding,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n    PretrainedConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    Trainer,\n)\nfrom transformers.modelcard import TASK_MAPPING\nfrom urllib3.exceptions import RequestError\n\nfrom ..constants import (\n    DUMMY_FILL_VALUE,\n    GENERATIVE_PIPELINE_TAGS,\n    LOCAL_MODELS_REQUIRED_FILES,\n    MERGE_TAGS,\n)\nfrom ..data_models import BenchmarkConfig, DatasetConfig, HFModelInfo, ModelConfig, Task\nfrom ..enums import (\n    BatchingPreference,\n    GenerativeType,\n    InferenceBackend,\n    ModelType,\n    TaskGroup,\n)\nfrom ..exceptions import (\n    HuggingFaceHubDown,\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsAdditionalArgument,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n    NoInternetConnection,\n)\nfrom ..languages import get_all_languages\nfrom ..task_utils import (\n    multiple_choice_classification,\n    question_answering,\n    token_classification,\n)\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import (\n    block_terminal_output,\n    create_model_cache_dir,\n    get_bos_token,\n    get_class_by_name,\n    get_eos_token,\n    internet_connection_available,\n)\nfrom .base import BenchmarkModule\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass HuggingFaceEncoderModel(BenchmarkModule):\n    \"\"\"An encoder model from the Hugging Face Hub.\"\"\"\n\n    fresh_model = False\n    batching_preference = BatchingPreference.NO_PREFERENCE\n    high_priority = True\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        model, tokenizer = load_model_and_tokenizer(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n        self._model: PreTrainedModel = model\n        self._tokenizer: PreTrainedTokenizer = tokenizer\n\n        self._model, self._tokenizer = align_model_and_tokenizer(\n            model=self._model,\n            tokenizer=self._tokenizer,\n            model_max_length=self.model_max_length,\n            raise_errors=benchmark_config.raise_errors,\n        )\n\n        super().__init__(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n    @cached_property\n    def num_params(self) -&gt; int:\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        token = (\n            self.benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True\n        )\n        hf_api = HfApi(token=token)\n        try:\n            repo_info = hf_api.model_info(\n                repo_id=self.model_config.adapter_base_model_id\n                or self.model_config.model_id,\n                revision=self.model_config.revision,\n            )\n        except (\n            RepositoryNotFoundError,\n            RevisionNotFoundError,\n            RequestException,\n            HFValidationError,\n        ):\n            repo_info = None\n\n        if (\n            repo_info is not None\n            and hasattr(repo_info, \"safetensors\")\n            and repo_info.safetensors is not None\n            and \"total\" in repo_info.safetensors\n        ):\n            num_params = repo_info.safetensors[\"total\"]\n        elif (\n            hasattr(self._model.config, \"num_params\")\n            and self._model.config.num_params is not None\n        ):\n            num_params = self._model.config.num_params\n        elif hasattr(self._model, \"parameters\"):\n            num_params = sum(p.numel() for p in self._model.parameters())\n        else:\n            logger.warning(\n                \"The number of parameters could not be determined for the model, since \"\n                \"the model is not stored in the safetensors format. If this is your \"\n                \"own model, then you can use this Hugging Face Space to convert your \"\n                \"model to the safetensors format: \"\n                \"https://huggingface.co/spaces/safetensors/convert.\"\n            )\n            num_params = -1\n        return num_params\n\n    @cached_property\n    def vocab_size(self) -&gt; int:\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        if (\n            hasattr(self._model.config, \"vocab_size\")\n            and self._model.config.vocab_size is not None\n        ):\n            vocab_size = self._model.config.vocab_size\n        elif (\n            hasattr(self._tokenizer, \"vocab_size\")\n            and self._tokenizer.vocab_size is not None\n        ):\n            vocab_size = self._tokenizer.vocab_size\n        else:\n            vocab_size = -1\n        return vocab_size\n\n    @cached_property\n    def model_max_length(self) -&gt; int:\n        \"\"\"The maximum context length of the model.\n\n        Returns:\n            The maximum context length of the model.\n        \"\"\"\n        all_max_lengths: list[int] = list()\n\n        # Add the registered max length of the tokenizer\n        if hasattr(\n            self._tokenizer, \"model_max_length\"\n        ) and self._tokenizer.model_max_length &lt; int(1e30):\n            all_max_lengths.append(self._tokenizer.model_max_length)\n\n        # Add the max length derived from the model's input sizes\n        if hasattr(self._tokenizer, \"max_model_input_sizes\"):\n            all_max_lengths.extend(\n                [\n                    size\n                    for size in self._tokenizer.max_model_input_sizes.values()\n                    if size is not None\n                ]\n            )\n\n        # Add max length candidates from the model's configuration\n        candidate_config_max_lengths = [\n            \"max_position_embeddings\",\n            \"max_sequence_length\",\n            \"model_max_length\",\n            \"sliding_window\",\n            \"sliding_window_size\",\n            \"n_positions\",\n        ]\n        for candidate_config_max_length in candidate_config_max_lengths:\n            if (\n                hasattr(self._model.config, candidate_config_max_length)\n                and (value := getattr(self._model.config, candidate_config_max_length))\n                is not None\n            ):\n                all_max_lengths.append(value)\n\n        # To avoid models having artificially low max lengths, we remove any max lengths\n        # that are less than 128\n        all_max_lengths = [\n            max_length for max_length in all_max_lengths if max_length &gt;= 128\n        ]\n\n        if len(list(all_max_lengths)) &gt; 0:\n            model_max_length = min(list(all_max_lengths))\n        else:\n            model_max_length = -1\n\n        return model_max_length\n\n    @property\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        match self.dataset_config.task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.TEXT_TO_TEXT\n                | TaskGroup.QUESTION_ANSWERING\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                return DataCollatorWithPadding(self._tokenizer, padding=\"longest\")\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                return DataCollatorForTokenClassification(\n                    tokenizer=self._tokenizer, label_pad_token_id=-100\n                )\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task group: {self.dataset_config.task.task_group}.\"\n                )\n\n    @property\n    def generative_type(self) -&gt; GenerativeType | None:\n        \"\"\"Get the generative type of the model.\n\n        Returns:\n            The generative type of the model, or None if it has not been set yet.\n        \"\"\"\n        return None\n\n    @property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `extract_labels_from_generation` property has not been implemented \"\n            \"for Hugging Face Encoder models.\"\n        )\n\n    @property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        match self.dataset_config.task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.TEXT_TO_TEXT\n                | TaskGroup.TOKEN_CLASSIFICATION\n            ):\n                return Trainer\n            case TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION:\n                return (\n                    multiple_choice_classification.MultipleChoiceClassificationTrainer\n                )\n            case TaskGroup.QUESTION_ANSWERING:\n                return question_answering.QuestionAnsweringTrainer\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task group: {self.dataset_config.task.task_group}.\"\n                )\n\n    def prepare_dataset(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n\n        def numericalise_labels(examples: dict) -&gt; dict:\n            if \"label\" in examples:\n                try:\n                    examples[\"label\"] = [\n                        self._model.config.label2id[lbl.lower()]\n                        for lbl in examples[\"label\"]\n                    ]\n                except KeyError:\n                    raise InvalidBenchmark(\n                        f\"One of the labels in the dataset, \"\n                        f\"{examples['label'].lower()}, does not occur in the \"\n                        f\"label2id dictionary {self._model.config.label2id}.\"\n                    )\n            return examples\n\n        def tokenise(examples: dict) -&gt; BatchEncoding:\n            return self._tokenizer(text=examples[\"text\"], truncation=True, padding=True)\n\n        match task.task_group:\n            case TaskGroup.SEQUENCE_CLASSIFICATION:\n                dataset = dataset.map(\n                    numericalise_labels, batched=True, load_from_cache_file=False\n                ).map(tokenise, batched=True, load_from_cache_file=False)\n\n            case TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION:\n                dataset = DatasetDict(\n                    train=dataset[\"train\"].map(\n                        partial(\n                            multiple_choice_classification.prepare_examples,\n                            tokenizer=self._tokenizer,\n                        ),\n                        batched=True,\n                        batch_size=1,\n                        remove_columns=dataset[\"train\"].column_names,\n                        load_from_cache_file=False,\n                        keep_in_memory=True,\n                    ),\n                    val=dataset[\"val\"].map(\n                        partial(\n                            multiple_choice_classification.prepare_examples,\n                            tokenizer=self._tokenizer,\n                        ),\n                        batched=True,\n                        batch_size=1,\n                        remove_columns=dataset[\"val\"].column_names,\n                        load_from_cache_file=False,\n                        keep_in_memory=True,\n                    ),\n                    test=dataset[\"test\"].map(\n                        partial(\n                            multiple_choice_classification.prepare_examples,\n                            tokenizer=self._tokenizer,\n                        ),\n                        batched=True,\n                        batch_size=1,\n                        remove_columns=dataset[\"test\"].column_names,\n                        load_from_cache_file=False,\n                        keep_in_memory=True,\n                    ),\n                )\n\n            case TaskGroup.TEXT_TO_TEXT:\n                dataset = dataset.map(\n                    tokenise,\n                    batched=True,\n                    load_from_cache_file=False,\n                    keep_in_memory=True,\n                )\n\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                dataset = dataset.map(\n                    partial(\n                        token_classification.tokenize_and_align_labels,\n                        tokenizer=self._tokenizer,\n                        label2id=self._model.config.label2id,\n                    ),\n                    batched=True,\n                    load_from_cache_file=False,\n                    keep_in_memory=True,\n                )\n\n            case TaskGroup.QUESTION_ANSWERING:\n                dataset = DatasetDict(\n                    dict(\n                        train=dataset[\"train\"].map(\n                            partial(\n                                question_answering.prepare_train_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                        val=dataset[\"val\"].map(\n                            partial(\n                                question_answering.prepare_train_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                        test=dataset[\"test\"].map(\n                            partial(\n                                question_answering.prepare_test_examples,\n                                tokenizer=self._tokenizer,\n                            ),\n                            batched=True,\n                            batch_size=10,\n                            remove_columns=dataset[\"test\"].column_names,\n                            load_from_cache_file=False,\n                            keep_in_memory=True,\n                        ),\n                    )\n                )\n\n                # The Trainer hides the columns that are not used by the model (here\n                # `id` and `offset_mapping` which we will need for our post-processing),\n                # so we put them back\n                for split_name, split in dataset.items():\n                    dataset[split_name].set_format(\n                        type=split.format[\"type\"], columns=list(split.features.keys())\n                    )\n\n            case _:\n                raise NotImplementedError(f\"Unsupported task group: {task.task_group}.\")\n\n        return dataset\n\n    @classmethod\n    def model_exists(\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        return (\n            model_info is not None\n            and model_info.pipeline_tag not in GENERATIVE_PIPELINE_TAGS\n        )\n\n    @classmethod\n    def get_model_config(\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        if model_info is None:\n            raise InvalidModel(f\"The model {model_id!r} could not be found.\")\n\n        language_mapping = get_all_languages()\n        language_codes = list(language_mapping.keys())\n\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=revision,\n            task=model_info.pipeline_tag,\n            languages=[\n                language_mapping[tag]\n                for tag in model_info.tags\n                if tag in language_codes\n            ],\n            merge=any(tag in model_info.tags for tag in MERGE_TAGS),\n            inference_backend=InferenceBackend.TRANSFORMERS,\n            model_type=ModelType.ENCODER,\n            fresh=False,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n        return model_config\n\n\ndef load_model_and_tokenizer(docs\n    model_config: ModelConfig,\n    dataset_config: DatasetConfig,\n    benchmark_config: BenchmarkConfig,\n) -&gt; tuple[PreTrainedModel, PreTrainedTokenizer]:\n    \"\"\"Load the model and tokenizer.\n\n    Args:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration\n\n    Returns:\n        The loaded model and tokenizer.\n    \"\"\"\n    config: \"PretrainedConfig\"\n    block_terminal_output()\n\n    model_id = model_config.model_id\n    task_group = dataset_config.task.task_group\n    ignore_mismatched_sizes = False\n\n    # Special case where there is a mismatch between the labels during training and\n    # testing\n    if dataset_config.task.task_group == TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION:\n        id2label = {0: \"0\", 1: \"1\"}\n    else:\n        id2label = dataset_config.id2label\n\n    config = load_hf_model_config(\n        model_id=model_id,\n        num_labels=len(id2label),\n        id2label=id2label,\n        label2id={label: idx for idx, label in id2label.items()},\n        revision=model_config.revision,\n        model_cache_dir=model_config.model_cache_dir,\n        api_key=benchmark_config.api_key,\n        trust_remote_code=benchmark_config.trust_remote_code,\n        run_with_cli=benchmark_config.run_with_cli,\n    )\n\n    model_kwargs = dict(\n        config=config,\n        ignore_mismatched_sizes=ignore_mismatched_sizes,\n        revision=model_config.revision,\n        token=benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True,\n        cache_dir=model_config.model_cache_dir,\n        trust_remote_code=benchmark_config.trust_remote_code,\n        torch_dtype=get_torch_dtype(\n            device=benchmark_config.device,\n            torch_dtype_is_set=config.to_dict().get(\"torch_dtype\") is not None,\n            bf16_available=(\n                torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n            ),\n        ),\n    )\n\n    # These are used when a timeout occurs\n    attempts_left = 5\n\n    model: PreTrainedModel | None = None\n    while True:\n        # Get the model class associated with the task group\n        model_cls_or_none: t.Type[\"PreTrainedModel\"] | None = get_class_by_name(\n            class_name=task_group_to_class_name(task_group=task_group),\n            module_name=\"transformers\",\n        )\n\n        # If the model class could not be found then raise an error\n        if not model_cls_or_none:\n            raise InvalidBenchmark(\n                f\"The task group {task_group.value!r} does not correspond to a \"\n                \"Hugging Face AutoModel type (such as \"\n                \"`AutoModelForSequenceClassification`).\"\n            )\n\n        # If the model is a DeBERTaV2 model then we ensure that\n        # `pooler_hidden_size` is the same size as `hidden_size`\n        if config.model_type == \"deberta-v2\":\n            config.pooler_hidden_size = config.hidden_size\n\n        try:\n            model_or_tuple = model_cls_or_none.from_pretrained(\n                model_config.model_id, **model_kwargs\n            )\n            break\n        except (KeyError, RuntimeError) as e:\n            if not model_kwargs[\"ignore_mismatched_sizes\"]:\n                logger.debug(\n                    f\"{type(e).__name__} occurred during the loading \"\n                    f\"of the {model_id!r} model. Retrying with \"\n                    \"`ignore_mismatched_sizes` set to True.\"\n                )\n                model_kwargs[\"ignore_mismatched_sizes\"] = True\n                continue\n            else:\n                raise InvalidModel(str(e))\n        except (TimeoutError, RequestError):\n            attempts_left -= 1\n            if attempts_left == 0:\n                raise InvalidModel(\"The model could not be loaded after 5 attempts.\")\n            logger.info(f\"Couldn't load the model {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n        except (OSError, ValueError) as e:\n            if \"checkpoint seems to be incorrect\" in str(e):\n                raise InvalidModel(\n                    f\"The model {model_id!r} has an incorrect checkpoint.\"\n                )\n            if \"trust_remote_code\" in str(e):\n                raise InvalidModel(\n                    f\"Loading the model {model_id!r} needs to trust remote code. \"\n                    \"If you trust the suppliers of this model, then you can enable \"\n                    \"this by setting the `--trust-remote-code` flag.\"\n                )\n            raise InvalidModel(\n                f\"The model {model_id!r} could not be loaded. The error was {e!r}.\"\n            )\n\n    if isinstance(model_or_tuple, tuple):\n        model = model_or_tuple[0]\n    else:\n        model = model_or_tuple\n\n    assert model is not None, \"The model should not be None.\"\n\n    model.eval()\n    model.to(benchmark_config.device)\n\n    if (\n        isinstance(model, PreTrainedModel)\n        and task_group == TaskGroup.QUESTION_ANSWERING\n    ):\n        model = setup_model_for_question_answering(model=model)\n\n    tokenizer = load_tokenizer(\n        model=model,\n        model_id=model_id,\n        trust_remote_code=benchmark_config.trust_remote_code,\n    )\n\n    return model, tokenizer\n\n\ndef get_model_repo_info(docs\n    model_id: str, revision: str, benchmark_config: BenchmarkConfig\n) -&gt; HFModelInfo | None:\n    \"\"\"Get the information about the model from the HF Hub or a local directory.\n\n    Args:\n        model_id:\n            The model ID.\n        revision:\n            The revision of the model.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The information about the model, or None if the model could not be found.\n    \"\"\"\n    token = benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True\n    hf_api = HfApi(token=token)\n    model_id, revision = model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n\n    # Get information on the model.\n    # The first case is when the model is a local model, in which case we create a dummy\n    # model info object.\n    model_info: HfApiModelInfo | None = None\n    if Path(model_id).is_dir():\n        logger.debug(f\"Checking for local model in {model_id}.\")\n        if all(\n            (Path(model_id) / required_file).exists()\n            for required_file in LOCAL_MODELS_REQUIRED_FILES\n        ):\n            model_info = HfApiModelInfo(id=model_id, tags=None, pipeline_tag=None)\n\n    # If the model does not exist locally, then we get the model info from the Hugging\n    # Face Hub\n    if model_info is None:\n        try:\n            model_info = hf_api.model_info(\n                repo_id=model_id, revision=revision, token=token\n            )\n        except (GatedRepoError, LocalTokenNotFoundError) as e:\n            try:\n                hf_whoami(token=token)\n                logger.warning(\n                    f\"Could not access the model {model_id} with the revision \"\n                    f\"{revision}. The error was {str(e)!r}.\"\n                )\n                return None\n            except LocalTokenNotFoundError:\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--api-key\",\n                    script_argument=\"api_key=&lt;your-api-key&gt;\",\n                    run_with_cli=benchmark_config.run_with_cli,\n                )\n        except (RepositoryNotFoundError, HFValidationError):\n            return None\n        except (OSError, RequestException):\n            if internet_connection_available():\n                raise HuggingFaceHubDown()\n            else:\n                raise NoInternetConnection()\n\n    # Get all the Hugging Face repository tags for the model. If the model is an adapter\n    # model, then we also get the tags for the base model\n    tags = model_info.tags or list()\n    has_base_model_tag = any(\n        tag.startswith(\"base_model:\") and tag.count(\":\") == 1 for tag in tags\n    )\n    base_model_id: str | None = None\n    if has_base_model_tag:\n        has_adapter_config = model_info.siblings is not None and any(\n            sibling.rfilename == \"adapter_config.json\"\n            for sibling in model_info.siblings\n        )\n        if has_adapter_config:\n            base_model_id = [\n                tag.split(\":\")[1]\n                for tag in tags\n                if tag.startswith(\"base_model:\") and tag.count(\":\") == 1\n            ][0]\n            base_model_info = hf_api.model_info(\n                repo_id=base_model_id,\n                revision=revision,\n                token=benchmark_config.api_key\n                or os.getenv(\"HUGGINGFACE_API_KEY\")\n                or True,\n            )\n            tags += base_model_info.tags or list()\n            tags = list(set(tags))\n\n    # Get the pipeline tag for the model. If it is not specified, then we determine it\n    # by checking the model's architecture as written in the model's Hugging Face config\n    pipeline_tag = model_info.pipeline_tag\n    if pipeline_tag is None:\n        hf_config = load_hf_model_config(\n            model_id=model_id,\n            num_labels=0,\n            id2label=dict(),\n            label2id=dict(),\n            revision=revision,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            api_key=benchmark_config.api_key,\n            trust_remote_code=benchmark_config.trust_remote_code,\n            run_with_cli=benchmark_config.run_with_cli,\n        )\n        class_names = hf_config.architectures\n        generative_class_names = [\n            class_name\n            for tag in GENERATIVE_PIPELINE_TAGS\n            for class_name in TASK_MAPPING[tag].values()\n        ]\n        if class_names is not None and any(\n            class_name in generative_class_names for class_name in class_names\n        ):\n            pipeline_tag = \"text-generation\"\n        else:\n            pipeline_tag = \"fill-mask\"\n\n    if benchmark_config.only_allow_safetensors:\n        # Check if any file ends with .safetensors\n        repo_files = hf_api.list_repo_files(repo_id=model_id, revision=revision)\n        has_safetensors = any(f.endswith(\".safetensors\") for f in repo_files)\n        if not has_safetensors:\n            msg = f\"Model {model_id} does not have safetensors weights available. \"\n            if benchmark_config.run_with_cli:\n                msg += \"Skipping since the `--only-allow-safetensors` flag is set.\"\n            else:\n                msg += (\n                    \"Skipping since the `only_allow_safetensors` argument is set \"\n                    \"to `True`.\"\n                )\n            raise InvalidModel(msg)\n\n    return HFModelInfo(\n        pipeline_tag=pipeline_tag, tags=tags, adapter_base_model_id=base_model_id\n    )\n\n\ndef load_tokenizer(docs\n    model: \"PreTrainedModel | None\", model_id: str, trust_remote_code: bool\n) -&gt; \"PreTrainedTokenizer\":\n    \"\"\"Load the tokenizer.\n\n    Args:\n        model:\n            The model, which is used to determine whether to add a prefix space to\n            the tokens. Can be None.\n        model_id:\n            The model identifier. Used for logging.\n        trust_remote_code:\n            Whether to trust remote code.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    loading_kwargs: dict[str, bool | str] = dict(\n        use_fast=True,\n        verbose=False,\n        trust_remote_code=trust_remote_code,\n        padding_side=\"right\",\n        truncation_side=\"right\",\n    )\n\n    # If the model is a subclass of a certain model types then we have to add a prefix\n    # space to the tokens, by the way the model is constructed.\n    if model is not None:\n        prefix_models = [\"Roberta\", \"GPT\", \"Deberta\"]\n        add_prefix = any(\n            model_type in type(model).__name__ for model_type in prefix_models\n        )\n        if add_prefix:\n            loading_kwargs[\"add_prefix_space\"] = True\n\n    num_retries = 5\n    for _ in range(num_retries):\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_id, **loading_kwargs)\n            break\n        except (JSONDecodeError, OSError, TypeError):\n            raise InvalidModel(f\"Could not load tokenizer for model {model_id!r}.\")\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load tokenizer for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n    else:\n        raise InvalidModel(\n            f\"Could not load tokenizer for model {model_id!r} after {num_retries} \"\n            \"attempts.\"\n        )\n\n    # Ensure that BOS, EOS and PAD tokens are set\n    tokenizer.bos_token, tokenizer.bos_token_id = get_bos_token(tokenizer=tokenizer)\n    tokenizer.eos_token, tokenizer.eos_token_id = get_eos_token(tokenizer=tokenizer)\n\n    return tokenizer\n\n\ndef get_torch_dtype(docs\n    device: torch.device, torch_dtype_is_set: bool, bf16_available: bool\n) -&gt; str | torch.dtype:\n    \"\"\"Get the torch dtype, used for loading the model.\n\n    Args:\n        device:\n            The device to use.\n        torch_dtype_is_set:\n            Whether the torch data type is set in the model configuration.\n        bf16_available:\n            Whether bfloat16 is available.\n\n    Returns:\n        The torch dtype.\n    \"\"\"\n    using_cuda = device == torch.device(\"cuda\")\n    if using_cuda and torch_dtype_is_set:\n        return \"auto\"\n    elif using_cuda and bf16_available:\n        return torch.bfloat16\n    elif using_cuda:\n        return torch.float16\n    return torch.float32\n\n\ndef load_hf_model_config(docs\n    model_id: str,\n    num_labels: int,\n    id2label: dict[int, str],\n    label2id: dict[str, int],\n    revision: str,\n    model_cache_dir: str | None,\n    api_key: str | None,\n    trust_remote_code: bool,\n    run_with_cli: bool,\n) -&gt; \"PretrainedConfig\":\n    \"\"\"Load the Hugging Face model configuration.\n\n    Args:\n        model_id:\n            The Hugging Face model ID.\n        num_labels:\n            The number of labels in the dataset.\n        id2label:\n            The mapping from label IDs to labels.\n        label2id:\n            The mapping from labels to label IDs.\n        revision:\n            The revision of the model.\n        model_cache_dir:\n            The directory to cache the model in.\n        api_key:\n            The Hugging Face API key.\n        trust_remote_code:\n            Whether to trust remote code.\n        run_with_cli:\n            Whether the script is being run with the CLI.\n\n    Returns:\n        The Hugging Face model configuration.\n    \"\"\"\n    while True:\n        try:\n            config = AutoConfig.from_pretrained(\n                model_id,\n                num_labels=num_labels,\n                id2label=id2label,\n                label2id=label2id,\n                revision=revision,\n                token=api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True,\n                trust_remote_code=trust_remote_code,\n                cache_dir=model_cache_dir,\n            )\n            if config.eos_token_id is not None and config.pad_token_id is None:\n                if isinstance(config.eos_token_id, list):\n                    config.pad_token_id = config.eos_token_id[0]\n                else:\n                    config.pad_token_id = config.eos_token_id\n            return config\n        except KeyError as e:\n            key = e.args[0]\n            raise InvalidModel(\n                f\"The model config for the model {model_id!r} could not be \"\n                f\"loaded, as the key {key!r} was not found in the config.\"\n            )\n        except (OSError, GatedRepoError) as e:\n            # TEMP: When the model is gated then we cannot set cache dir, for some\n            # reason (since transformers v4.38.2, still a problem in v4.48.0). This\n            # should be included back in when this is fixed.\n            if \"gated repo\" in str(e):\n                model_cache_dir = None\n                continue\n            raise InvalidModel(\n                f\"Couldn't load model config for {model_id!r}. The error was \"\n                f\"{e!r}. Skipping\"\n            )\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load model config for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n        except ValueError as e:\n            if \"awaiting a review from the repo authors\" in str(e):\n                raise InvalidModel(\n                    f\"The model {model_id!r} is awaiting a review from the repository \"\n                    \"authors. Please try again later.\"\n                )\n            if \"trust_remote_code\" in str(e):\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--trust-remote-code\",\n                    script_argument=\"trust_remote_code=True\",\n                    run_with_cli=run_with_cli,\n                )\n            raise InvalidModel(\n                f\"The config for the model {model_id!r} could not be loaded. The \"\n                f\"error was {e!r}.\"\n            )\n\ndocs\ndef setup_model_for_question_answering(model: \"PreTrainedModel\") -&gt; \"PreTrainedModel\":\n    \"\"\"Setup a model for question answering.\n\n    Args:\n        model:\n            The model to setup.\n\n    Returns:\n        The setup model.\n    \"\"\"\n    # Get the models' token type embedding children, if they exist\n    children = get_children_of_module(name=\"model\", module=model)\n\n    # If the model has token type embeddings then get them\n    if children:\n        # Get the list of attributes that are token type embeddings\n        attribute_list = list()\n        done = False\n        while not done:\n            for key, value in children.items():\n                attribute_list.append(key)\n                if isinstance(value, dict):\n                    children = value\n                else:\n                    done = True\n                break\n\n        # Get the token type embeddings\n        token_type_embeddings = model\n        for attribute in attribute_list:\n            token_type_embeddings = getattr(token_type_embeddings, attribute)\n\n        # If the token type embeddings has shape (1, ...) then set the shape to\n        # (2, ...) by randomly initializing the second token type embedding\n        if token_type_embeddings.weight.data.shape[0] == 1:\n            token_type_embeddings.weight.data = torch.cat(\n                (\n                    token_type_embeddings.weight.data,\n                    torch.rand_like(token_type_embeddings.weight.data),\n                ),\n                dim=0,\n            )\n            token_type_embeddings.num_embeddings = 2\n\n        # Set the model config to use the new type vocab size\n        model.config.type_vocab_size = 2\n\n    return model\n\n\ndef get_children_of_module(docs\n    name: str, module: nn.Module\n) -&gt; nn.Module | dict[str, t.Any] | None:\n    \"\"\"Get the children of a module.\n\n    Args:\n        name:\n            The name of the module.\n        module:\n            The module to get the children of.\n\n    Returns:\n        The children of the module, or None if the module has no children.\n    \"\"\"\n    if len(list(module.children())) == 0:\n        if name == \"token_type_embeddings\":\n            return module\n        else:\n            return None\n    else:\n        submodules = dict()\n        for subname, submodule in module.named_children():\n            children = get_children_of_module(name=subname, module=submodule)\n            if children:\n                submodules[subname] = children\n        return submodules\n\n\ndef align_model_and_tokenizer(docs\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    model_max_length: int,\n    raise_errors: bool = False,\n) -&gt; tuple[\"PreTrainedModel\", \"PreTrainedTokenizer\"]:\n    \"\"\"Aligns the model and the tokenizer.\n\n    Args:\n        model:\n            The model to fix.\n        tokenizer:\n            The tokenizer to fix.\n        model_max_length:\n            The maximum length of the model.\n        raise_errors:\n            Whether to raise errors instead of trying to fix them silently.\n\n    Returns:\n        The fixed model and tokenizer.\n    \"\"\"\n    # Ensure that the model max length is at most 5,000, to avoid OOM errors\n    model_max_length = min(model_max_length, 5_000)\n\n    if model_max_length &gt; 0:\n        tokenizer.model_max_length = model_max_length\n    else:\n        tokenizer.model_max_length = 512\n\n    # Move the model to the CPU, since otherwise we can't catch the IndexErrors when\n    # finding the maximum sequence length of the model\n    model_device = model.device\n    model.to(torch.device(\"cpu\"))\n\n    # Manually check that this model max length is valid for the model, and adjust\n    # otherwise\n    initial_max_length = tokenizer.model_max_length\n    for max_length in range(initial_max_length, 0, -1):\n        tokenizer.model_max_length = max_length\n        dummy_inputs = torch.full(\n            size=(1, max_length),\n            fill_value=DUMMY_FILL_VALUE,\n            dtype=torch.long,\n            device=model.device,\n        )\n        with torch.inference_mode():\n            try:\n                model(dummy_inputs, attention_mask=torch.ones_like(dummy_inputs))\n                break\n\n            # This happens if `max_length` is too large\n            except IndexError:\n                continue\n\n    # Move the model back to the original device\n    model.to(model_device)\n\n    # If there is a mismatch between the vocab size according to the tokenizer and\n    # the vocab size according to the model, we raise an error\n    if hasattr(model.config, \"vocab_size\"):\n        if model.config.vocab_size &lt; len(tokenizer):\n            if raise_errors:\n                raise InvalidModel(\n                    \"The vocab size of the tokenizer is larger than the vocab size of \"\n                    \"the model. As the --raise-errors option was specified, the \"\n                    \"embeddings of the model will not be automatically adjusted.\"\n                )\n            if hasattr(model, \"resize_token_embeddings\"):\n                model.resize_token_embeddings(new_num_tokens=tokenizer.vocab_size + 1)\n\n    if tokenizer.bos_token is None and tokenizer.eos_token is not None:\n        tokenizer.bos_token = tokenizer.eos_token\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n\n    return model, tokenizer\n\n\ndef task_group_to_class_name(task_group: TaskGroup) -&gt; str:docs\n    \"\"\"Convert a task group to a class name.\n\n    Args:\n        task_group:\n            The task group.\n\n    Returns:\n        The class name.\n    \"\"\"\n    pascal_case = task_group.title().replace(\"_\", \"\")\n    special_case_mapping = dict(\n        MultipleChoiceClassification=\"SequenceClassification\",\n        Speed=\"SequenceClassification\",\n    )\n    pascal_case = special_case_mapping.get(pascal_case, pascal_case)\n    return f\"AutoModelFor{pascal_case}\"\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/litellm/","title":"euroeval.benchmark_modules.litellm","text":"euroeval.benchmark_modules.litellm<p> source module euroeval.benchmark_modules.litellm </p> <p>Generative models from an inference API, using the LiteLLM framework.</p> <p> Classes </p> <ul> <li> <p>LiteLLMModel \u2014 A generative model from LiteLLM.</p> </li> </ul> <p> source class LiteLLMModel() </p> <p><p>Bases : BenchmarkModule</p></p> <p>A generative model from LiteLLM.</p> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>num_params \u2014 The number of parameters in the model.</p> </li> <li> <p>vocab_size \u2014 The vocabulary size of the model.</p> </li> <li> <p>model_max_length \u2014 The maximum length of the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> </ul> <p> source property LiteLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source method LiteLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>NeedsAdditionalArgument</p> </li> </ul> <p> source method LiteLLMModel.num_params() \u2192 int </p> <p>The number of parameters in the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The number of parameters in the model.</p> </li> </ul> <p> source method LiteLLMModel.vocab_size() \u2192 int </p> <p>The vocabulary size of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The vocabulary size of the model.</p> </li> </ul> <p> source method LiteLLMModel.model_max_length() \u2192 int </p> <p>The maximum length of the model.</p> <p> Returns </p> <ul> <li> <p>int \u2014 The maximum length of the model.</p> </li> </ul> <p> source property LiteLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property LiteLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source property LiteLLMModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source classmethod LiteLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> </ul> <p> source classmethod LiteLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> source method LiteLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/litellm/","title":"euroeval.benchmark_modules.litellm","text":"euroeval.benchmark_modules.litellm<p> docs module euroeval.benchmark_modules.litellm </p> <pre><code>\"\"\"Generative models from an inference API, using the LiteLLM framework.\"\"\"\n\nimport collections.abc as c\nimport itertools as it\nimport json\nimport logging\nimport os\nimport random\nimport re\nimport typing as t\nfrom functools import cached_property, partial\nfrom time import sleep\n\nimport litellm\nfrom datasets import DatasetDict\nfrom huggingface_hub import HfApi\nfrom huggingface_hub.errors import (\n    HFValidationError,\n    RepositoryNotFoundError,\n    RevisionNotFoundError,\n)\nfrom litellm.exceptions import (\n    APIConnectionError,\n    APIError,\n    AuthenticationError,\n    BadRequestError,\n    InternalServerError,\n    NotFoundError,\n    ServiceUnavailableError,\n    Timeout,\n)\nfrom litellm.types.utils import ModelResponse\nfrom requests.exceptions import RequestException\nfrom transformers import Trainer\n\nfrom ..constants import (\n    MAX_LOGPROBS,\n    REASONING_MAX_TOKENS,\n    TASK_GROUPS_USING_LOGPROBS,\n    TASKS_USING_JSON,\n)\nfrom ..data_models import BenchmarkConfig, GenerativeModelOutput, ModelConfig, Task\nfrom ..enums import (\n    BatchingPreference,\n    GenerativeType,\n    InferenceBackend,\n    ModelType,\n    TaskGroup,\n)\nfrom ..exceptions import (\n    InvalidBenchmark,\n    NeedsAdditionalArgument,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import create_model_cache_dir\nfrom .base import BenchmarkModule\nfrom .hf import HuggingFaceEncoderModel, load_hf_model_config, load_tokenizer\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nVOCAB_SIZE_MAPPING = {\n    # OpenAI models\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 50_257,\n    \"(code|text)-davinci-00[2-9]\": 50_281,\n    \"gpt-3.5-turbo(-16k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-(32k)?(-[0-9]{4})?\": 100_256,\n    \"gpt-4-[0-9]{4}-preview\": 100_256,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 100_256,\n    \"gpt-4-(vision|turbo)(-preview)?\": 100_256,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 100_256,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 200_019,\n    \"o[1-9](-mini|-preview)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": -1,\n}\n\n\nMODEL_MAX_LENGTH_MAPPING = {\n    # OpenAI models\n    \"(text-)?(ada|babbage|curie|davinci)(-001)?\": 2_050,\n    \"text-davinci-00[2-9]\": 4_098,\n    \"code-davinci-00[1-9]\": 8_002,\n    \"gpt-3.5-turbo-0613\": 4_096,\n    \"gpt-3.5-turbo(-[0-9]{4})?\": 16_385,\n    \"gpt-3.5-turbo-16k(-[0-9]{4})?\": 16_384,\n    \"gpt-4(-[0-9]{4})?\": 8_191,\n    \"gpt-4-32k(-[0-9]{4})?\": 32_767,\n    \"gpt-4-[0-9]{4}-preview\": 128_000,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    \"gpt-4-(vision|turbo)(-preview)?\": 128_000,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": 4_095,\n    \"gpt-4o(-mini)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    \"o1-(mini|preview)(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 128_000,\n    \"o1(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 200_000,\n    \"o[2-9](-mini|-preview)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": 200_000,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": 200_000,\n}\n\n\nNUM_PARAMS_MAPPING = {\n    # OpenAI models\n    \"(text-)?ada(-001)?\": 350_000_000,\n    \"(text-)?babbage(-001)?\": 3_000_000_000,\n    \"(text-)?curie(-001)?\": 13_000_000_000,\n    \"((text|code)-)?davinci(-00[1-9])?\": 175_000_000_000,\n    \"gpt-(3.5|4)-turbo-((16|32)k)?(-[0-9]{4})?\": -1,\n    \"gpt-4-[0-9]{4}-preview\": -1,\n    \"gpt-4-turbo(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4-(vision|turbo)(-preview)?\": -1,\n    \"gpt-3.5-turbo-instruct(-[0-9]{4})?\": -1,\n    \"gpt-4o(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"gpt-4o-mini(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    \"o[1-9](-mini|-preview)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\": -1,\n    # Anthropic models\n    \"claude-[1-9](-[1-9])?-(opus|sonnet|haiku)-[0-9]{8}\": -1,\n}\n\n\nREASONING_MODELS = [\"o[1-9](-mini|-preview)?(-[0-9]{4}-[0-9]{2}-[0-9]{2})?\"]\n\n\nclass LiteLLMModel(BenchmarkModule):docs\n    \"\"\"A generative model from LiteLLM.\"\"\"\n\n    fresh_model = False\n    batching_preference = BatchingPreference.SINGLE_SAMPLE\n    high_priority = False\n\n    @property\n    def generative_type(self) -&gt; GenerativeType | None:docs\n        \"\"\"Get the generative type of the model.\n\n        Returns:\n            The generative type of the model, or None if it has not been set yet.\n        \"\"\"\n        if re.fullmatch(\n            pattern=\"|\".join(REASONING_MODELS), string=self.model_config.model_id\n        ):\n            return GenerativeType.REASONING\n        else:\n            return GenerativeType.INSTRUCTION_TUNED\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        assert \"messages\" in inputs, \"The input must contain a 'messages' key.\"\n        assert len(inputs[\"messages\"]) == 1, (\n            \"API models only support single-sample batching.\"\n        )\n        messages = inputs[\"messages\"][0]\n\n        generation_kwargs: dict[str, t.Any] = dict(\n            model=self.model_config.model_id,\n            max_completion_tokens=(\n                REASONING_MAX_TOKENS\n                if self.generative_type == GenerativeType.REASONING\n                else self.dataset_config.max_generated_tokens\n            ),\n            stop=[],\n            temperature=0.0,\n            seed=4242,\n            api_key=self.benchmark_config.api_key,\n            api_base=self.benchmark_config.api_base,\n            api_version=self.benchmark_config.api_version,\n        )\n\n        if self.dataset_config.task.task_group in TASK_GROUPS_USING_LOGPROBS:\n            generation_kwargs[\"logprobs\"] = True\n            generation_kwargs[\"top_logprobs\"] = MAX_LOGPROBS\n\n        if self.dataset_config.task in TASKS_USING_JSON:\n            assert \"json\" in messages[0][\"content\"].lower(), (\n                \"Prompt must contain 'json' for JSON tasks.\"\n            )\n            generation_kwargs[\"response_format\"] = dict(type=\"json_object\")\n\n        # This drops generation kwargs that are not supported by the model\n        litellm.drop_params = True\n\n        # Extract the generated sequences from the model response. Some APIs cannot\n        # handle using newlines as stop sequences, so we try both.\n        num_attempts = 10\n        for _ in range(num_attempts):\n            try:\n                model_response = litellm.completion(\n                    messages=messages, max_retries=3, **generation_kwargs\n                )\n                break\n            except BadRequestError as e:\n                if \"stop_sequences\" in str(e).lower():\n                    generation_kwargs[\"stop\"] = None\n                elif \"you are not allowed to request logprobs\" in str(e).lower():\n                    generation_kwargs.pop(\"logprobs\")\n                    generation_kwargs.pop(\"top_logprobs\")\n                elif (\n                    \"'temperature' is not supported with this model.\" in str(e).lower()\n                ):\n                    generation_kwargs.pop(\"temperature\")\n                else:\n                    raise InvalidBenchmark(\n                        f\"Failed to generate text. The error message was: {e}\"\n                    )\n            except (\n                Timeout,\n                ServiceUnavailableError,\n                APIConnectionError,\n                InternalServerError,\n            ):\n                logger.debug(\n                    \"Service temporarily unavailable. Retrying in 5 seconds...\"\n                )\n                sleep(5)\n            except APIError as e:\n                raise InvalidBenchmark(\n                    f\"Failed to generate text. The error message was: {e}\"\n                )\n            except AuthenticationError:\n                raise NeedsAdditionalArgument(\n                    cli_argument=\"--api-key\",\n                    script_argument=\"api_key=&lt;your-api-key&gt;\",\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n        else:\n            raise InvalidBenchmark(\n                message=f\"Failed to generate text, after {num_attempts} attempts.\"\n            )\n\n        assert isinstance(model_response, ModelResponse)\n        model_response_choices = model_response.choices[0]\n        assert isinstance(model_response_choices, litellm.Choices)\n        generation_output = model_response_choices.message[\"content\"] or \"\"\n        generation_output = generation_output.strip()\n\n        # Structure the model output as a GenerativeModelOutput object\n        model_output = GenerativeModelOutput(sequences=[generation_output])\n        if hasattr(model_response_choices, \"logprobs\"):\n            logprobs_list: list[list[tuple[str, float]]] = [\n                [\n                    (top_logprob.token, top_logprob.logprob)\n                    for top_logprob in content.top_logprobs\n                ]\n                for content in model_response_choices.logprobs.content or list()\n            ]\n            model_output.scores = [logprobs_list]\n\n        return model_output\n\n    @cached_property\n    def num_params(self) -&gt; int:docs\n        \"\"\"The number of parameters in the model.\n\n        Returns:\n            The number of parameters in the model.\n        \"\"\"\n        for key, value in NUM_PARAMS_MAPPING.items():\n            if re.fullmatch(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                hf_api = HfApi()\n                try:\n                    repo_info = hf_api.model_info(\n                        repo_id=model_id,\n                        revision=self.model_config.revision,\n                        token=os.getenv(\"HUGGINGFACE_API_KEY\")\n                        or self.benchmark_config.api_key\n                        or True,\n                    )\n                except (\n                    RepositoryNotFoundError,\n                    RevisionNotFoundError,\n                    RequestException,\n                    HFValidationError,\n                ):\n                    repo_info = None\n\n                if (\n                    repo_info is not None\n                    and hasattr(repo_info, \"safetensors\")\n                    and repo_info.safetensors is not None\n                    and \"total\" in repo_info.safetensors\n                ):\n                    return repo_info.safetensors[\"total\"]\n                elif (\n                    hasattr(hf_config, \"num_params\")\n                    and hf_config.num_params is not None\n                ):\n                    return hf_config.num_params\n\n        return -1\n\n    @cached_property\n    def vocab_size(self) -&gt; int:docs\n        \"\"\"The vocabulary size of the model.\n\n        Returns:\n            The vocabulary size of the model.\n        \"\"\"\n        for key, value in VOCAB_SIZE_MAPPING.items():\n            if re.fullmatch(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                tokenizer = load_tokenizer(\n                    model=None,\n                    model_id=model_id,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                )\n\n                if (\n                    hasattr(hf_config, \"vocab_size\")\n                    and hf_config.vocab_size is not None\n                ):\n                    vocab_size = hf_config.vocab_size\n                elif (\n                    hasattr(tokenizer, \"vocab_size\")\n                    and tokenizer.vocab_size is not None\n                ):\n                    vocab_size = tokenizer.vocab_size\n                else:\n                    vocab_size = -1\n                return vocab_size\n\n        return -1\n\n    @cached_property\n    def model_max_length(self) -&gt; int:docs\n        \"\"\"The maximum length of the model.\n\n        Returns:\n            The maximum length of the model.\n        \"\"\"\n        for key, value in MODEL_MAX_LENGTH_MAPPING.items():\n            if re.fullmatch(pattern=key, string=self.model_config.model_id) is not None:\n                return value\n\n        if self.model_config.model_id.startswith(\"huggingface/\"):\n            model_id = self.model_config.model_id.split(sep=\"/\", maxsplit=1)[-1]\n            if HuggingFaceEncoderModel.model_exists(\n                model_id=model_id, benchmark_config=self.benchmark_config\n            ):\n                hf_config = load_hf_model_config(\n                    model_id=model_id,\n                    num_labels=self.dataset_config.num_labels,\n                    id2label=self.dataset_config.id2label,\n                    label2id=self.dataset_config.label2id,\n                    revision=self.model_config.revision,\n                    model_cache_dir=self.model_config.model_cache_dir,\n                    api_key=self.benchmark_config.api_key,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                    run_with_cli=self.benchmark_config.run_with_cli,\n                )\n\n                tokenizer = load_tokenizer(\n                    model=None,\n                    model_id=model_id,\n                    trust_remote_code=self.benchmark_config.trust_remote_code,\n                )\n\n                all_max_lengths: list[int] = list()\n\n                # Add the registered max length of the tokenizer\n                if hasattr(\n                    tokenizer, \"model_max_length\"\n                ) and tokenizer.model_max_length &lt; int(1e30):\n                    all_max_lengths.append(tokenizer.model_max_length)\n\n                # Add the max length derived from the model's input sizes\n                if hasattr(tokenizer, \"max_model_input_sizes\"):\n                    all_max_lengths.extend(\n                        [\n                            size\n                            for size in tokenizer.max_model_input_sizes.values()\n                            if size is not None\n                        ]\n                    )\n\n                # Add max length candidates from the model's configuration\n                candidate_config_max_lengths = [\n                    \"max_position_embeddings\",\n                    \"max_sequence_length\",\n                    \"model_max_length\",\n                    \"sliding_window\",\n                    \"sliding_window_size\",\n                    \"n_positions\",\n                ]\n                for candidate_config_max_length in candidate_config_max_lengths:\n                    if (\n                        hasattr(hf_config, candidate_config_max_length)\n                        and (value := getattr(hf_config, candidate_config_max_length))\n                        is not None\n                    ):\n                        all_max_lengths.append(value)\n\n                # To avoid models having artificially low max lengths, we remove any max\n                # lengths that are less than 128\n                all_max_lengths = [\n                    max_length for max_length in all_max_lengths if max_length &gt;= 128\n                ]\n\n                if len(list(all_max_lengths)) &gt; 0:\n                    return min(list(all_max_lengths))\n\n        return -1\n\n    @propertydocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `data_collator` property has not been implemented for LiteLLM models.\"\n        )\n\n    @property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        match self.dataset_config.task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                return partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.TEXT_TO_TEXT:\n                return text_to_text.extract_labels_from_generation\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                return partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.QUESTION_ANSWERING:\n                return question_answering.extract_labels_from_generation\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task group: {self.dataset_config.task.task_group}.\"\n                )\n\n    @property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `trainer_class` property has not been implemented for LiteLLM models.\"\n        )\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        if model_id in litellm.model_list:\n            return True\n\n        num_attempts = 10\n        for _ in range(num_attempts):\n            try:\n                litellm.completion(\n                    messages=[dict(role=\"user\", content=\"X\")],\n                    model=model_id,\n                    max_tokens=1,\n                    api_key=benchmark_config.api_key,\n                    api_base=benchmark_config.api_base,\n                    api_version=benchmark_config.api_version,\n                )\n                return True\n            except APIError as e:\n                if \"'503 Service Unavailable\" not in str(e):\n                    raise e\n                logger.warning(\n                    f\"Failed to check if model {model_id!r} exists. Retrying in \"\n                    f\"{num_attempts} seconds...\"\n                )\n                sleep(10)\n            except (BadRequestError, NotFoundError):\n                candidate_models = [\n                    candidate_model_id\n                    for candidate_model_id in litellm.model_list\n                    if candidate_model_id.startswith(model_id)\n                ]\n                match len(candidate_models):\n                    case 0:\n                        pass\n                    case 1:\n                        logger.warning(\n                            f\"Could not find the model ID {model_id!r}. Did you mean \"\n                            f\"{candidate_models[0]!r}?\"\n                        )\n                    case _:\n                        candidate_models_str = \"', '\".join(candidate_models)\n                        logger.warning(\n                            f\"Could not find the model ID {model_id!r}. Did you mean \"\n                            f\"any of the following model IDs: '{candidate_models_str}'?\"\n                        )\n                return False\n        else:\n            logger.error(\n                f\"Failed to check if model {model_id!r} exists after {num_attempts} \"\n                \"attempts. Assuming it does not exist.\"\n            )\n            return False\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        return ModelConfig(\n            model_id=model_id,\n            revision=\"main\",\n            task=\"text-generation\",\n            languages=list(),\n            merge=False,\n            inference_backend=InferenceBackend.LITELLM,\n            model_type=ModelType.GENERATIVE,\n            fresh=False,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=None,\n        )\n\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        if task.task_group == TaskGroup.QUESTION_ANSWERING:\n            dataset = dataset.map(\n                lambda examples: dict(\n                    label=[\n                        dict(\n                            id=id,\n                            answers=dict(\n                                answer_start=answer_dct[\"answer_start\"],\n                                text=[\n                                    answer_text.lower()\n                                    for answer_text in answer_dct[\"text\"]\n                                ],\n                            ),\n                        )\n                        for id, answer_dct in zip(examples[\"id\"], examples[\"answers\"])\n                    ]\n                ),\n                batched=True,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            )\n\n        if self.benchmark_config.few_shot:\n            few_shot_examples = self._extract_few_shot_examples(\n                dataset=dataset, task=task, itr_idx=itr_idx\n            )\n        else:\n            few_shot_examples = list()\n\n        dataset[\"test\"] = dataset[\"test\"].map(\n            partial(self._apply_prompt, few_shot_examples=few_shot_examples, task=task),\n            batched=True,\n            load_from_cache_file=False,\n            keep_in_memory=True,\n        )\n\n        return dataset\n\n    def _extract_few_shot_examples(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; list[dict[str, t.Any]]:\n        \"\"\"Extract few-shot examples from a dataset.\n\n        This will always extract the examples from the training split.\n\n        We ensure that the few-shot examples are unique by picking them one at a time.\n\n        Args:\n            dataset:\n                The dataset to extract the few-shot examples from.\n            task:\n                The task that is being benchmarked.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        random_seed = 4242 + itr_idx\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, t.Any]] = list()\n        shuffled_train = dataset[\"train\"].shuffle(seed=random_seed)\n\n        match task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                labels = it.cycle(self.dataset_config.labels)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: x[\"label\"].lower() == label.lower()\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case TaskGroup.TEXT_TO_TEXT:\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                labels = it.cycle(\n                    [\n                        label.lower()\n                        for label in self.dataset_config.labels\n                        if label.lower().startswith(\"b-\")\n                    ]\n                )\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: label in [tag.lower() for tag in x[\"labels\"]]\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"tokens\"] != example[\"tokens\"]\n                    )\n\n            case TaskGroup.QUESTION_ANSWERING:\n                # Locate the maximum number of tokens that constitutes a short example\n                for max_num_tokens in [512, 1024, 2048, 4096, 8192]:\n                    train_with_short_examples = dataset[\"train\"].filter(\n                        lambda example: len(example[\"context\"]) &lt; max_num_tokens\n                    )\n                    num_short_examples = len(train_with_short_examples)\n                    if num_short_examples &gt;= self.dataset_config.num_few_shot_examples:\n                        break\n                else:\n                    raise InvalidBenchmark(\n                        \"Could not find enough short examples for few-shot learning.\"\n                    )\n\n                shuffled_train = train_with_short_examples.shuffle(seed=random_seed)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"context\"] != example[\"context\"]\n                    )\n\n            case _:\n                raise NotImplementedError(f\"Unsupported task group: {task.task_group}.\")\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_prompt(\n        self,\n        examples: dict[str, t.Any],\n        few_shot_examples: list[dict[str, t.Any]],\n        task: Task,\n    ) -&gt; dict[str, t.Any]:\n        \"\"\"Apply prompt template to an example, potentially with few-shot examples.\n\n        Args:\n            examples:\n                The examples to apply the few-shot examples to.\n            few_shot_examples:\n                The few-shot examples to apply.\n            task:\n                The task that is being benchmarked.\n\n        Returns:\n            The example with the few-shot examples applied.\n        \"\"\"\n\n        def create_prompt(**kwargs: str) -&gt; tuple[str, str]:\n            \"\"\"Create a prompt from the given keyword arguments.\n\n            Args:\n                kwargs:\n                    The keyword arguments to use in the prompt.\n\n            Returns:\n                A pair (prompt, label), where \"label\" is an empty string if the model is\n                not instruction tuned (as in this case it is included in the prompt).\n            \"\"\"\n            label_key = \"label\" if \"label\" in kwargs else \"target_text\"\n            label = kwargs.pop(label_key)\n            label_mapping = self.dataset_config.prompt_label_mapping\n            label = label_mapping.get(label, label)\n            prompt = self.dataset_config.instruction_prompt.format(**kwargs)\n            return prompt, label\n\n        match task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"label\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), label=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case TaskGroup.TEXT_TO_TEXT:\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        target_text=example[\"target_text\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), target_text=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case TaskGroup.TOKEN_CLASSIFICATION:\n\n                def create_label(example: dict) -&gt; str:\n                    prompt_labels = self.dataset_config.prompt_label_mapping.values()\n                    labels: dict[str, list[str]] = {\n                        prompt_label: list() for prompt_label in prompt_labels\n                    }\n                    for token, label in zip(example[\"tokens\"], example[\"labels\"]):\n                        label = label.lower()\n                        if label == \"o\":\n                            continue\n                        prompt_label = self.dataset_config.prompt_label_mapping[label]\n                        if label.startswith(\"b-\"):\n                            labels[prompt_label].append(token)\n                        elif label.startswith(\"i-\"):\n                            labels[prompt_label][-1] += \" \" + token\n                    return json.dumps(labels, ensure_ascii=False)\n\n                few_shot_sections = [\n                    create_prompt(\n                        text=\" \".join(example[\"tokens\"]).replace(\"\\n\", \" \").strip(),\n                        label=create_label(example=example),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=\" \".join(tokens).replace(\"\\n\", \" \").strip(), label=\"\"\n                    )\n                    for tokens in examples[\"tokens\"]\n                ]\n\n            case TaskGroup.QUESTION_ANSWERING:\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"context\"].replace(\"\\n\", \" \").strip(),\n                        question=example[\"question\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \"),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=context.replace(\"\\n\", \" \").strip(),\n                        question=question.replace(\"\\n\", \" \").strip(),\n                        label=\"\",\n                    )\n                    for context, question in zip(\n                        examples[\"context\"], examples[\"question\"]\n                    )\n                ]\n\n            case _:\n                raise NotImplementedError(f\"Unsupported task group: {task.task_group}.\")\n\n        few_shot_messages = [\n            dict(role=role, content=content)\n            for prompt, label in few_shot_sections\n            for role, content in [(\"user\", prompt), (\"assistant\", label)]\n        ]\n\n        messages_list = [\n            few_shot_messages + [dict(role=\"user\", content=prompt)]\n            for prompt, _ in new_sections\n        ]\n\n        examples[\"messages\"] = messages_list\n        return examples\n</code></pre>"},{"location":"api/euroeval/benchmark_modules/vllm/","title":"euroeval.benchmark_modules.vllm","text":"euroeval.benchmark_modules.vllm<p> source module euroeval.benchmark_modules.vllm </p> <p>Generative models using the vLLM inference framework.</p> <p> Classes </p> <ul> <li> <p>VLLMModel \u2014 A generative model using the vLLM inference framework.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>load_model_and_tokenizer \u2014 Load the model and tokenizer.</p> </li> <li> <p>load_tokenizer \u2014 Load the tokenizer.</p> </li> <li> <p>clear_vllm \u2014 Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> </li> <li> <p>get_end_of_reasoning_token_id \u2014 Get the end of reasoning token ID for a generative model.</p> </li> </ul> <p> source class VLLMModel(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) </p> <p><p>Bases : HuggingFaceEncoderModel</p></p> <p>A generative model using the vLLM inference framework.</p> <p>Initialise the vLLM model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Attributes </p> <ul> <li> <p>generative_type :  GenerativeType | None \u2014 Get the generative type of the model.</p> </li> <li> <p>data_collator :  c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator used to prepare samples during finetuning.</p> </li> <li> <p>compute_metrics :  ComputeMetricsFunction \u2014 The function used to compute the metrics.</p> </li> <li> <p>extract_labels_from_generation :  ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> <li> <p>trainer_class :  t.Type[Trainer] \u2014 The Trainer class to use for finetuning.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>prepare_dataset \u2014 Prepare the dataset for the model.</p> </li> <li> <p>generate \u2014 Generate outputs from the model.</p> </li> <li> <p>model_exists \u2014 Check if a model exists.</p> </li> <li> <p>get_model_config \u2014 Fetch the model configuration.</p> </li> </ul> <p> source property VLLMModel.generative_type: GenerativeType | None </p> <p>Get the generative type of the model.</p> <p> Returns </p> <ul> <li> <p>GenerativeType | None \u2014 The generative type of the model, or None if it has not been set yet.</p> </li> </ul> <p> source property VLLMModel.extract_labels_from_generation: ExtractLabelsFunction </p> <p>The function used to extract the labels from the generated output.</p> <p> Returns </p> <ul> <li> <p>ExtractLabelsFunction \u2014 The function used to extract the labels from the generated output.</p> </li> </ul> <p> source method VLLMModel.prepare_dataset(dataset: DatasetDict, task: Task, itr_idx: int) \u2192 DatasetDict </p> <p>Prepare the dataset for the model.</p> <p>This includes things like tokenisation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to prepare.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task to prepare the dataset for.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the dataset in the iterator.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetDict \u2014 The prepared dataset.</p> </li> </ul> <p> source method VLLMModel.generate(inputs: dict) \u2192 GenerativeModelOutput </p> <p>Generate outputs from the model.</p> <p> Parameters </p> <ul> <li> <p>inputs :  dict \u2014</p> <p>A batch of inputs to pass through the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The generated model outputs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source classmethod VLLMModel.model_exists(model_id: str, benchmark_config: BenchmarkConfig) \u2192 bool | NeedsExtraInstalled | NeedsEnvironmentVariable </p> <p>Check if a model exists.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool | NeedsExtraInstalled | NeedsEnvironmentVariable \u2014 Whether the model exists, or an error describing why we cannot check whether the model exists.</p> </li> </ul> <p> source classmethod VLLMModel.get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetch the model configuration.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source property VLLMModel.data_collator: c.Callable[[list[t.Any]], dict[str, t.Any]] </p> <p>The data collator used to prepare samples during finetuning.</p> <p> Returns </p> <ul> <li> <p>c.Callable[[list[t.Any]], dict[str, t.Any]] \u2014 The data collator.</p> </li> </ul> <p> source property VLLMModel.trainer_class: t.Type[Trainer] </p> <p>The Trainer class to use for finetuning.</p> <p> Returns </p> <ul> <li> <p>t.Type[Trainer] \u2014 The Trainer class.</p> </li> </ul> <p> source load_model_and_tokenizer(model_config: ModelConfig, benchmark_config: BenchmarkConfig, output_scores: bool) \u2192 tuple[LLM, PreTrainedTokenizer] </p> <p>Load the model and tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>output_scores :  bool \u2014</p> <p>Whether to output scores.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[LLM, PreTrainedTokenizer] \u2014 The loaded model and tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> <li> <p>InvalidModel</p> </li> </ul> <p> source load_tokenizer(model_id: str, revision: str, adapter_base_model_id: str | None, trust_remote_code: bool, model_max_length: int, model_cache_dir: str, token: str | bool) \u2192 PreTrainedTokenizer </p> <p>Load the tokenizer.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model identifier.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The base model ID for the adapter model. Can be None if the model is not an adapter model.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code.</p> </li> <li> <p>model_max_length :  int \u2014</p> <p>The maximum length of the model.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The cache directory for the model.</p> </li> <li> <p>token :  str | bool \u2014</p> <p>The Hugging Face API token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>PreTrainedTokenizer \u2014 The loaded tokenizer.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source clear_vllm() \u2192 None </p> <p>Clear the GPU memory used by the vLLM model, enabling re-initialisation.</p> <p> source get_end_of_reasoning_token_id(model: LLM, tokenizer: PreTrainedTokenizer) \u2192 int | None </p> <p>Get the end of reasoning token ID for a generative model.</p> <p>This assumes that the reasoning token is of the form  and that the end of reasoning token is  (for X being any string without spaces).</p> <p> Parameters </p> <ul> <li> <p>model :  LLM \u2014</p> <p>The vLLM model.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int | None \u2014 The end of reasoning token ID, or None if it could not be found.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_modules/vllm/","title":"euroeval.benchmark_modules.vllm","text":"euroeval.benchmark_modules.vllm<p> docs module euroeval.benchmark_modules.vllm </p> <pre><code>\"\"\"Generative models using the vLLM inference framework.\"\"\"\n\nimport collections.abc as c\nimport importlib.util\nimport itertools as it\nimport json\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport typing as t\nfrom functools import partial\nfrom pathlib import Path\nfrom time import sleep\nfrom types import MethodType\n\nimport torch\nfrom datasets import DatasetDict\nfrom huggingface_hub import snapshot_download\nfrom pydantic import conlist, create_model\nfrom tqdm.auto import tqdm\nfrom transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizer, Trainer\nfrom urllib3.exceptions import RequestError\n\nfrom ..constants import (\n    GENERATIVE_PIPELINE_TAGS,\n    MAX_LOGPROBS,\n    MERGE_TAGS,\n    REASONING_MAX_TOKENS,\n    TASK_GROUPS_USING_LOGPROBS,\n    TASKS_USING_JSON,\n)\nfrom ..data_models import (\n    BenchmarkConfig,\n    DatasetConfig,\n    GenerativeModelOutput,\n    ModelConfig,\n    Task,\n)\nfrom ..enums import (\n    BatchingPreference,\n    GenerativeType,\n    InferenceBackend,\n    ModelType,\n    TaskGroup,\n)\nfrom ..exceptions import (\n    InvalidBenchmark,\n    InvalidModel,\n    NeedsEnvironmentVariable,\n    NeedsExtraInstalled,\n)\nfrom ..languages import get_all_languages\nfrom ..task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom ..types import ExtractLabelsFunction\nfrom ..utils import (\n    clear_memory,\n    create_model_cache_dir,\n    get_bos_token,\n    get_end_of_chat_token_ids,\n    get_eos_token,\n    log_once,\n    should_prompts_be_stripped,\n)\nfrom .hf import HuggingFaceEncoderModel, get_model_repo_info, load_hf_model_config\n\nif t.TYPE_CHECKING or importlib.util.find_spec(\"vllm\") is not None:\n    from vllm import LLM, RequestOutput, SamplingParams\n    from vllm.lora.request import LoRARequest\n    from vllm.sampling_params import GuidedDecodingParams\n\n    try:\n        from vllm.model_executor.parallel_utils.parallel_state import (\n            destroy_model_parallel,\n        )\n    except ImportError:\n        from vllm.distributed.parallel_state import destroy_model_parallel\n\nif t.TYPE_CHECKING or importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass VLLMModel(HuggingFaceEncoderModel):docs\n    \"\"\"A generative model using the vLLM inference framework.\"\"\"\n\n    fresh_model = False\n    batching_preference = BatchingPreference.ALL_AT_ONCE\n    high_priority = True\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        dataset_config: DatasetConfig,\n        benchmark_config: BenchmarkConfig,\n    ) -&gt; None:\n        \"\"\"Initialise the vLLM model.\n\n        Args:\n            model_config:\n                The model configuration.\n            dataset_config:\n                The dataset configuration.\n            benchmark_config:\n                The benchmark configuration.\n        \"\"\"\n        if (\n            importlib.util.find_spec(\"vllm\") is None\n            or importlib.util.find_spec(\"ray\") is None\n        ):\n            raise NeedsExtraInstalled(extra=\"generative\")\n\n        output_scores = dataset_config.task.task_group in TASK_GROUPS_USING_LOGPROBS\n        model, tokenizer = load_model_and_tokenizer(\n            model_config=model_config,\n            benchmark_config=benchmark_config,\n            output_scores=output_scores,\n        )\n        self._model: LLM = model\n        self._tokenizer: PreTrainedTokenizer = tokenizer\n        self.end_of_reasoning_token_id = get_end_of_reasoning_token_id(\n            model=self._model, tokenizer=self._tokenizer\n        )\n\n        # We specify `HuggingFaceEncoderModel` here instead of `VLLMModel`, as we want\n        # to call the `__init__` method of the `BenchmarkModule` class.\n        super(HuggingFaceEncoderModel, self).__init__(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n        self.buffer[\"output_scores\"] = output_scores\n        self.buffer[\"instruction_model\"] = self._tokenizer.chat_template is not None\n        if self.model_config.adapter_base_model_id is not None:\n            adapter_path = snapshot_download(\n                repo_id=self.model_config.model_id,\n                cache_dir=Path(self.model_config.model_cache_dir),\n            )\n            self.buffer[\"lora_request\"] = LoRARequest(\n                lora_name=\"adapter\", lora_int_id=1, lora_path=adapter_path\n            )\n\n    @property\n    def generative_type(self) -&gt; GenerativeType | None:docs\n        \"\"\"Get the generative type of the model.\n\n        Returns:\n            The generative type of the model, or None if it has not been set yet.\n        \"\"\"\n        if not hasattr(self, \"_tokenizer\"):\n            return None\n        elif self.end_of_reasoning_token_id is not None:\n            return GenerativeType.REASONING\n        elif self._tokenizer.chat_template is not None:\n            return GenerativeType.INSTRUCTION_TUNED\n        else:\n            return GenerativeType.BASE\n\n    @property\n    def extract_labels_from_generation(self) -&gt; ExtractLabelsFunction:docs\n        \"\"\"The function used to extract the labels from the generated output.\n\n        Returns:\n            The function used to extract the labels from the generated output.\n        \"\"\"\n        match self.dataset_config.task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                return partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.TEXT_TO_TEXT:\n                return text_to_text.extract_labels_from_generation\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                return partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.QUESTION_ANSWERING:\n                return question_answering.extract_labels_from_generation\n            case _:\n                raise NotImplementedError(\n                    f\"Unsupported task group: {self.dataset_config.task.task_group}.\"\n                )\n\n    def prepare_dataset(docs\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; DatasetDict:\n        \"\"\"Prepare the dataset for the model.\n\n        This includes things like tokenisation.\n\n        Args:\n            dataset:\n                The dataset to prepare.\n            task:\n                The task to prepare the dataset for.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The prepared dataset.\n        \"\"\"\n        if task.task_group == TaskGroup.QUESTION_ANSWERING:\n            dataset = dataset.map(\n                lambda examples: dict(\n                    label=[\n                        dict(\n                            id=id,\n                            answers=dict(\n                                answer_start=answer_dct[\"answer_start\"],\n                                text=[\n                                    answer_text.lower()\n                                    for answer_text in answer_dct[\"text\"]\n                                ],\n                            ),\n                        )\n                        for id, answer_dct in zip(examples[\"id\"], examples[\"answers\"])\n                    ]\n                ),\n                batched=True,\n                load_from_cache_file=False,\n                keep_in_memory=True,\n            )\n\n        if self.benchmark_config.few_shot:\n            few_shot_examples = self._extract_few_shot_examples(\n                dataset=dataset, task=task, itr_idx=itr_idx\n            )\n        else:\n            few_shot_examples = list()\n\n        dataset[\"test\"] = dataset[\"test\"].map(\n            partial(self._apply_prompt, few_shot_examples=few_shot_examples, task=task),\n            batched=True,\n            load_from_cache_file=False,\n            keep_in_memory=True,\n        )\n\n        return dataset\n\n    def generate(self, inputs: dict) -&gt; GenerativeModelOutput:docs\n        \"\"\"Generate outputs from the model.\n\n        Args:\n            inputs:\n                A batch of inputs to pass through the model.\n\n        Returns:\n            The generated model outputs.\n        \"\"\"\n        # Define which tokens to use as stopping criteria. We want to use the padding\n        # token, end-of-sentence token, and a double newline if the model isn't\n        # instruction tuned (since these separate the few-shot examples in the input in\n        # this case)\n        stop_tokens: list[str] = list()\n        if self.buffer[\"instruction_model\"] is False:\n            stop_tokens.append(\"\\n\\n\")\n        if self._tokenizer.pad_token_id is not None:\n            stop_tokens.append(self._tokenizer.pad_token)\n        if self._tokenizer.eos_token_id is not None:\n            stop_tokens.append(self._tokenizer.eos_token)\n            if self._tokenizer.pad_token_id is None:\n                self._tokenizer.pad_token_id = self._tokenizer.eos_token_id\n                self._tokenizer.pad_token = self._tokenizer.eos_token\n        if (\n            self._tokenizer.bos_token_id is not None\n            and self._tokenizer.pad_token_id is None\n        ):\n            self._tokenizer.pad_token_id = self._tokenizer.bos_token_id\n            self._tokenizer.pad_token = self._tokenizer.bos_token\n        elif (\n            self._tokenizer.eos_token_id is not None\n            and self._tokenizer.pad_token_id is None\n        ):\n            self._tokenizer.pad_token_id = self._tokenizer.eos_token_id\n            self._tokenizer.pad_token = self._tokenizer.eos_token\n        elif self._tokenizer.pad_token_id is None:\n            pad_token_candidates = [\"&lt;pad&gt;\", \"[pad]\", \"&lt;|endoftext|&gt;\", \"&lt;|im_end|&gt;\"]\n            pad_token_candidates.extend([c.upper() for c in pad_token_candidates])\n            for candidate in pad_token_candidates:\n                if candidate in self._tokenizer.get_vocab():\n                    pad_token_id = self._tokenizer.get_vocab()[candidate]\n                    self._tokenizer.pad_token = candidate\n                    self._tokenizer.pad_token_id = pad_token_id\n                    break\n            else:\n                raise InvalidModel(\n                    \"Could not find a suitable token to use as a padding token, since \"\n                    \"the model does not have a BOS, EOS, or padding token, and does \"\n                    f\"not have any of the following tokens in its vocabulary: \"\n                    f\"{pad_token_candidates}.\"\n                )\n\n        assert self._tokenizer.pad_token_id is not None\n\n        # Add end of chat token as a stopping token, if it exists\n        end_of_chat_token_ids = get_end_of_chat_token_ids(tokenizer=self._tokenizer)\n        if end_of_chat_token_ids is not None:\n            end_of_chat_token = self._tokenizer.decode(end_of_chat_token_ids).strip()\n            if end_of_chat_token:\n                stop_tokens.append(end_of_chat_token)\n\n        if self.dataset_config.task in TASKS_USING_JSON:\n            ner_tag_names = list(self.dataset_config.prompt_label_mapping.values())\n            keys_and_their_types: dict[str, t.Any] = {\n                tag_name: (conlist(str, max_length=5), ...)\n                for tag_name in ner_tag_names\n            }\n            pydantic_class = create_model(\"AnswerFormat\", **keys_and_their_types)\n            schema = pydantic_class.model_json_schema()\n            guided_decoding = GuidedDecodingParams(\n                json=schema, backend=\"outlines\", whitespace_pattern=r\" ?\"\n            )\n        else:\n            guided_decoding = None\n\n        # Define the parameters used for vLLM generation\n        max_tokens: int = (\n            REASONING_MAX_TOKENS\n            if self.generative_type == GenerativeType.REASONING\n            else self.dataset_config.max_generated_tokens\n        )\n        sampling_params = SamplingParams(\n            max_tokens=max_tokens,\n            logprobs=MAX_LOGPROBS if self.buffer[\"output_scores\"] else None,\n            temperature=0.0,\n            stop=[stop_token for stop_token in stop_tokens if stop_token],\n            guided_decoding=guided_decoding,\n        )\n\n        # If any of the prompts are empty then we need to replace them with a BOS token\n        # so that the vLLM model can generate from them\n        prompts: list[str] = inputs[\"text\"]\n        if any(len(prompt) == 0 for prompt in prompts):\n            logger.debug(\"Found empty prompts, replacing with BOS token.\")\n            prompts = [\n                prompt if len(prompt) &gt; 0 else str(self._tokenizer.bos_token)\n                for prompt in prompts\n            ]\n\n        # Strip the prompts if the model's tokeniser requires it\n        labels_to_be_generated = list(self.dataset_config.prompt_label_mapping.values())\n        if len(labels_to_be_generated) == 0:\n            labels_to_be_generated = [\"negative\", \"positive\"]\n        if not self.buffer.get(\n            \"instruction_model\", False\n        ) and should_prompts_be_stripped(\n            labels_to_be_generated=labels_to_be_generated, tokenizer=self._tokenizer\n        ):\n            log_once(message=\"Stripping prompts.\", level=logging.DEBUG)\n            prompts = [prompt.strip() for prompt in prompts]\n\n        # Generate sequences using vLLM\n        input_is_a_test = len(prompts) == 1 and len(set(prompts[0])) == 1\n        raw_outputs = self._model.generate(\n            prompts=prompts,\n            sampling_params=sampling_params,\n            use_tqdm=(not input_is_a_test),\n            lora_request=self.buffer.get(\"lora_request\"),\n        )\n        completion_ids: list[list[int]] = [\n            output.outputs[0].token_ids for output in raw_outputs\n        ]\n        if self.end_of_reasoning_token_id in completion_ids[0]:\n            completion_ids = [\n                token_ids[token_ids.index(self.end_of_reasoning_token_id) + 2 :]\n                if self.end_of_reasoning_token_id in token_ids\n                else token_ids\n                for token_ids in completion_ids\n            ]\n        completions = self._tokenizer.batch_decode(\n            sequences=[\n                torch.LongTensor(completion_id) for completion_id in completion_ids\n            ],\n            skip_special_tokens=True,\n        )\n        completions = [completion.strip() for completion in completions]\n\n        # Add logprobs scores to the output\n        if self.buffer[\"output_scores\"]:\n            scores: list[list[list[tuple[str, float]]]] = [\n                [\n                    [\n                        (obj.decoded_token, obj.logprob)\n                        for obj in token_logprobs_dict.values()\n                    ]\n                    for token_logprobs_dict in raw_output.outputs[0].logprobs\n                ]\n                for raw_output in raw_outputs\n            ]\n            scores = [\n                score_list[\n                    raw_output.outputs[0].token_ids.index(\n                        self.end_of_reasoning_token_id\n                    )\n                    + 2 :\n                ]\n                if self.end_of_reasoning_token_id in raw_output.outputs[0].token_ids\n                else score_list\n                for raw_output, score_list in zip(raw_outputs, scores)\n            ]\n            output = GenerativeModelOutput(sequences=completions, scores=scores)\n        else:\n            output = GenerativeModelOutput(sequences=completions)\n\n        return output\n\n    @classmethod\n    def model_exists(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; bool | NeedsExtraInstalled | NeedsEnvironmentVariable:\n        \"\"\"Check if a model exists.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            Whether the model exists, or an error describing why we cannot check\n            whether the model exists.\n        \"\"\"\n        using_api = (\n            benchmark_config.api_base is not None\n            or benchmark_config.api_version is not None\n        )\n        if using_api:\n            return False\n\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        return (\n            model_info is not None\n            and model_info.pipeline_tag in GENERATIVE_PIPELINE_TAGS\n        )\n\n    @classmethod\n    def get_model_config(docs\n        cls, model_id: str, benchmark_config: BenchmarkConfig\n    ) -&gt; ModelConfig:\n        \"\"\"Fetch the model configuration.\n\n        Args:\n            model_id:\n                The model ID.\n            benchmark_config:\n                The benchmark configuration.\n\n        Returns:\n            The model configuration.\n        \"\"\"\n        model_id, revision = (\n            model_id.split(\"@\") if \"@\" in model_id else (model_id, \"main\")\n        )\n        model_info = get_model_repo_info(\n            model_id=model_id, revision=revision, benchmark_config=benchmark_config\n        )\n        if model_info is None:\n            raise InvalidModel(f\"The model {model_id!r} could not be found.\")\n\n        language_mapping = get_all_languages()\n        language_codes = list(language_mapping.keys())\n\n        model_config = ModelConfig(\n            model_id=model_id,\n            revision=revision,\n            task=model_info.pipeline_tag,\n            languages=[\n                language_mapping[tag]\n                for tag in model_info.tags\n                if tag in language_codes\n            ],\n            merge=any(tag in model_info.tags for tag in MERGE_TAGS),\n            inference_backend=InferenceBackend.VLLM,\n            model_type=ModelType.GENERATIVE,\n            fresh=False,\n            model_cache_dir=create_model_cache_dir(\n                cache_dir=benchmark_config.cache_dir, model_id=model_id\n            ),\n            adapter_base_model_id=model_info.adapter_base_model_id,\n        )\n\n        return model_config\n\n    def _extract_few_shot_examples(\n        self, dataset: DatasetDict, task: Task, itr_idx: int\n    ) -&gt; list[dict[str, t.Any]]:\n        \"\"\"Extract few-shot examples from a dataset.\n\n        This will always extract the examples from the training split.\n\n        We ensure that the few-shot examples are unique by picking them one at a time.\n\n        Args:\n            dataset:\n                The dataset to extract the few-shot examples from.\n            task:\n                The task that is being benchmarked.\n            itr_idx:\n                The index of the dataset in the iterator.\n\n        Returns:\n            The few-shot examples.\n        \"\"\"\n        random_seed = 4242 + itr_idx\n        num_few_shots = self.dataset_config.num_few_shot_examples\n        few_shot_examples: list[dict[str, t.Any]] = list()\n        shuffled_train = dataset[\"train\"].shuffle(seed=random_seed)\n\n        match task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                labels = it.cycle(self.dataset_config.labels)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: x[\"label\"].lower() == label.lower()\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case TaskGroup.TEXT_TO_TEXT:\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"text\"] != example[\"text\"]\n                    )\n\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                labels = it.cycle(\n                    [\n                        label.lower()\n                        for label in self.dataset_config.labels\n                        if label.lower().startswith(\"b-\")\n                    ]\n                )\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    label = next(labels)\n                    possible_examples = shuffled_train.filter(\n                        lambda x: label in [tag.lower() for tag in x[\"labels\"]]\n                    )\n                    if len(possible_examples) == 0:\n                        continue\n                    example = possible_examples.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"tokens\"] != example[\"tokens\"]\n                    )\n\n            case TaskGroup.QUESTION_ANSWERING:\n                # Locate the maximum number of tokens that constitutes a short example\n                for max_num_tokens in [512, 1024, 2048, 4096, 8192]:\n                    train_with_short_examples = dataset[\"train\"].filter(\n                        lambda example: len(example[\"context\"]) &lt; max_num_tokens\n                    )\n                    num_short_examples = len(train_with_short_examples)\n                    if num_short_examples &gt;= self.dataset_config.num_few_shot_examples:\n                        break\n                else:\n                    raise InvalidBenchmark(\n                        \"Could not find enough short examples for few-shot learning.\"\n                    )\n\n                shuffled_train = train_with_short_examples.shuffle(seed=random_seed)\n                while (\n                    len(few_shot_examples) &lt; num_few_shots and len(shuffled_train) &gt; 0\n                ):\n                    example = shuffled_train.select(range(1))[0]\n                    few_shot_examples.append(example)\n                    shuffled_train = shuffled_train.filter(\n                        lambda x: x[\"context\"] != example[\"context\"]\n                    )\n\n            case _:\n                raise NotImplementedError(f\"Unsupported task group: {task.task_group}.\")\n\n        random.seed(random_seed)\n        random.shuffle(few_shot_examples)\n        return few_shot_examples\n\n    def _apply_prompt(\n        self,\n        examples: dict[str, t.Any],\n        few_shot_examples: list[dict[str, t.Any]],\n        task: Task,\n    ) -&gt; dict[str, t.Any]:\n        \"\"\"Apply prompt template to an example, potentially with few-shot examples.\n\n        Args:\n            examples:\n                The examples to apply the few-shot examples to.\n            few_shot_examples:\n                The few-shot examples to apply.\n            task:\n                The task that is being benchmarked.\n\n        Returns:\n            The example with the few-shot examples applied.\n        \"\"\"\n\n        def create_prompt(**kwargs: str) -&gt; tuple[str, str]:\n            \"\"\"Create a prompt from the given keyword arguments.\n\n            Args:\n                kwargs:\n                    The keyword arguments to use in the prompt.\n\n            Returns:\n                A pair (prompt, label), where \"label\" is an empty string if the model is\n                not instruction tuned (as in this case it is included in the prompt).\n            \"\"\"\n            label_key = \"label\" if \"label\" in kwargs else \"target_text\"\n            label = kwargs.pop(label_key)\n            assert label is not None, (\n                f\"Found a None label for the prompt: {kwargs}. This should not happen.\"\n            )\n            label_mapping = self.dataset_config.prompt_label_mapping\n            label = label_mapping.get(label, label)\n            if self.buffer[\"instruction_model\"]:\n                prompt = self.dataset_config.instruction_prompt.format(**kwargs)\n                return prompt, label\n            else:\n                kwargs[label_key] = label\n                return self.dataset_config.prompt_template.format(**kwargs), \"\"\n\n        match task.task_group:\n            case (\n                TaskGroup.SEQUENCE_CLASSIFICATION\n                | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n            ):\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"label\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), label=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case TaskGroup.TEXT_TO_TEXT:\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"text\"].replace(\"\\n\", \" \").strip(),\n                        target_text=example[\"target_text\"].replace(\"\\n\", \" \").strip(),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(text=text.replace(\"\\n\", \" \").strip(), target_text=\"\")\n                    for text in examples[\"text\"]\n                ]\n\n            case TaskGroup.TOKEN_CLASSIFICATION:\n\n                def create_label(example: dict) -&gt; str:\n                    prompt_labels = self.dataset_config.prompt_label_mapping.values()\n                    labels: dict[str, list[str]] = {\n                        prompt_label: list() for prompt_label in prompt_labels\n                    }\n                    for token, label in zip(example[\"tokens\"], example[\"labels\"]):\n                        label = label.lower()\n                        if label == \"o\":\n                            continue\n                        prompt_label = self.dataset_config.prompt_label_mapping[label]\n                        if label.startswith(\"b-\"):\n                            labels[prompt_label].append(token)\n                        elif label.startswith(\"i-\"):\n                            labels[prompt_label][-1] += \" \" + token\n                    return json.dumps(labels, ensure_ascii=False)\n\n                few_shot_sections = [\n                    create_prompt(\n                        text=\" \".join(example[\"tokens\"]).replace(\"\\n\", \" \").strip(),\n                        label=create_label(example=example),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=\" \".join(tokens).replace(\"\\n\", \" \").strip(), label=\"\"\n                    )\n                    for tokens in examples[\"tokens\"]\n                ]\n\n            case TaskGroup.QUESTION_ANSWERING:\n                few_shot_sections = [\n                    create_prompt(\n                        text=example[\"context\"].replace(\"\\n\", \" \").strip(),\n                        question=example[\"question\"].replace(\"\\n\", \" \").strip(),\n                        label=example[\"answers\"][\"text\"][0].replace(\"\\n\", \" \"),\n                    )\n                    for example in few_shot_examples\n                ]\n                new_sections = [\n                    create_prompt(\n                        text=context.replace(\"\\n\", \" \").strip(),\n                        question=question.replace(\"\\n\", \" \").strip(),\n                        label=\"\",\n                    )\n                    for context, question in zip(\n                        examples[\"context\"], examples[\"question\"]\n                    )\n                ]\n\n            case _:\n                raise NotImplementedError(f\"Unsupported task group: {task.task_group}.\")\n\n        if self.buffer[\"instruction_model\"]:\n            few_shot_messages = [\n                dict(role=role, content=content)\n                for prompt, label in few_shot_sections\n                for role, content in [(\"user\", prompt), (\"assistant\", label)]\n            ]\n\n            messages_list = [\n                few_shot_messages + [dict(role=\"user\", content=prompt)]\n                for prompt, _ in new_sections\n            ]\n\n            # Pick the chat template that matches the language of the dataset, if such a\n            # template exists\n            chat_template: str | None = None\n            if isinstance(self._tokenizer.chat_template, dict):\n                language_codes = [\n                    language.code for language in self.dataset_config.languages\n                ]\n                for name, candidate_template in self._tokenizer.chat_template.items():\n                    if name.lower() in language_codes:\n                        chat_template = candidate_template\n                        log_once(\n                            f\"Using the {name!r} chat template for the tokenizer.\",\n                            level=logging.DEBUG,\n                        )\n                        break\n\n            texts = [\n                self._tokenizer.apply_chat_template(\n                    conversation=messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                    chat_template=chat_template,\n                )\n                for messages in messages_list\n            ]\n\n            examples[\"text\"] = texts\n\n        else:\n            prompt_prefix = \"\"\n            if self.dataset_config.prompt_prefix:\n                prompt_prefix = self.dataset_config.prompt_prefix + \"\\n\\n\"\n\n            few_shot_prompt = \"\\n\\n\".join([prompt for prompt, _ in few_shot_sections])\n            if few_shot_prompt:\n                few_shot_prompt += \"\\n\\n\"\n\n            examples[\"text\"] = [\n                prompt_prefix + few_shot_prompt + new_prompt\n                for new_prompt, _ in new_sections\n            ]\n\n        return examples\n\n    @propertydocs\n    def data_collator(self) -&gt; c.Callable[[list[t.Any]], dict[str, t.Any]]:\n        \"\"\"The data collator used to prepare samples during finetuning.\n\n        Returns:\n            The data collator.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `data_collator` property has not been implemented for vLLM models.\"\n        )\n\n    @property\n    def trainer_class(self) -&gt; t.Type[\"Trainer\"]:docs\n        \"\"\"The Trainer class to use for finetuning.\n\n        Returns:\n            The Trainer class.\n        \"\"\"\n        raise NotImplementedError(\n            \"The `trainer_class` property has not been implemented for vLLM models.\"\n        )\n\n\ndef load_model_and_tokenizer(docs\n    model_config: ModelConfig, benchmark_config: BenchmarkConfig, output_scores: bool\n) -&gt; \"tuple[LLM, PreTrainedTokenizer]\":\n    \"\"\"Load the model and tokenizer.\n\n    Args:\n        model_config:\n            The model configuration.\n        benchmark_config:\n            The benchmark configuration.\n        output_scores:\n            Whether to output scores.\n\n    Returns:\n        The loaded model and tokenizer.\n    \"\"\"\n    # Prefer base model ID if the model is an adapter - the adapter will be added on\n    # during inference in this case\n    model_id = model_config.adapter_base_model_id or model_config.model_id\n\n    hf_model_config = load_hf_model_config(\n        model_id=model_id,\n        num_labels=0,\n        id2label=dict(),\n        label2id=dict(),\n        revision=model_config.revision,\n        model_cache_dir=model_config.model_cache_dir,\n        api_key=benchmark_config.api_key,\n        trust_remote_code=benchmark_config.trust_remote_code,\n        run_with_cli=benchmark_config.run_with_cli,\n    )\n\n    quantization = None\n    if hasattr(hf_model_config, \"quantization_config\"):\n        quantization = hf_model_config.quantization_config.get(\"quant_method\")\n\n    # The quantised models require extra dependencies\n    if quantization == \"gptq\" and (\n        importlib.util.find_spec(\"auto_gptq\") is None\n        or importlib.util.find_spec(\"optimum\") is None\n    ):\n        raise NeedsExtraInstalled(extra=\"quantization\")\n    if quantization == \"awq\" and importlib.util.find_spec(\"awq\") is None:\n        raise NeedsExtraInstalled(extra=\"quantization\")\n\n    dtype: str | torch.dtype = \"auto\"\n    if quantization is not None and hf_model_config.torch_dtype != torch.float16:\n        logger.info(\n            \"You are loading a quantized model with dtype \"\n            f\"{hf_model_config.torch_dtype}, which vLLM does not support. Setting \"\n            \"dtype to float16 instead.\"\n        )\n        dtype = torch.float16\n\n    if model_config.adapter_base_model_id is not None:\n        download_dir = str(Path(model_config.model_cache_dir) / \"base_model\")\n    else:\n        download_dir = str(model_config.model_cache_dir)\n\n    potential_max_model_length_config_names = [\n        \"max_position_embeddings\",\n        \"max_sequence_length\",\n        \"model_max_length\",\n        \"sliding_window\",\n        \"sliding_window_size\",\n        \"n_positions\",\n    ]\n    true_max_model_len_candidates: list[int] = list()\n    for config_name in potential_max_model_length_config_names:\n        if hasattr(hf_model_config, config_name):\n            model_len = getattr(hf_model_config, config_name)\n            if model_len is not None:\n                true_max_model_len_candidates.append(model_len)\n\n    if len(true_max_model_len_candidates) &gt; 0:\n        true_max_model_len = min(true_max_model_len_candidates)\n    else:\n        true_max_model_len = 5_000\n\n    clear_vllm()\n\n    executor_backend = \"ray\" if torch.cuda.device_count() &gt; 1 else \"mp\"\n\n    try:\n        model = LLM(\n            model=model_id,\n            tokenizer=model_id,\n            gpu_memory_utilization=0.95,\n            max_model_len=min(true_max_model_len, 5_000),\n            download_dir=download_dir,\n            trust_remote_code=benchmark_config.trust_remote_code,\n            revision=model_config.revision,\n            seed=4242,\n            distributed_executor_backend=executor_backend,\n            tensor_parallel_size=torch.cuda.device_count(),\n            disable_custom_all_reduce=True,\n            quantization=quantization,\n            dtype=dtype,\n            enforce_eager=True,\n            max_logprobs=MAX_LOGPROBS if output_scores else None,\n            # TEMP: Prefix caching isn't supported with sliding window in vLLM yet,\n            # so we disable it for now\n            enable_prefix_caching=False,\n            enable_lora=model_config.adapter_base_model_id is not None,\n            max_lora_rank=256,\n        )\n    except (ValueError, OSError) as e:\n        if \"awaiting a review from the repo authors\" in str(e):\n            raise InvalidModel(\n                f\"The model {model_id!r} is awaiting a review from the repository \"\n                \"authors. Please try again later.\"\n            )\n        elif \"trust_remote_code\" in str(e):\n            raise InvalidModel(\n                f\"Loading the model {model_id!r} needs to trust remote code. \"\n                \"If you trust the suppliers of this model, then you can enable \"\n                \"this by setting the `--trust-remote-code` flag.\"\n            )\n        raise InvalidModel(\n            f\"The model {model_id!r} could not be loaded. The error was {e!r}.\"\n        )\n\n    model._run_engine = MethodType(_run_engine_with_fixed_progress_bars, model)\n    model.config = hf_model_config\n\n    tokenizer = load_tokenizer(\n        model_id=model_config.model_id,\n        revision=model_config.revision,\n        adapter_base_model_id=model_config.adapter_base_model_id,\n        trust_remote_code=benchmark_config.trust_remote_code,\n        model_max_length=true_max_model_len,\n        model_cache_dir=model_config.model_cache_dir,\n        token=benchmark_config.api_key or os.getenv(\"HUGGINGFACE_API_KEY\") or True,\n    )\n\n    return model, tokenizer\n\n\ndef load_tokenizer(docs\n    model_id: str,\n    revision: str,\n    adapter_base_model_id: str | None,\n    trust_remote_code: bool,\n    model_max_length: int,\n    model_cache_dir: str,\n    token: str | bool,\n) -&gt; \"PreTrainedTokenizer\":\n    \"\"\"Load the tokenizer.\n\n    Args:\n        model_id:\n            The model identifier.\n        revision:\n            The revision of the model.\n        adapter_base_model_id:\n            The base model ID for the adapter model. Can be None if the model is not an\n            adapter model.\n        trust_remote_code:\n            Whether to trust remote code.\n        model_max_length:\n            The maximum length of the model.\n        model_cache_dir:\n            The cache directory for the model.\n        token:\n            The Hugging Face API token.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    config = AutoConfig.from_pretrained(\n        adapter_base_model_id or model_id,\n        revision=revision,\n        cache_dir=model_cache_dir,\n        token=token,\n        trust_remote_code=trust_remote_code,\n    )\n    num_retries = 5\n    for _ in range(num_retries):\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_id,\n                use_fast=True,\n                verbose=False,\n                trust_remote_code=trust_remote_code,\n                padding_side=\"left\",\n                truncation_side=\"left\",\n                model_max_length=model_max_length,\n                config=config,\n                token=token,\n            )\n            break\n        except (json.JSONDecodeError, OSError, TypeError) as e:\n            if adapter_base_model_id is None or model_id == adapter_base_model_id:\n                raise InvalidModel(\n                    f\"Could not load tokenizer for model {model_id!r}. The error was \"\n                    f\"{str(e)}.\"\n                )\n            logger.debug(\n                f\"Could not load tokenizer for {model_id!r}. Falling back to \"\n                f\"{adapter_base_model_id!r}.\"\n            )\n            model_id = adapter_base_model_id\n        except (TimeoutError, RequestError):\n            logger.info(f\"Couldn't load tokenizer for {model_id!r}. Retrying.\")\n            sleep(5)\n            continue\n    else:\n        raise InvalidModel(\n            f\"Could not load tokenizer for model {model_id!r} after {num_retries} \"\n            \"attempts.\"\n        )\n\n    # Ensure that BOS, EOS and PAD tokens are set\n    tokenizer.bos_token, tokenizer.bos_token_id = get_bos_token(tokenizer=tokenizer)\n    tokenizer.eos_token, tokenizer.eos_token_id = get_eos_token(tokenizer=tokenizer)\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    return tokenizer\n\n\ndef _run_engine_with_fixed_progress_bars(\n    self: \"LLM\", use_tqdm: bool\n) -&gt; list[\"RequestOutput\"]:\n    if use_tqdm:\n        num_requests = self.llm_engine.get_num_unfinished_requests()\n        pbar = tqdm(\n            total=num_requests, leave=False, disable=hasattr(sys, \"_called_from_test\")\n        )\n    else:\n        pbar = None\n\n    # Run the engine.\n    outputs: list[\"RequestOutput\"] = list()\n    while self.llm_engine.has_unfinished_requests():\n        step_outputs = self.llm_engine.step()\n        for output in step_outputs:\n            if output.finished:\n                outputs.append(output)\n                if pbar is not None:\n                    pbar.update(1)\n\n    if pbar is not None:\n        pbar.close()\n\n    # Sort the outputs by request ID. This is necessary because some requests may be\n    # finished earlier than its previous requests.\n    outputs = sorted(outputs, key=lambda x: int(x.request_id))\n\n    return outputs\n\n\ndef clear_vllm() -&gt; None:docs\n    \"\"\"Clear the GPU memory used by the vLLM model, enabling re-initialisation.\"\"\"\n    try:\n        destroy_model_parallel()\n    except ImportError:\n        pass\n    clear_memory()\n    if ray.is_initialized():\n        ray.shutdown()\n\n\ndef get_end_of_reasoning_token_id(docs\n    model: \"LLM\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; int | None:\n    \"\"\"Get the end of reasoning token ID for a generative model.\n\n    This assumes that the reasoning token is of the form &lt;X&gt; and that the end of\n    reasoning token is &lt;/X&gt; (for X being any string without spaces).\n\n    Args:\n        model:\n            The vLLM model.\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The end of reasoning token ID, or None if it could not be found.\n    \"\"\"\n    if tokenizer.chat_template is None:\n        prompt = \"What is your name?\"\n    else:\n        prompt = tokenizer.apply_chat_template(\n            conversation=[dict(role=\"user\", content=\"What is your name?\")],\n            add_generation_prompt=True,\n            tokenize=False,\n        )\n\n    # Generate a completion and remove the BOS token from it, to not confuse it with the\n    # potential reasoning token\n    completion = (\n        model.generate(\n            prompts=[prompt],\n            sampling_params=SamplingParams(max_tokens=3, temperature=0.0),\n            use_tqdm=False,\n        )[0]\n        .outputs[0]\n        .text\n    )\n    if tokenizer.bos_token is not None:\n        completion = completion.replace(tokenizer.bos_token, \"\").strip()\n\n    # If it doesn't contain a reasoning token, we can't find the end of reasoning token\n    match = re.search(pattern=r\"&lt;\\w+&gt;\", string=completion)\n    if match is None:\n        log_once(\n            message=(\n                \"Could not find a reasoning token, so assuming the model is not a \"\n                \"reasoning model.\"\n            ),\n            level=logging.DEBUG,\n        )\n        return None\n\n    # Check that the found reasoning token and its associated end-of-reasoning tokens\n    # are both special tokens\n    reasoning_token = match.group()\n    end_of_reasoning_token = f\"&lt;/{reasoning_token[1:-1]}&gt;\"\n    special_tokens = [\n        decoder_token.content\n        for decoder_token in tokenizer.added_tokens_decoder.values()\n    ]\n    special_tokens.extend(\n        [encoder_token for encoder_token in tokenizer.added_tokens_encoder.keys()]\n    )\n    special_tokens.extend(tokenizer.all_special_tokens)\n    if (\n        reasoning_token not in special_tokens\n        or end_of_reasoning_token not in special_tokens\n    ):\n        log_once(\n            message=(\n                f\"Detected reasoning token {reasoning_token!r} and end of reasoning \"\n                f\"token {end_of_reasoning_token!r}, but one of them is not registered \"\n                \"as a special token, so assuming it is not a real reasoning token.\"\n            ),\n            level=logging.DEBUG,\n        )\n        return None\n\n    log_once(\n        message=f\"Detected reasoning token {reasoning_token!r}.\", level=logging.DEBUG\n    )\n\n    # Encode the end of reasoning token and return its ID\n    end_of_reasoning_token_id = tokenizer.encode(\n        text=end_of_reasoning_token, add_special_tokens=False\n    )[0]\n\n    return end_of_reasoning_token_id\n</code></pre>"},{"location":"api/euroeval/task_utils/","title":"euroeval.task_utils","text":"euroeval.task_utils<p> source package euroeval.task_utils </p> <p>Utility functions related to the different tasks and task groups.</p> <p> Modules </p> <ul> <li> <p>euroeval.task_utils.multiple_choice_classification \u2014 Utility functions related to the multiple-choice classification task group.</p> </li> <li> <p>euroeval.task_utils.question_answering \u2014 Utility functions related to the question-answering task group.</p> </li> <li> <p>euroeval.task_utils.sequence_classification \u2014 Utility functions related to the sequence-classification task group.</p> </li> <li> <p>euroeval.task_utils.text_to_text \u2014 Utility functions related to the text-to-text task group.</p> </li> <li> <p>euroeval.task_utils.token_classification \u2014 Utility functions related to the token-classification task group.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/","title":"euroeval.task_utils","text":"euroeval.task_utils<p> docs package euroeval.task_utils </p> <pre><code>\"\"\"Utility functions related to the different tasks and task groups.\"\"\"\n</code></pre>"},{"location":"api/euroeval/task_utils/multiple_choice_classification/","title":"euroeval.task_utils.multiple_choice_classification","text":"euroeval.task_utils.multiple_choice_classification<p> source module euroeval.task_utils.multiple_choice_classification </p> <p>Utility functions related to the multiple-choice classification task group.</p> <p> Classes </p> <ul> <li> <p>MultipleChoiceClassificationTrainer \u2014 Trainer subclass for question answering tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>prepare_examples \u2014 Prepare the features.</p> </li> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels.</p> </li> </ul> <p> source class MultipleChoiceClassificationTrainer() </p> <p><p>Bases : Trainer</p></p> <p>Trainer subclass for question answering tasks.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method MultipleChoiceClassificationTrainer.evaluate(eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] | None </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014</p> <p>The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014</p> <p>The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014</p> <p>The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] | None \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source prepare_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare the features.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>The examples to prepare.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: np.ndarray, dataset: Dataset) \u2192 tuple[Predictions, Labels] </p> <p>Postprocess the predictions and labels.</p> <p> Parameters </p> <ul> <li> <p>predictions :  np.ndarray \u2014</p> <p>The model predictions, of shape (num_examples, 2).</p> </li> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset containing the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Predictions, Labels] \u2014 The postprocessed predictions and labels.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/multiple_choice_classification/","title":"euroeval.task_utils.multiple_choice_classification","text":"euroeval.task_utils.multiple_choice_classification<p> docs module euroeval.task_utils.multiple_choice_classification </p> <pre><code>\"\"\"Utility functions related to the multiple-choice classification task group.\"\"\"\n\nimport hashlib\nimport logging\nimport re\nimport typing as t\nfrom collections import defaultdict\n\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import BatchEncoding, PreTrainedTokenizer, Trainer\n\nif t.TYPE_CHECKING:\n    from ..types import Labels, Predictions\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass MultipleChoiceClassificationTrainer(Trainer):docs\n    \"\"\"Trainer subclass for question answering tasks.\"\"\"\n\n    def evaluate(docs\n        self,\n        eval_dataset: \"Dataset | None\" = None,\n        ignore_keys: list[str] | None = None,\n        metric_key_prefix: str = \"eval\",\n    ) -&gt; dict[str, float] | None:\n        \"\"\"Evaluate the model on the given dataset.\n\n        Args:\n            eval_dataset:\n                The dataset to evaluate on. If None, then use the stored evaluation\n                dataset.\n            ignore_keys:\n                The keys to ignore when computing the metrics.\n            metric_key_prefix:\n                The prefix to use for the metric keys.\n\n        Returns:\n            The metrics computed on the evaluation dataset.\n        \"\"\"\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n\n        eval_loop = (\n            self.prediction_loop\n            if self.args.use_legacy_prediction_loop\n            else self.evaluation_loop\n        )\n        output = eval_loop(\n            eval_dataloader,\n            description=\"Evaluation\",\n            prediction_loss_only=None,\n            ignore_keys=ignore_keys,\n            metric_key_prefix=metric_key_prefix,\n        )\n\n        if metric_key_prefix == \"test\":\n            preds_and_labels = postprocess_predictions_and_labels(\n                predictions=output.predictions, dataset=eval_dataset\n            )\n            output.metrics.update(self.compute_metrics(preds_and_labels))\n\n            # Prefix all keys with metric_key_prefix + '_'\n            for key in list(output.metrics.keys()):\n                if not key.startswith(f\"{metric_key_prefix}_\"):\n                    output.metrics[f\"{metric_key_prefix}_{key}\"] = output.metrics.pop(\n                        key\n                    )\n\n        # Only the main node log the results by default\n        if self.args.should_log:\n            self.log(output.metrics)\n\n        self.control = self.callback_handler.on_evaluate(\n            self.args,\n            self.state,\n            self.control,  # type: ignore[has-type]\n            output.metrics,\n        )\n        return output.metrics\n\n\ndef prepare_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare the features.\n\n    Args:\n        examples:\n            The examples to prepare.\n        tokenizer:\n            The tokenizer to use to prepare the examples.\n\n    Returns:\n        The prepared examples.\n    \"\"\"\n    doc: str = examples[\"text\"][0]\n    sections = doc.split(\"\\n\")\n\n    choice_idxs = [\n        idx\n        for idx, section in enumerate(sections)\n        if re.match(pattern=r\"^[a-e]\\. \", string=section) is not None\n    ]\n    choices = [sections[idx] for idx in choice_idxs]\n\n    # Check that the choices are present, and that all of them are at the end\n    assert len(choices) &gt; 0, \"No choices found in the document.\"\n    assert all(\n        choice_idx == len(sections) - i\n        for i, choice_idx in enumerate(sorted(choice_idxs, reverse=True), start=1)\n    ), \"Choices are not at the end of the document.\"\n\n    question_idx = min(choice_idxs) - 2  # -2 to remove the 'Choices:' line\n    context_and_question = \"\\n\".join(sections[: question_idx + 1]).strip()\n\n    new_examples = tokenizer(\n        text=[context_and_question] * len(choices),\n        text_pair=[choice[3:] for choice in choices],\n        padding=True,\n        truncation=True,\n    )\n    new_examples[\"label\"] = [\n        int(choice.startswith(f\"{letter}. \") and letter == examples[\"label\"][0])\n        for letter, choice in zip(\"abcde\", choices)\n    ]\n    new_examples[\"id\"] = [hashlib.md5(string=doc.encode()).hexdigest()] * len(choices)\n    return new_examples\n\n\ndef postprocess_predictions_and_labels(docs\n    predictions: np.ndarray, dataset: \"Dataset\"\n) -&gt; tuple[\"Predictions\", \"Labels\"]:\n    \"\"\"Postprocess the predictions and labels.\n\n    Args:\n        predictions:\n            The model predictions, of shape (num_examples, 2).\n        dataset:\n            The dataset containing the examples.\n\n    Returns:\n        The postprocessed predictions and labels.\n    \"\"\"\n    mapping = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}\n\n    all_predictions: list[str] = list()\n    all_labels: list[str] = list()\n\n    pred_label_dict = defaultdict(list)\n    for pred_arr, example in zip(predictions, dataset):\n        pred_label_dict[example[\"id\"]].append((pred_arr[1], example[\"label\"]))\n\n    # Compute the final predictions and labels\n    for id_ in set(dataset[\"id\"]):\n        preds, labels = zip(*pred_label_dict[id_])\n\n        # Some IDs appear multiple times in the dataset, since we are bootstrapping.\n        # Here we separate them into their respective groups.\n        assert len(labels) % sum(labels) == 0, (\n            \"The number of labels is not divisible by the sum of the labels.\"\n        )\n        group_size = len(labels) // sum(labels)\n        preds_groups = [\n            preds[i : i + group_size] for i in range(0, len(preds), group_size)\n        ]\n        labels_groups = [\n            labels[i : i + group_size] for i in range(0, len(labels), group_size)\n        ]\n        for preds_group, labels_group in zip(preds_groups, labels_groups):\n            prediction: str = mapping[np.argmax(preds_group).item()]\n            label: str = mapping[np.argmax(labels_group).item()]\n            all_predictions.append(prediction)\n            all_labels.append(label)\n\n    return all_predictions, all_labels\n</code></pre>"},{"location":"api/euroeval/task_utils/question_answering/","title":"euroeval.task_utils.question_answering","text":"euroeval.task_utils.question_answering<p> source module euroeval.task_utils.question_answering </p> <p>Utility functions related to the question-answering task group.</p> <p> Classes </p> <ul> <li> <p>QuestionAnsweringTrainer \u2014 Trainer subclass for question answering tasks.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>prepare_train_examples \u2014 Prepare the features for training.</p> </li> <li> <p>prepare_test_examples \u2014 Prepare test examples.</p> </li> <li> <p>postprocess_predictions_and_labels \u2014 Postprocess the predictions and labels, to allow easier metric computation.</p> </li> <li> <p>find_best_answer \u2014 Find the best answer for a given example.</p> </li> <li> <p>find_valid_answers \u2014 Find the valid answers from the start and end indexes.</p> </li> </ul> <p> source class QuestionAnsweringTrainer(model: PreTrainedModel | nn.Module, processing_class: PreTrainedTokenizerBase, args: TrainingArguments, train_dataset: Dataset, eval_dataset: Dataset, compute_metrics: c.Callable[[EvalPrediction], dict[str, float]], callbacks: list[TrainerCallback], data_collator: c.Callable) </p> <p><p>Bases : Trainer</p></p> <p>Trainer subclass for question answering tasks.</p> <p>Initialize the trainer.</p> <p> Methods </p> <ul> <li> <p>evaluate \u2014 Evaluate the model on the given dataset.</p> </li> </ul> <p> source method QuestionAnsweringTrainer.evaluate(eval_dataset: Dataset | None = None, orig_eval_dataset: Dataset | None = None, ignore_keys: list[str] | None = None, metric_key_prefix: str = 'eval') \u2192 dict[str, float] | None </p> <p>Evaluate the model on the given dataset.</p> <p> Parameters </p> <ul> <li> <p>eval_dataset :  Dataset | None \u2014</p> <p>The dataset to evaluate on. If None, then use the stored evaluation dataset.</p> </li> <li> <p>orig_eval_dataset :  Dataset | None \u2014</p> <p>The original evaluation dataset, before any postprocessing. If None, then use the stored original evaluation dataset.</p> </li> <li> <p>ignore_keys :  list[str] | None \u2014</p> <p>The keys to ignore when computing the metrics.</p> </li> <li> <p>metric_key_prefix :  str \u2014</p> <p>The prefix to use for the metric keys.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] | None \u2014 The metrics computed on the evaluation dataset.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> source prepare_train_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare the features for training.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>The examples to prepare.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer to use to prepare the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared examples.</p> </li> </ul> <p> source prepare_test_examples(examples: BatchEncoding, tokenizer: PreTrainedTokenizer) \u2192 BatchEncoding </p> <p>Prepare test examples.</p> <p> Parameters </p> <ul> <li> <p>examples :  BatchEncoding \u2014</p> <p>Dictionary of test examples.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to preprocess the examples.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 The prepared test examples.</p> </li> </ul> <p> source postprocess_predictions_and_labels(predictions: list, dataset: Dataset, prepared_dataset: Dataset, cls_token_index: int) \u2192 tuple[list[dict], list[dict]] </p> <p>Postprocess the predictions and labels, to allow easier metric computation.</p> <p> Parameters </p> <ul> <li> <p>predictions :  list \u2014</p> <p>A pair of (start_logits, end_logits) predictions.</p> </li> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset containing the examples.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[dict], list[dict]] \u2014 The postprocessed predictions and labels.</p> </li> </ul> <p> source find_best_answer(all_start_logits: np.ndarray, all_end_logits: np.ndarray, prepared_dataset: Dataset, feature_indices: list[int], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float, cls_token_index: int) \u2192 str </p> <p>Find the best answer for a given example.</p> <p> Parameters </p> <ul> <li> <p>all_start_logits :  np.ndarray \u2014</p> <p>The start logits for all the features.</p> </li> <li> <p>all_end_logits :  np.ndarray \u2014</p> <p>The end logits for all the features.</p> </li> <li> <p>prepared_dataset :  Dataset \u2014</p> <p>The dataset containing the prepared examples.</p> </li> <li> <p>feature_indices :  list[int] \u2014</p> <p>The indices of the features associated with the current example.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> <li> <p>cls_token_index :  int \u2014</p> <p>The index of the CLS token.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The best answer for the example.</p> </li> </ul> <p> source find_valid_answers(start_logits: np.ndarray, end_logits: np.ndarray, offset_mapping: list[tuple[int, int]], context: str, max_answer_length: int, num_best_logits: int, min_null_score: float) \u2192 list[dict] </p> <p>Find the valid answers from the start and end indexes.</p> <p> Parameters </p> <ul> <li> <p>start_logits :  np.ndarray \u2014</p> <p>The logits for the start of the answer.</p> </li> <li> <p>end_logits :  np.ndarray \u2014</p> <p>The logits for the end of the answer.</p> </li> <li> <p>offset_mapping :  list[tuple[int, int]] \u2014</p> <p>The offset mapping, being a list of pairs of integers for each token index, containing the start and end character index in the original context.</p> </li> <li> <p>context :  str \u2014</p> <p>The context of the example.</p> </li> <li> <p>max_answer_length :  int \u2014</p> <p>The maximum length of the answer.</p> </li> <li> <p>num_best_logits :  int \u2014</p> <p>The number of best logits to consider. Note that this function will run in O(<code>num_best_logits</code> ^ 2) time.</p> </li> <li> <p>min_null_score :  float \u2014</p> <p>The minimum score an answer can have.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict] \u2014 A list of the valid answers, each being a dictionary with keys \"text\" and \"score\", the score being the sum of the start and end logits.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/question_answering/","title":"euroeval.task_utils.question_answering","text":"euroeval.task_utils.question_answering<p> docs module euroeval.task_utils.question_answering </p> <pre><code>\"\"\"Utility functions related to the question-answering task group.\"\"\"\n\nimport collections.abc as c\nimport logging\nimport typing as t\nfrom collections import defaultdict\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers import PreTrainedTokenizer\nfrom transformers.trainer import Trainer\n\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..utils import (\n    get_special_token_metadata,\n    raise_if_model_output_contains_nan_values,\n)\n\nif t.TYPE_CHECKING:\n    import torch.nn as nn\n    from datasets.arrow_dataset import Dataset\n    from transformers import (\n        BaseImageProcessor,\n        EvalPrediction,\n        FeatureExtractionMixin,\n        PreTrainedModel,\n        PreTrainedTokenizerBase,\n        ProcessorMixin,\n        TrainerCallback,\n        TrainingArguments,\n    )\n    from transformers.tokenization_utils_base import BatchEncoding\n\n    from ..types import Labels, Predictions\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass QuestionAnsweringTrainer(Trainer):docs\n    \"\"\"Trainer subclass for question answering tasks.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel | nn.Module\",\n        processing_class: \"PreTrainedTokenizerBase\",\n        args: \"TrainingArguments\",\n        train_dataset: \"Dataset\",\n        eval_dataset: \"Dataset\",\n        compute_metrics: \"c.Callable[[EvalPrediction], dict[str, float]]\",\n        callbacks: \"list[TrainerCallback]\",\n        data_collator: \"c.Callable\",\n    ) -&gt; None:\n        \"\"\"Initialize the trainer.\"\"\"\n        super().__init__(\n            model=model,\n            processing_class=processing_class,\n            args=args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            compute_metrics=compute_metrics,\n            callbacks=callbacks,\n            data_collator=data_collator,\n        )\n\n        # Get the CLS token id for the tokenizer\n        if self.tokenizer is not None:\n            assert isinstance(self.tokenizer, PreTrainedTokenizer)\n            special_token_metadata = get_special_token_metadata(self.tokenizer)\n            self.cls_token_id = special_token_metadata[\"cls_token_id\"]\n\n        # Set the label names\n        self.label_names = [\"start_positions\", \"end_positions\"]\n\n    def evaluate(docs\n        self,\n        eval_dataset: \"Dataset | None\" = None,\n        orig_eval_dataset: \"Dataset | None\" = None,\n        ignore_keys: list[str] | None = None,\n        metric_key_prefix: str = \"eval\",\n    ) -&gt; dict[str, float] | None:\n        \"\"\"Evaluate the model on the given dataset.\n\n        Args:\n            eval_dataset:\n                The dataset to evaluate on. If None, then use the stored evaluation\n                dataset.\n            orig_eval_dataset:\n                The original evaluation dataset, before any postprocessing. If None,\n                then use the stored original evaluation dataset.\n            ignore_keys:\n                The keys to ignore when computing the metrics.\n            metric_key_prefix:\n                The prefix to use for the metric keys.\n\n        Returns:\n            The metrics computed on the evaluation dataset.\n        \"\"\"\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n\n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics  # type: ignore[has-type]\n        self.compute_metrics = None\n        eval_loop = (\n            self.prediction_loop\n            if self.args.use_legacy_prediction_loop\n            else self.evaluation_loop\n        )\n        try:\n            output = eval_loop(\n                eval_dataloader,\n                description=\"Evaluation\",\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n                metric_key_prefix=metric_key_prefix,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n\n        if orig_eval_dataset is not None:\n            preds_and_labels = postprocess_predictions_and_labels(\n                predictions=output.predictions,\n                dataset=orig_eval_dataset,\n                prepared_dataset=eval_dataset,\n                cls_token_index=self.cls_token_id,\n            )\n            output.metrics.update(self.compute_metrics(preds_and_labels))\n\n            # Prefix all keys with metric_key_prefix + '_'\n            for key in list(output.metrics.keys()):\n                if not key.startswith(f\"{metric_key_prefix}_\"):\n                    output.metrics[f\"{metric_key_prefix}_{key}\"] = output.metrics.pop(\n                        key\n                    )\n\n        # Only the main node log the results by default\n        if self.args.should_log:\n            self.log(output.metrics)\n\n        self.control = self.callback_handler.on_evaluate(\n            self.args,\n            self.state,\n            self.control,  # type: ignore[has-type]\n            output.metrics,\n        )\n        return output.metrics\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    if model_output_dtype in [np.float16, np.float32, np.float64]:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n        score_dict: dict[str, float] | None = metric.compute(\n            predictions=predictions, references=labels, **cfg.compute_kwargs\n        )\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    raw_predictions = model_output.sequences\n    predictions = [\n        dict(id=id, prediction_text=predicted_answer.lower(), no_answer_probability=0.0)\n        for id, predicted_answer in zip(input_batch[\"id\"], raw_predictions)\n    ]\n    return predictions\n\n\ndef prepare_train_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare the features for training.\n\n    Args:\n        examples:\n            The examples to prepare.\n        tokenizer:\n            The tokenizer to use to prepare the examples.\n\n    Returns:\n        The prepared examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token_id = special_token_metadata[\"cls_token_id\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a\n    # stride. This results in one example possible giving several features when a\n    # context is long, each of those features having a context that overlaps a bit the\n    # context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # The offset mappings will give us a map from token to character position in the\n    # original context. This will help us compute the start_positions and\n    # end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Initialise the start- and end positions of the answers\n    tokenized_examples[\"start_positions\"] = list()\n    tokenized_examples[\"end_positions\"] = list()\n\n    for i, offsets in enumerate(offset_mapping):\n        # Get the input IDs for the current example\n        input_ids = tokenized_examples.input_ids[i]\n\n        # We will label impossible answers with the index of the CLS token\n        cls_index = input_ids.index(cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # Manually ensure that the special tokens are set to None in `sequence_ids`\n        for special_token in tokenizer.special_tokens_map.keys():\n            if hasattr(tokenizer, f\"{special_token}_id\"):\n                special_token_id = getattr(tokenizer, f\"{special_token}_id\")\n                if special_token_id is not None:\n                    sequence_ids = [\n                        None if token_id == special_token_id else seq_id\n                        for token_id, seq_id in zip(input_ids, sequence_ids)\n                    ]\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples.start_positions.append(cls_index)\n            tokenized_examples.end_positions.append(cls_index)\n\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is\n            # labeled with the CLS index).\n            if not (\n                offsets[token_start_index][0] &lt;= start_char\n                and offsets[token_end_index][1] &gt;= end_char\n            ):\n                tokenized_examples.start_positions.append(cls_index)\n                tokenized_examples.end_positions.append(cls_index)\n\n            # Otherwise move the token_start_index and token_end_index to the two ends\n            # of the answer. Note: we could go after the last offset if the answer is\n            # the last word (edge case).\n            else:\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_start_index][0] &lt;= start_char\n                ):\n                    token_start_index += 1\n                token_start_index -= 1\n                tokenized_examples.start_positions.append(token_start_index)\n                while (\n                    token_start_index &lt;= token_end_index\n                    and offsets[token_end_index][1] &gt;= end_char\n                ):\n                    token_end_index -= 1\n                token_end_index += 1\n                tokenized_examples.end_positions.append(token_end_index)\n                assert token_end_index &gt;= token_start_index\n\n    return tokenized_examples\n\n\ndef prepare_test_examples(docs\n    examples: \"BatchEncoding\", tokenizer: \"PreTrainedTokenizer\"\n) -&gt; \"BatchEncoding\":\n    \"\"\"Prepare test examples.\n\n    Args:\n        examples:\n            Dictionary of test examples.\n        tokenizer:\n            The tokenizer used to preprocess the examples.\n\n    Returns:\n        The prepared test examples.\n    \"\"\"\n    # Some of the questions have lots of whitespace on the left, which is not useful\n    # and will make the truncation of the context fail (the tokenized question will\n    # take a lots of space). So we remove that left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Extract special token metadata from the tokenizer\n    special_token_metadata = get_special_token_metadata(tokenizer=tokenizer)\n    has_cls_token = special_token_metadata[\"has_cls_token\"]\n    has_sep_token = special_token_metadata[\"has_sep_token\"]\n    cls_token = special_token_metadata[\"cls_token\"]\n    sep_token = special_token_metadata[\"sep_token\"]\n\n    # If the tokenizer is not adding special tokens, then we add them manually\n    if not has_cls_token and not has_sep_token:\n        examples[\"question\"] = [\n            f\"{cls_token}{q}{sep_token}\" for q in examples[\"question\"]\n        ]\n        examples[\"context\"] = [f\"{c}{sep_token}\" for c in examples[\"context\"]]\n\n    # Set the stride used during tokenization, when the context is long enough to be\n    # split into several features. Since we are always keeping the question tokens, we\n    # need to make sure that the stride does not exceed the resulting maximum context\n    # length.\n    max_question_tokens = max(len(tokenizer(q).input_ids) for q in examples[\"question\"])\n    num_special_tokens = int(has_cls_token) + int(has_sep_token)\n    stride = tokenizer.model_max_length // 4\n    max_length = tokenizer.model_max_length - stride\n    stride = min(stride, max_length - max_question_tokens - num_special_tokens)\n    max_length = tokenizer.model_max_length - stride\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows\n    # using a stride. This results in one example possible giving several features when\n    # a context is long, each of those features having a context that overlaps a bit\n    # the context of the previous feature.\n    tokenized_examples = tokenizer(\n        text=examples[\"question\"],\n        text_pair=examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we\n    # need a map from a feature to its corresponding example. This key gives us just\n    # that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"id\"] = list()\n\n    for i in range(len(tokenized_examples.input_ids)):\n        # Grab the sequence corresponding to that example (to know what is the context\n        # and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n\n        # One example can give several spans, this is the index of the example\n        # containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples.id.append(examples[\"id\"][sample_index])\n\n        # Set to (-1, -1) the offset_mapping that are not part of the context so it's\n        # easy to determine if a token position is part of the context or not.\n        tokenized_examples.offset_mapping[i] = [\n            (o if sequence_ids[k] == context_index else (-1, -1))\n            for k, o in enumerate(tokenized_examples.offset_mapping[i])\n        ]\n\n    return tokenized_examples\n\n\ndef postprocess_predictions_and_labels(docs\n    predictions: list,\n    dataset: \"Dataset\",\n    prepared_dataset: \"Dataset\",\n    cls_token_index: int,\n) -&gt; tuple[list[dict], list[dict]]:\n    \"\"\"Postprocess the predictions and labels, to allow easier metric computation.\n\n    Args:\n        predictions:\n            A pair of (start_logits, end_logits) predictions.\n        dataset:\n            The dataset containing the examples.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The postprocessed predictions and labels.\n    \"\"\"\n    # Extract the logits from the predictions\n    all_start_logits = predictions[0]\n    all_end_logits = predictions[1]\n\n    # Build a map from an example to its corresponding features, being the blocks of\n    # text from the context that we're feeding into the model. An example can have\n    # multiple features/blocks if it has a long context.\n    id_to_index = {k: i for i, k in enumerate(dataset[\"id\"])}\n    features_per_example = defaultdict(list)\n    for i, feature in enumerate(prepared_dataset):\n        id = feature[\"id\"]\n        example_index = id_to_index[id]\n        features_per_example[example_index].append(i)\n\n    # Loop over all the examples\n    predictions = list()\n    labels = list()\n    for example_index, example in enumerate(dataset):\n        # Extract the best valid answer associated with the current example\n        best_answer = find_best_answer(\n            all_start_logits=all_start_logits,\n            all_end_logits=all_end_logits,\n            prepared_dataset=prepared_dataset,\n            feature_indices=features_per_example[example_index],\n            context=example[\"context\"],\n            max_answer_length=30,\n            num_best_logits=20,\n            min_null_score=0.0,\n            cls_token_index=cls_token_index,\n        )\n\n        # Create the final prediction dictionary, to be added to the list of\n        # predictions\n        prediction = dict(\n            id=example[\"id\"], prediction_text=best_answer, no_answer_probability=0.0\n        )\n\n        # Add the answer to the list of predictions\n        predictions.append(prediction)\n\n        # Create the associated reference dictionary, to be added to the list of\n        # references\n        label = dict(\n            id=example[\"id\"],\n            answers=dict(\n                text=example[\"answers\"][\"text\"],\n                answer_start=example[\"answers\"][\"answer_start\"],\n            ),\n        )\n\n        # Add the answer and label to the list of predictions and labels, respectively\n        labels.append(label)\n\n    return predictions, labels\n\n\ndef find_best_answer(docs\n    all_start_logits: np.ndarray,\n    all_end_logits: np.ndarray,\n    prepared_dataset: \"Dataset\",\n    feature_indices: list[int],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n    cls_token_index: int,\n) -&gt; str:\n    \"\"\"Find the best answer for a given example.\n\n    Args:\n        all_start_logits:\n            The start logits for all the features.\n        all_end_logits:\n            The end logits for all the features.\n        prepared_dataset:\n            The dataset containing the prepared examples.\n        feature_indices:\n            The indices of the features associated with the current example.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider.\n        min_null_score:\n            The minimum score an answer can have.\n        cls_token_index:\n            The index of the CLS token.\n\n    Returns:\n        The best answer for the example.\n    \"\"\"\n    # Loop through all the features associated to the current example\n    valid_answers = list()\n    for feature_index in feature_indices:\n        # Get the features associated with the current example\n        features = prepared_dataset[feature_index]\n\n        # Get the predictions of the model for this feature\n        start_logits = all_start_logits[feature_index]\n        end_logits = all_end_logits[feature_index]\n\n        # Update minimum null prediction\n        cls_index = features[\"input_ids\"].index(cls_token_index)\n        feature_null_score = (start_logits[cls_index] + end_logits[cls_index]).item()\n        if min_null_score &lt; feature_null_score:\n            min_null_score = feature_null_score\n\n        # Find the valid answers for the feature\n        valid_answers_for_feature = find_valid_answers(\n            start_logits=start_logits,\n            end_logits=end_logits,\n            offset_mapping=features[\"offset_mapping\"],\n            context=context,\n            max_answer_length=max_answer_length,\n            num_best_logits=num_best_logits,\n            min_null_score=min_null_score,\n        )\n        valid_answers.extend(valid_answers_for_feature)\n\n    # In the very rare edge case we have not a single non-null prediction, we create a\n    # fake prediction to avoid failure\n    if not valid_answers:\n        return \"\"\n\n    # Otherwise, we select the answer with the largest score as the best answer, and\n    # return it\n    best_answer_dict = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n    return best_answer_dict[\"text\"]\n\n\ndef find_valid_answers(docs\n    start_logits: np.ndarray,\n    end_logits: np.ndarray,\n    offset_mapping: list[tuple[int, int]],\n    context: str,\n    max_answer_length: int,\n    num_best_logits: int,\n    min_null_score: float,\n) -&gt; list[dict]:\n    \"\"\"Find the valid answers from the start and end indexes.\n\n    Args:\n        start_logits:\n            The logits for the start of the answer.\n        end_logits:\n            The logits for the end of the answer.\n        offset_mapping:\n            The offset mapping, being a list of pairs of integers for each token index,\n            containing the start and end character index in the original context.\n        context:\n            The context of the example.\n        max_answer_length:\n            The maximum length of the answer.\n        num_best_logits:\n            The number of best logits to consider. Note that this function will run in\n            O(`num_best_logits` ^ 2) time.\n        min_null_score:\n            The minimum score an answer can have.\n\n    Returns:\n        A list of the valid answers, each being a dictionary with keys \"text\" and\n        \"score\", the score being the sum of the start and end logits.\n    \"\"\"\n    # Fetch the top-k predictions for the start- and end token indices\n    start_indexes = np.argsort(start_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n    end_indexes = np.argsort(end_logits)[-1 : -num_best_logits - 1 : -1].tolist()\n\n    # We loop over all combinations of starting and ending indexes for valid answers\n    valid_answers = list()\n    for start_index in start_indexes:\n        for end_index in end_indexes:\n            # If the starting or ending index is out-of-scope, meaning that they are\n            # either out of bounds or correspond to part of the input_ids that are not\n            # in the context, then we skip this index\n            if (\n                start_index &gt;= len(offset_mapping)\n                or end_index &gt;= len(offset_mapping)\n                or tuple(offset_mapping[start_index]) == (-1, -1)\n                or tuple(offset_mapping[end_index]) == (-1, -1)\n            ):\n                continue\n\n            # Do not consider answers with a length that is either negative or greater\n            # than the context length\n            max_val = max_answer_length + start_index - 1\n            if end_index &lt; start_index or end_index &gt; max_val:\n                continue\n\n            # If we got to this point then the answer is valid, so we store the\n            # corresponding start- and end character indices in the original context,\n            # and from these extract the answer\n            start_char = offset_mapping[start_index][0]\n            end_char = offset_mapping[end_index][1]\n            text = context[start_char:end_char]\n\n            # Compute the score of the answer, being the sum of the start and end\n            # logits. Intuitively, this indicates how likely the answer is to be\n            # correct, and allows us to pick the best valid answer.\n            score = start_logits[start_index] + end_logits[end_index]\n\n            # Add the answer to the list of valid answers, if the score is greater\n            # than the minimum null score\n            if score &gt; min_null_score:\n                valid_answers.append(dict(score=score, text=text))\n\n    return valid_answers\n</code></pre>"},{"location":"api/euroeval/task_utils/sequence_classification/","title":"euroeval.task_utils.sequence_classification","text":"euroeval.task_utils.sequence_classification<p> source module euroeval.task_utils.sequence_classification </p> <p>Utility functions related to the sequence-classification task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>get_closest_logprobs_labels \u2014 Get the labels with the highest predicted logprob value.</p> </li> <li> <p>get_closest_word_edit_labels \u2014 Get the labels with the smallest edit distance to the predicted labels.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> source get_closest_logprobs_labels(generation_logprobs: list[list[list[tuple[str, float]]]], dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the highest predicted logprob value.</p> <p>In case a candidate label is split into multiple tokens, we only use the first token to compute the logprob value. E.g., if the candidate label \"positive\" is tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to represent the logprob value of the entire label.</p> <p> Parameters </p> <ul> <li> <p>generation_logprobs :  list[list[list[tuple[str, float]]]] \u2014</p> <p>The logprobs of the generated tokens, for all samples in the batch. Of shape (batch_size, num_tokens, num_logprobs).</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If no candidate label can be found for any of the generated labels.</p> </li> </ul> <p> source get_closest_word_edit_labels(generated_sequences: list[str], dataset_config: DatasetConfig) \u2192 list[str] </p> <p>Get the labels with the smallest edit distance to the predicted labels.</p> <p> Parameters </p> <ul> <li> <p>generated_sequences :  list[str] \u2014</p> <p>The generated sequences from the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The candidate labels with the smallest edit distance to the predicted labels.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/sequence_classification/","title":"euroeval.task_utils.sequence_classification","text":"euroeval.task_utils.sequence_classification<p> docs module euroeval.task_utils.sequence_classification </p> <pre><code>\"\"\"Utility functions related to the sequence-classification task group.\"\"\"\n\nimport logging\nimport re\nimport typing as t\n\nimport evaluate\nimport Levenshtein\nimport numpy as np\nfrom evaluate import EvaluationModule\n\nfrom ..data_models import BenchmarkConfig, GenerativeModelOutput\nfrom ..utils import log_once, raise_if_model_output_contains_nan_values\n\nif t.TYPE_CHECKING:\n    from ..data_models import DatasetConfig\n    from ..types import Labels, Predictions\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    label2id = {label: idx for idx, label in dataset_config.id2label.items()}\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    if model_output_dtype in [np.float16, np.float32, np.float64]:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    prompt_label_to_label_mapping = {\n        prompt_label: label\n        for label, prompt_label in dataset_config.prompt_label_mapping.items()\n    }\n    predictions = [\n        (\n            label2id[prompt_label_to_label_mapping[pred.lower()]]\n            if isinstance(pred, str)\n            else pred\n        )\n        for pred in predictions\n    ]\n\n    label_ids = [\n        label2id[label.lower()] if isinstance(label, str) else label for label in labels\n    ]\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n        score_dict: dict[str, float] | None = metric.compute(\n            predictions=predictions, references=label_ids, **cfg.compute_kwargs\n        )\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list],\n    model_output: GenerativeModelOutput,\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    if model_output.scores is not None:\n        return get_closest_logprobs_labels(\n            generation_logprobs=model_output.scores, dataset_config=dataset_config\n        )\n    else:\n        return get_closest_word_edit_labels(\n            generated_sequences=model_output.sequences, dataset_config=dataset_config\n        )\n\n\ndef get_closest_logprobs_labels(docs\n    generation_logprobs: list[list[list[tuple[str, float]]]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[str]:\n    \"\"\"Get the labels with the highest predicted logprob value.\n\n    In case a candidate label is split into multiple tokens, we only use the first\n    token to compute the logprob value. E.g., if the candidate label \"positive\" is\n    tokenised as [\"pos\", \"itive\"], we only use the logprob value of \"pos\" to\n    represent the logprob value of the entire label.\n\n    Args:\n        generation_logprobs:\n            The logprobs of the generated tokens, for all samples in the batch. Of shape\n            (batch_size, num_tokens, num_logprobs).\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n\n    Raises:\n        InvalidBenchmark:\n            If no candidate label can be found for any of the generated labels.\n    \"\"\"\n    english_labels = list(dataset_config.id2label.values())\n    english2local = dataset_config.prompt_label_mapping\n    candidate_labels = [\n        english2local[lbl].lower() for lbl in english_labels\n    ] + english_labels\n\n    output_labels: list[str] = list()\n    for sample in generation_logprobs:\n        for logprob_list in sample:\n            generated_labels = [\n                re.sub(\n                    pattern=r\"^[^a-z\u00e6\u00f8\u00e5\u00fc\u00f6\u00e4]+|[^a-z\u00e6\u00f8\u00e5\u00fc\u00f6\u00e4]+$\",\n                    repl=\"\",\n                    string=label.lower(),\n                )\n                for label, _ in logprob_list\n            ]\n            generated_labels = [label for label in generated_labels if label != \"\"]\n\n            # We want to use the first generated label which starts with a candidate\n            # label, as the output label\n            output_label: str | None = None\n            for generated_label in generated_labels:\n                candidate_output_labels = [\n                    candidate_label\n                    for candidate_label in candidate_labels\n                    if candidate_label.startswith(generated_label)\n                ]\n                if candidate_output_labels:\n                    output_label = candidate_output_labels[0]\n                    break\n\n            if output_label is not None:\n                output_label = english2local.get(output_label, output_label)\n                output_labels.append(output_label)\n                break\n        else:\n            if len(sample) == 0:\n                log_once(\n                    \"The model outputted an empty string, so no candidate labels could \"\n                    f\"be determined. Using {candidate_labels[0]!r} as the output \"\n                    \"label.\",\n                    level=logging.DEBUG,\n                )\n            else:\n                log_once(\n                    \"Could not find a candidate label for any of the generated \"\n                    f\"labels in the sample {sample}. Using {candidate_labels[0]!r} \"\n                    \"as the output label.\",\n                    level=logging.DEBUG,\n                )\n            output_labels.append(candidate_labels[0])\n\n    assert len(output_labels) == len(generation_logprobs)\n    return output_labels\n\n\ndef get_closest_word_edit_labels(docs\n    generated_sequences: list[str], dataset_config: \"DatasetConfig\"\n) -&gt; list[str]:\n    \"\"\"Get the labels with the smallest edit distance to the predicted labels.\n\n    Args:\n        generated_sequences:\n            The generated sequences from the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The candidate labels with the smallest edit distance to the predicted labels.\n    \"\"\"\n    candidate_labels = [\n        dataset_config.prompt_label_mapping[lbl]\n        for lbl in dataset_config.id2label.values()\n    ]\n    new_predicted_labels: list[str] = list()\n    for predicted_label in generated_sequences:\n        edit_distances = [\n            Levenshtein.distance(s1=predicted_label.lower(), s2=candidate_label.lower())\n            for candidate_label in candidate_labels\n        ]\n        closest_label = candidate_labels[np.argmin(edit_distances).item()]\n        new_predicted_labels.append(closest_label)\n    return new_predicted_labels\n</code></pre>"},{"location":"api/euroeval/task_utils/text_to_text/","title":"euroeval.task_utils.text_to_text","text":"euroeval.task_utils.text_to_text<p> source module euroeval.task_utils.text_to_text </p> <p>Utility functions related to the text-to-text task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first sequence contains the model outputs and the second sequence contains the true labels.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/text_to_text/","title":"euroeval.task_utils.text_to_text","text":"euroeval.task_utils.text_to_text<p> docs module euroeval.task_utils.text_to_text </p> <pre><code>\"\"\"Utility functions related to the text-to-text task group.\"\"\"\n\nimport logging\nimport typing as t\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\n\nfrom ..constants import METRIC_ATTRIBUTES_TAKING_UP_MEMORY\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..exceptions import InvalidBenchmark\nfrom ..utils import (\n    HiddenPrints,\n    clear_memory,\n    raise_if_model_output_contains_nan_values,\n)\n\nif t.TYPE_CHECKING:\n    from ..types import Labels, Predictions\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first sequence contains the model outputs and the second sequence\n            contains the true labels.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    model_output_dtype = np.asarray(model_outputs).dtype\n    output_is_prob = model_output_dtype in [np.float16, np.float32, np.float64]\n    if output_is_prob:\n        predictions = np.asarray(model_outputs).argmax(axis=-1)\n    else:\n        predictions = model_outputs\n\n    results: dict[str, float] = dict()\n    for cfg in dataset_config.task.metrics:\n        metric = metrics[cfg.name]\n        assert isinstance(metric, EvaluationModule)\n\n        # Some metrics can be computed on hardware accelerators. In this case we\n        # start by setting the device to the same device as the model\n        if cfg.compute_kwargs.get(\"device\", None) == \"auto\":\n            cfg.compute_kwargs[\"device\"] = benchmark_config.device.type\n\n        while True:\n            try:\n                with HiddenPrints():\n                    score_dict: dict[str, float] | None = metric.compute(\n                        predictions=predictions, references=labels, **cfg.compute_kwargs\n                    )\n\n                # Clear the cache of the BERTScorer to avoid memory leaks\n                for attribute in METRIC_ATTRIBUTES_TAKING_UP_MEMORY:\n                    if hasattr(metric, attribute):\n                        delattr(metric, attribute)\n\n                clear_memory()\n                break\n            except Exception as e:\n                # Clear the cache of the BERTScorer to avoid memory leaks\n                if hasattr(metric, \"cached_bertscorer\"):\n                    del metric.cached_bertscorer\n                    clear_memory()\n\n                oom_error = [\n                    \"CUDA out of memory\",\n                    \"CUDA error\",\n                    \"MPS backend out of memory\",\n                ]\n                if not any(error in str(e) for error in oom_error):\n                    raise InvalidBenchmark(str(e))\n\n                if cfg.compute_kwargs.get(\"batch_size\", 1) &gt; 1:\n                    batch_size = cfg.compute_kwargs[\"batch_size\"]\n                    cfg.compute_kwargs[\"batch_size\"] = batch_size // 2\n                    logger.debug(\n                        \"Out of memory error occurred during the computation of \"\n                        f\"the metric {cfg.pretty_name}. Reducing the batch size to \"\n                        f\"{cfg.compute_kwargs['batch_size']}.\"\n                    )\n                elif cfg.compute_kwargs.get(\"device\", \"cpu\") != \"cpu\":\n                    cfg.compute_kwargs[\"batch_size\"] = 32\n                    cfg.compute_kwargs[\"device\"] = \"cpu\"\n                    logger.debug(\n                        \"Out of memory error occurred during the computation of \"\n                        f\"the metric {cfg.pretty_name}. Moving the computation to \"\n                        \"the CPU.\"\n                    )\n                else:\n                    raise InvalidBenchmark(str(e))\n\n        # The metric returns None if we are running on multi-GPU and the current\n        # process is not the main process\n        if score_dict is not None:\n            scores = score_dict[cfg.results_key]\n            if isinstance(scores, list):\n                scores = sum(scores) / len(scores)\n            results[cfg.name] = scores\n\n    return results\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    return model_output.sequences\n</code></pre>"},{"location":"api/euroeval/task_utils/token_classification/","title":"euroeval.task_utils.token_classification","text":"euroeval.task_utils.token_classification<p> source module euroeval.task_utils.token_classification </p> <p>Utility functions related to the token-classification task group.</p> <p> Functions </p> <ul> <li> <p>compute_metrics \u2014 Compute the metrics needed for evaluation.</p> </li> <li> <p>extract_labels_from_generation \u2014 Extract the predicted labels from the generated output.</p> </li> <li> <p>tokenize_and_align_labels \u2014 Tokenise all texts and align the labels with them.</p> </li> <li> <p>handle_unk_tokens \u2014 Replace unknown tokens in the tokens with the corresponding word.</p> </li> </ul> <p> source compute_metrics(model_outputs_and_labels: tuple[Predictions, Labels], has_misc_tags: bool, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Compute the metrics needed for evaluation.</p> <p> Parameters </p> <ul> <li> <p>model_outputs_and_labels :  tuple[Predictions, Labels] \u2014</p> <p>The first array contains the probability predictions and the second array contains the true labels.</p> </li> <li> <p>has_misc_tags :  bool \u2014</p> <p>Whether the dataset has MISC tags.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary with the names of the metrics as keys and the metric values as values.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source extract_labels_from_generation(input_batch: dict[str, list], model_output: GenerativeModelOutput, dataset_config: DatasetConfig) \u2192 list[t.Any] </p> <p>Extract the predicted labels from the generated output.</p> <p> Parameters </p> <ul> <li> <p>input_batch :  dict[str, list] \u2014</p> <p>The input batch, where the keys are the feature names and the values are lists with the feature values.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The raw generated output of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[t.Any] \u2014 The predicted labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul> <p> source tokenize_and_align_labels(examples: dict, tokenizer: PreTrainedTokenizer, label2id: dict[str, int]) \u2192 BatchEncoding </p> <p>Tokenise all texts and align the labels with them.</p> <p> Parameters </p> <ul> <li> <p>examples :  dict \u2014</p> <p>The examples to be tokenised.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>A pretrained tokenizer.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>A dictionary that converts NER tags to IDs.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BatchEncoding \u2014 A dictionary containing the tokenized data as well as labels.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source handle_unk_tokens(tokenizer: PreTrainedTokenizer, tokens: list[str], words: list[str]) \u2192 list[str] </p> <p>Replace unknown tokens in the tokens with the corresponding word.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the words.</p> </li> <li> <p>tokens :  list[str] \u2014</p> <p>The list of tokens.</p> </li> <li> <p>words :  list[str] \u2014</p> <p>The list of words.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The list of tokens with unknown tokens replaced by the corresponding word.</p> </li> </ul>"},{"location":"src/euroeval/task_utils/token_classification/","title":"euroeval.task_utils.token_classification","text":"euroeval.task_utils.token_classification<p> docs module euroeval.task_utils.token_classification </p> <pre><code>\"\"\"Utility functions related to the token-classification task group.\"\"\"\n\nimport importlib.util\nimport logging\nimport re\nimport typing as t\nfrom copy import deepcopy\n\nimport evaluate\nimport numpy as np\nfrom evaluate import EvaluationModule\nfrom transformers import PreTrainedTokenizer\n\nfrom ..data_models import BenchmarkConfig, DatasetConfig, GenerativeModelOutput\nfrom ..exceptions import InvalidBenchmark, NeedsExtraInstalled\nfrom ..utils import raise_if_model_output_contains_nan_values\n\nif t.TYPE_CHECKING:\n    from transformers import BatchEncoding\n\n    from ..types import Labels, Predictions\n\nif importlib.util.find_spec(\"demjson3\") is not None:\n    import demjson3\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef compute_metrics(docs\n    model_outputs_and_labels: tuple[\"Predictions\", \"Labels\"],\n    has_misc_tags: bool,\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Compute the metrics needed for evaluation.\n\n    Args:\n        model_outputs_and_labels:\n            The first array contains the probability predictions and the second\n            array contains the true labels.\n        has_misc_tags:\n            Whether the dataset has MISC tags.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        A dictionary with the names of the metrics as keys and the metric values as\n        values.\n    \"\"\"\n    model_outputs, labels = model_outputs_and_labels\n    raise_if_model_output_contains_nan_values(model_output=model_outputs)\n\n    metrics = {\n        metric_cfg.name: (\n            evaluate.load(\n                path=metric_cfg.huggingface_id, cache_dir=benchmark_config.cache_dir\n            )\n            if metric_cfg.huggingface_id != \"\"\n            else None\n        )\n        for metric_cfg in dataset_config.task.metrics\n    }\n\n    predictions: list[list[str]]\n    if not isinstance(model_outputs[0][0], str):\n        raw_predictions: list[list[int]] = np.argmax(model_outputs, axis=-1).tolist()\n\n        # Remove ignored index (special tokens)\n        predictions = [\n            [\n                dataset_config.id2label[pred_id]\n                for pred_id, lbl_id in zip(pred, label)\n                if lbl_id != -100\n            ]\n            for pred, label in zip(raw_predictions, labels)\n        ]\n        labels = [\n            [\n                (\n                    dataset_config.id2label[int(lbl_id)]\n                    if isinstance(lbl_id, int) or isinstance(lbl_id, np.int_)\n                    else lbl_id\n                )\n                for lbl_id in label\n                if lbl_id != -100\n            ]\n            for label in labels\n        ]\n\n    else:\n        predictions = model_outputs  # type: ignore[assignment]\n\n    # Replace predicted tag with either MISC or O tags if they are not part of the\n    # dataset\n    labels_without_misc = {\n        label\n        for label in dataset_config.id2label.values()\n        if label not in {\"b-misc\", \"i-misc\"}\n    }\n    ner_tag: str\n    for i, prediction_list in enumerate(predictions):\n        for j, ner_tag in enumerate(prediction_list):\n            if ner_tag not in labels_without_misc:\n                if has_misc_tags and ner_tag[:2] == \"b-\":\n                    predictions[i][j] = \"b-misc\"\n                elif has_misc_tags and ner_tag[:2] == \"i-\":\n                    predictions[i][j] = \"i-misc\"\n                else:\n                    predictions[i][j] = \"o\"\n\n    # Remove MISC labels from predictions\n    predictions_no_misc = deepcopy(predictions)\n    for i, prediction_list in enumerate(predictions_no_misc):\n        for j, ner_tag in enumerate(prediction_list):\n            if ner_tag[-4:] == \"misc\":\n                predictions_no_misc[i][j] = \"o\"\n\n    # Remove MISC labels from labels\n    labels_no_misc: list[list[str]] = deepcopy(labels)  # type: ignore[arg-type]\n    for i, label_list in enumerate(labels_no_misc):\n        for j, ner_tag in enumerate(label_list):\n            if (\n                isinstance(ner_tag, str)\n                and len(ner_tag) &gt;= 4\n                and ner_tag[-4:] == \"misc\"\n            ):\n                labels_no_misc[i][j] = \"o\"\n\n    # Compute the metrics\n    # We manually set the F1 metric to be 100% if both the labels and the models\n    # have no NER tags in them, since this causes an error with the `compute`\n    # method otherwise\n    predictions_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in prediction_list)\n        for prediction_list in predictions\n    )\n    labels_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in label_list) for label_list in labels\n    )\n    if predictions_all_zero and labels_all_zero:\n        results = dict(overall_f1=1.0)\n    else:\n        metric = metrics[\"micro_f1\"]\n        assert isinstance(metric, EvaluationModule)\n        results = metric.compute(predictions=predictions, references=labels)\n\n    # Compute the metrics without MISC tags\n    # We manually set the F1 metric to be 100% if both the labels and the models\n    # have no NER tags in them, since this causes an error with the `compute`\n    # method otherwise\n    predictions_no_misc_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in prediction_list)\n        for prediction_list in predictions_no_misc\n    )\n    labels_no_misc_all_zero = all(\n        all(ner_tag == \"o\" for ner_tag in label_list) for label_list in labels_no_misc\n    )\n    if predictions_no_misc_all_zero and labels_no_misc_all_zero:\n        results_no_misc = dict(overall_f1=1.0)\n    else:\n        metric = metrics[\"micro_f1_no_misc\"]\n        assert isinstance(metric, EvaluationModule)\n        results_no_misc = metric.compute(\n            predictions=predictions_no_misc, references=labels_no_misc\n        )\n\n    # Raise error if the metrics are invalid\n    if results is None or results_no_misc is None:\n        raise InvalidBenchmark(\"The predictions and labels are not of the same length.\")\n\n    return dict(\n        micro_f1_no_misc=results_no_misc[\"overall_f1\"], micro_f1=results[\"overall_f1\"]\n    )\n\n\ndef extract_labels_from_generation(docs\n    input_batch: dict[str, list],\n    model_output: \"GenerativeModelOutput\",\n    dataset_config: \"DatasetConfig\",\n) -&gt; list[t.Any]:\n    \"\"\"Extract the predicted labels from the generated output.\n\n    Args:\n        input_batch:\n            The input batch, where the keys are the feature names and the values\n            are lists with the feature values.\n        model_output:\n            The raw generated output of the model.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        The predicted labels.\n    \"\"\"\n    if importlib.util.find_spec(\"demjson3\") is None:\n        raise NeedsExtraInstalled(extra=\"generative\")\n\n    raw_predictions = model_output.sequences\n\n    # Attempt to extract the JSON dictionary from the predictions\n    json_regex = r\"\\{.+?\\}\"\n    json_matches = [\n        re.search(pattern=json_regex, string=raw_prediction, flags=re.DOTALL)\n        or raw_prediction\n        for raw_prediction in raw_predictions\n    ]\n    raw_predictions = [\n        json_match.group() if isinstance(json_match, re.Match) else json_match\n        for json_match in json_matches\n    ]\n\n    tokens = input_batch[\"tokens\"]\n    predicted_labels: list[list[str]] = [[\"o\"] * len(token_ids) for token_ids in tokens]\n    for idx, raw_prediction in enumerate(raw_predictions):\n        try:\n            json_output = demjson3.decode(txt=raw_prediction)\n            if not isinstance(json_output, dict):\n                logger.debug(\n                    \"The model output is not a JSON dictionary, so cannot parse \"\n                    f\"it. Skipping. Here is the output: {raw_prediction}\"\n                )\n                continue\n            elif not all(isinstance(key, str) for key in json_output.keys()):\n                logger.debug(\n                    \"The model output is not a JSON dictionary with string keys, \"\n                    \"so cannot parse it. Skipping. Here is the output: \"\n                    f\"{raw_prediction}\"\n                )\n                continue\n            elif not all(isinstance(value, list) for value in json_output.values()):\n                logger.debug(\n                    \"The model output is not a JSON dictionary with list values, \"\n                    \"so cannot parse it. Skipping. Here is the output: \"\n                    f\"{raw_prediction}\"\n                )\n                continue\n            prediction_dict: dict[str, list[str]] = json_output\n        except demjson3.JSONDecodeError:\n            logger.debug(\n                \"The model output is not valid JSON, so cannot parse it. Skipping. \"\n                f\"Here is the output: {raw_prediction!r}\"\n            )\n            continue\n\n        prompt_label_mapping = dataset_config.prompt_label_mapping\n        for prompt_tag_name, named_entities in prediction_dict.items():\n            try:\n                tag_name = [\n                    tag[2:]\n                    for tag, prompt_tag in prompt_label_mapping.items()\n                    if prompt_tag == prompt_tag_name\n                ][0]\n            except IndexError:\n                logger.debug(\n                    \"The model produced an invalid prompt tag name, \"\n                    f\"{prompt_tag_name}. Skipping.\"\n                )\n                continue\n\n            named_entities = [str(named_entity) for named_entity in named_entities]\n            for named_entity in named_entities:\n                for ne_idx, named_entity_word in enumerate(named_entity.split()):\n                    for token_idx, token in enumerate(tokens[idx]):\n                        if named_entity_word in token:\n                            if ne_idx == 0:\n                                predicted_labels[idx][token_idx] = f\"b-{tag_name}\"\n                            elif (\n                                predicted_labels[idx][token_idx] == \"o\"\n                                and predicted_labels[idx][token_idx - 1][2:] == tag_name\n                            ):\n                                predicted_labels[idx][token_idx] = f\"i-{tag_name}\"\n    return predicted_labels\n\n\ndef tokenize_and_align_labels(docs\n    examples: dict, tokenizer: \"PreTrainedTokenizer\", label2id: dict[str, int]\n) -&gt; \"BatchEncoding\":\n    \"\"\"Tokenise all texts and align the labels with them.\n\n    Args:\n        examples:\n            The examples to be tokenised.\n        tokenizer:\n            A pretrained tokenizer.\n        label2id:\n            A dictionary that converts NER tags to IDs.\n\n    Returns:\n        A dictionary containing the tokenized data as well as labels.\n    \"\"\"\n    # Tokenize the texts. We use the `is_split_into_words` argument here because\n    # the texts in our dataset are lists of words (with a label for each word)\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], is_split_into_words=True, truncation=True, padding=True\n    )\n\n    # Extract a mapping between all the tokens and their corresponding word. If the\n    # tokenizer is of a \"fast\" variant then this can be accessed through the\n    # `word_ids` method. Otherwise, we have to extract it manually.\n    all_labels: list[list[int]] = list()\n    labels: list[str]\n    word_ids: list[int | None]\n    for i, labels in enumerate(examples[\"labels\"]):\n        # Try to get the word IDs from the tokenizer\n        try:\n            word_ids = tokenized_inputs.word_ids(batch_index=i)\n\n        # If the tokenizer is not of a \"fast\" variant, we have to extract the word\n        # IDs manually\n        except ValueError:\n            # Get the list of words in the document\n            words: list[str] = examples[\"tokens\"][i]\n\n            # Get the list of token IDs in the document\n            tok_ids: list[int] = tokenized_inputs.input_ids[i]\n\n            # Decode the token IDs\n            tokens = tokenizer.convert_ids_to_tokens(tok_ids)\n            assert isinstance(tokens, list)\n\n            # Remove prefixes from the tokens\n            prefixes_to_remove = [\"\u2581\", \"##\"]\n            for tok_idx, tok in enumerate(tokens):\n                if tok:\n                    for prefix in prefixes_to_remove:\n                        if tok.startswith(prefix):\n                            tokens[tok_idx] = tok[len(prefix) :]\n\n            # Replace UNK tokens with the correct word\n            tokens = handle_unk_tokens(tokenizer=tokenizer, tokens=tokens, words=words)\n\n            # Get list of special tokens. Some tokenizers do not record these\n            # properly, which is why we convert the values to their indices and\n            # then back to strings\n            sp_toks = [\n                tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(sp_tok))\n                for sp_tok in tokenizer.special_tokens_map.values()\n            ]\n\n            # Replace special tokens with `None`\n            tokens_with_none = [None if tok in sp_toks else tok for tok in tokens]\n\n            # Get the alignment between the words and the tokens, on a character\n            # level\n            word_idxs = [\n                word_idx for word_idx, word in enumerate(words) for _ in str(word)\n            ]\n            token_idxs = [\n                tok_idx\n                for tok_idx, tok_or_none in enumerate(tokens_with_none)\n                for _ in str(tok_or_none)\n                if tok_or_none is not None\n            ]\n            alignment = list(zip(word_idxs, token_idxs))\n\n            # Raise error if there are not as many characters in the words as in\n            # the tokens. This can be due to the use of a different prefix.\n            if len(word_idxs) != len(token_idxs):\n                raise InvalidBenchmark(\n                    \"The tokens could not be aligned with the words during manual \"\n                    \"word-token alignment. It seems that the tokenizer is neither \"\n                    \"of the fast variant nor of a SentencePiece/WordPiece variant.\"\n                )\n\n            # Get the aligned word IDs\n            word_ids = list()\n            for tok_idx, tok_or_none in enumerate(tokens_with_none):\n                if tok_or_none is None or tok_or_none == \"\":\n                    word_ids.append(None)\n                else:\n                    word_idx = [\n                        word_idx\n                        for word_idx, token_idx in alignment\n                        if token_idx == tok_idx\n                    ][0]\n                    word_ids.append(word_idx)\n\n        previous_word_idx: int | None = None\n        label_ids: list[int] = list()\n        for word_id in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100\n            # so they are automatically ignored in the loss function\n            if word_id is None:\n                label_ids.append(-100)\n\n            # We set the label for the first token of each word\n            elif word_id != previous_word_idx:\n                label = labels[word_id]\n                try:\n                    label_id = label2id[label.lower()]\n                except KeyError:\n                    msg = f\"The label {label} was not found in the model's config.\"\n                    raise InvalidBenchmark(msg)\n                label_ids.append(label_id)\n\n            # For the other tokens in a word, we set the label to -100\n            else:\n                label_ids.append(-100)\n\n            previous_word_idx = word_id\n\n        all_labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n\n\ndef handle_unk_tokens(docs\n    tokenizer: \"PreTrainedTokenizer\", tokens: list[str], words: list[str]\n) -&gt; list[str]:\n    \"\"\"Replace unknown tokens in the tokens with the corresponding word.\n\n    Args:\n        tokenizer:\n            The tokenizer used to tokenize the words.\n        tokens:\n            The list of tokens.\n        words:\n            The list of words.\n\n    Returns:\n        The list of tokens with unknown tokens replaced by the corresponding word.\n    \"\"\"\n    # Locate the token indices of the unknown tokens\n    token_unk_idxs = [i for i, tok in enumerate(tokens) if tok == tokenizer.unk_token]\n\n    # Locate the word indices of the words which contain an unknown token\n    word_unk_idxs = [\n        i\n        for i, word in enumerate(words)\n        if tokenizer.unk_token\n        in tokenizer.convert_ids_to_tokens(\n            tokenizer.encode(word, add_special_tokens=False)\n        )\n    ]\n\n    # Iterate over the token index and word index pairs\n    for tok_idx, word_idx in zip(token_unk_idxs, word_unk_idxs):\n        # Fetch the word\n        word = words[word_idx]\n\n        # Tokenize the word, which is now a list containing at least one UNK token\n        tokens_with_unk = tokenizer.convert_ids_to_tokens(\n            tokenizer.encode(word, add_special_tokens=False)\n        )\n\n        # Iterate over the tokens in the word\n        for possible_unk_token in tokens_with_unk:\n            # If the token is not an UNK token then we remove the first occurence\n            # of the content of this token from the word. The result of the `word`\n            # variable will be the content of the UNK token.\n            # NOTE: This is a bit hacky and not bulletproof. For instance, if the\n            # word is \"1925-1950\" and the tokenizer splits it into [\"[UNK]\", \"-\",\n            # \"19\", \"50\"], then the result will be 2519 instead of 1925. This\n            # happens almost never, however, so we can live with it.\n            if possible_unk_token != tokenizer.unk_token:\n                word = word.replace(possible_unk_token, \"\", 1)\n\n        # Replace the token with the word\n        tokens[tok_idx] = word\n\n    return tokens\n</code></pre>"},{"location":"api/euroeval/benchmarker/","title":"euroeval.benchmarker","text":"euroeval.benchmarker<p> source module euroeval.benchmarker </p> <p>Class that benchmarks language models.</p> <p> Classes </p> <ul> <li> <p>Benchmarker \u2014 Benchmarking all the Scandinavian language models.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>model_has_been_benchmarked \u2014 Checks whether a model has already been benchmarked on a dataset.</p> </li> <li> <p>adjust_logging_level \u2014 Adjust the logging level based on verbosity.</p> </li> <li> <p>clear_model_cache_fn \u2014 Clear the model cache.</p> </li> <li> <p>prepare_dataset_configs \u2014 Prepare the dataset configuration(s) to be benchmarked.</p> </li> <li> <p>initial_logging \u2014 Initial logging at the start of the benchmarking process.</p> </li> </ul> <p> source class Benchmarker(progress_bar: bool = True, save_results: bool = True, task: str | list[str] | None = None, dataset: list[str] | str | None = None, language: str | list[str] = 'all', model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int = 32, raise_errors: bool = False, cache_dir: str = '.euroeval_cache', api_key: str | None = None, force: bool = False, verbose: bool = False, trust_remote_code: bool = False, use_flash_attention: bool | None = None, clear_model_cache: bool = False, evaluate_test_split: bool = False, few_shot: bool = True, num_iterations: int = 10, api_base: str | None = None, api_version: str | None = None, debug: bool = False, run_with_cli: bool = False, only_allow_safetensors: bool = False) </p> <p>Benchmarking all the Scandinavian language models.</p> <p>Initialise the benchmarker.</p> <p> Attributes </p> <ul> <li> <p>benchmark_config_default_params \u2014</p> <p>The default parameters for the benchmark configuration.</p> </li> <li> <p>benchmark_config \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>force \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already.</p> </li> <li> <p>results_path \u2014</p> <p>The path to the results file.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether progress bars should be shown. Defaults to True.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to True.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Set this to 'all' if all languages should be considered. Defaults to \"all\".</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to None.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to None.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to None.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use. Defaults to 32.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation. Defaults to False.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models. Defaults to '.euroeval_cache'.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to False.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output. This is automatically set if <code>debug</code> is True. Defaults to False.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models. Defaults to False.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then it will be used if it is installed and the model is a decoder model. Defaults to None.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to False.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to False.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to True.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to 10.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API. Defaults to None.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Defaults to None.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to output debug information. Defaults to False.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmarker is being run from the command-line interface. Defaults to False.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014</p> <p>Whether to only allow models that use the safetensors format. Defaults to False.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>benchmark \u2014 Benchmarks models on datasets.</p> </li> </ul> <p> source property Benchmarker.benchmark_results: list[BenchmarkResult] </p> <p>The benchmark results.</p> <p> source method Benchmarker.benchmark(model: list[str] | str, task: str | list[str] | None = None, dataset: list[str] | str | None = None, progress_bar: bool | None = None, save_results: bool | None = None, language: str | list[str] | None = None, model_language: str | list[str] | None = None, dataset_language: str | list[str] | None = None, device: Device | None = None, batch_size: int | None = None, raise_errors: bool | None = None, cache_dir: str | None = None, api_key: str | None = None, force: bool | None = None, verbose: bool | None = None, trust_remote_code: bool | None = None, use_flash_attention: bool | None = None, clear_model_cache: bool | None = None, evaluate_test_split: bool | None = None, few_shot: bool | None = None, num_iterations: int | None = None, only_allow_safetensors: bool | None = None) \u2192 list[BenchmarkResult] </p> <p>Benchmarks models on datasets.</p> <p> Parameters </p> <ul> <li> <p>model :  list[str] | str \u2014</p> <p>The full Hugging Face Hub path(s) to the pretrained transformer model. The specific model version to use can be added after the suffix '@': \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id, and defaults to the latest version if not specified.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks benchmark the model(s) on. Mutually exclusive with <code>dataset</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>dataset :  list[str] | str | None \u2014</p> <p>The datasets to benchmark on. Mutually exclusive with <code>task</code>. If both <code>task</code> and <code>dataset</code> are None then all datasets will be benchmarked. Defaults to None.</p> </li> <li> <p>progress_bar :  bool | None \u2014</p> <p>Whether progress bars should be shown. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>save_results :  bool | None \u2014</p> <p>Whether to save the benchmark results to 'euroeval_benchmark_results.jsonl'. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If specified then this overrides the <code>language</code> parameter for model languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If specified then this overrides the <code>language</code> parameter for dataset languages. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for benchmarking. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>raise_errors :  bool | None \u2014</p> <p>Whether to raise errors instead of skipping the model evaluation.</p> </li> <li> <p>cache_dir :  str | None \u2014</p> <p>Directory to store cached models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>force :  bool | None \u2014</p> <p>Whether to force evaluations of models, even if they have been benchmarked already. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>verbose :  bool | None \u2014</p> <p>Whether to output additional output. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>trust_remote_code :  bool | None \u2014</p> <p>Whether to trust remote code when loading models. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>clear_model_cache :  bool | None \u2014</p> <p>Whether to clear the model cache after benchmarking each model. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>evaluate_test_split :  bool | None \u2014</p> <p>Whether to evaluate the test split of the datasets. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>few_shot :  bool | None \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>num_iterations :  int | None \u2014</p> <p>The number of times each model should be evaluated. This is only meant to be used for power users, and scores will not be allowed on the leaderboards if this is changed. Defaults to the value specified when initialising the benchmarker.</p> </li> <li> <p>only_allow_safetensors :  bool | None \u2014</p> <p>Whether to only allow models that use the safetensors format. Defaults to the value specified when initialising the benchmarker.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[BenchmarkResult] \u2014 A list of benchmark results.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If both <code>task</code> and <code>dataset</code> are specified.</p> </li> <li> <p>benchmark_output_or_err</p> </li> <li> <p>e</p> </li> </ul> <p> source model_has_been_benchmarked(model_id: str, dataset: str, few_shot: bool, validation_split: bool, benchmark_results: list[BenchmarkResult]) \u2192 bool </p> <p>Checks whether a model has already been benchmarked on a dataset.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>dataset :  str \u2014</p> <p>The dataset.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether the model was evaluated using few-shot evaluation.</p> </li> <li> <p>validation_split :  bool \u2014</p> <p>Whether the model was evaluated on the validation split.</p> </li> <li> <p>benchmark_results :  list[BenchmarkResult] \u2014</p> <p>The benchmark results.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the model has already been evaluated on the dataset.</p> </li> </ul> <p> source adjust_logging_level(verbose: bool, ignore_testing: bool = False) \u2192 int </p> <p>Adjust the logging level based on verbosity.</p> <p> Parameters </p> <ul> <li> <p>verbose :  bool \u2014</p> <p>Whether to output additional output.</p> </li> <li> <p>ignore_testing :  bool \u2014</p> <p>Whether to ignore the testing flag.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The logging level that was set.</p> </li> </ul> <p> source clear_model_cache_fn(cache_dir: str) \u2192 None </p> <p>Clear the model cache.</p> <p>Note that this will not remove the stored completions.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The path to the cache directory.</p> </li> </ul> <p> source prepare_dataset_configs(dataset_names: list[str]) \u2192 list[DatasetConfig] </p> <p>Prepare the dataset configuration(s) to be benchmarked.</p> <p> Parameters </p> <ul> <li> <p>dataset_names :  list[str] \u2014</p> <p>The dataset names to benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetConfig] \u2014 The prepared list of model IDs.</p> </li> </ul> <p> source initial_logging(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 None </p> <p>Initial logging at the start of the benchmarking process.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model we are evaluating.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset we are evaluating on.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The general benchmark configuration.</p> </li> </ul>"},{"location":"src/euroeval/benchmarker/","title":"euroeval.benchmarker","text":"euroeval.benchmarker<p> docs module euroeval.benchmarker </p> <pre><code>\"\"\"Class that benchmarks language models.\"\"\"\n\nimport json\nimport logging\nimport re\nimport sys\nimport typing as t\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom time import sleep\n\nfrom torch.distributed import destroy_process_group\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .constants import GENERATIVE_PIPELINE_TAGS\nfrom .data_loading import load_data\nfrom .data_models import BenchmarkConfigParams, BenchmarkResult\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device, ModelType\nfrom .exceptions import InvalidBenchmark, InvalidModel\nfrom .finetuning import finetune\nfrom .generation import generate\nfrom .model_config import get_model_config\nfrom .model_loading import load_model\nfrom .scores import log_scores\nfrom .speed_benchmark import benchmark_speed\nfrom .tasks import SPEED\nfrom .utils import enforce_reproducibility\n\nif t.TYPE_CHECKING:\n    from .benchmark_modules import BenchmarkModule\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass Benchmarker:docs\n    \"\"\"Benchmarking all the Scandinavian language models.\n\n    Attributes:\n        benchmark_config_default_params:\n            The default parameters for the benchmark configuration.\n        benchmark_config:\n            The benchmark configuration.\n        force:\n            Whether to force evaluations of models, even if they have been benchmarked\n            already.\n        results_path:\n            The path to the results file.\n        benchmark_results:\n            The benchmark results.\n    \"\"\"\n\n    def __init__(\n        self,\n        progress_bar: bool = True,\n        save_results: bool = True,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        language: str | list[str] = \"all\",\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        device: Device | None = None,\n        batch_size: int = 32,\n        raise_errors: bool = False,\n        cache_dir: str = \".euroeval_cache\",\n        api_key: str | None = None,\n        force: bool = False,\n        verbose: bool = False,\n        trust_remote_code: bool = False,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool = False,\n        evaluate_test_split: bool = False,\n        few_shot: bool = True,\n        num_iterations: int = 10,\n        api_base: str | None = None,\n        api_version: str | None = None,\n        debug: bool = False,\n        run_with_cli: bool = False,\n        only_allow_safetensors: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialise the benchmarker.\n\n        Args:\n            progress_bar:\n                Whether progress bars should be shown. Defaults to True.\n            save_results:\n                Whether to save the benchmark results to\n                'euroeval_benchmark_results.jsonl'. Defaults to True.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Set this to 'all' if all languages should be considered.\n                Defaults to \"all\".\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to None.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to None.\n            device:\n                The device to use for benchmarking. Defaults to None.\n            batch_size:\n                The batch size to use. Defaults to 32.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n                Defaults to False.\n            cache_dir:\n                Directory to store cached models. Defaults to '.euroeval_cache'.\n            api_key:\n                The API key to use for a given inference API.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to False.\n            verbose:\n                Whether to output additional output. This is automatically set if\n                `debug` is True. Defaults to False.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to False.\n            use_flash_attention:\n                Whether to use Flash Attention. If None then it will be used if it is\n                installed and the model is a decoder model. Defaults to None.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model.\n                Defaults to False.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. Defaults to False.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to True.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to 10.\n            api_base:\n                The base URL for a given inference API. Only relevant if `model` refers\n                to a model on an inference API. Defaults to None.\n            api_version:\n                The version of the API to use. Defaults to None.\n            debug:\n                Whether to output debug information. Defaults to False.\n            run_with_cli:\n                Whether the benchmarker is being run from the command-line interface.\n                Defaults to False.\n            only_allow_safetensors:\n                Whether to only allow models that use the safetensors format. Defaults\n                to False.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        self.benchmark_config_default_params = BenchmarkConfigParams(\n            progress_bar=progress_bar,\n            save_results=save_results,\n            task=task,\n            dataset=dataset,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            device=device,\n            batch_size=batch_size,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            api_key=api_key,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            evaluate_test_split=evaluate_test_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n            api_base=api_base,\n            api_version=api_version,\n            debug=debug,\n            run_with_cli=run_with_cli,\n            only_allow_safetensors=only_allow_safetensors,\n        )\n\n        self.benchmark_config = build_benchmark_config(\n            first_time=True, **self.benchmark_config_default_params.model_dump()\n        )\n\n        # Initialise variable storing model lists, so we only have to fetch it once\n        self._model_lists: dict[str, list[str]] | None = None\n\n        self.results_path = Path.cwd() / \"euroeval_benchmark_results.jsonl\"\n        adjust_logging_level(verbose=self.benchmark_config.verbose)\n\n    @property\n    def benchmark_results(self) -&gt; list[BenchmarkResult]:docs\n        \"\"\"The benchmark results.\"\"\"\n        if self.results_path.exists():\n            with self.results_path.open() as f:\n                return [\n                    BenchmarkResult.from_dict(json.loads(line))\n                    for line in f\n                    if line.strip()\n                ]\n        else:\n            return list()\n\n    def benchmark(docs\n        self,\n        model: list[str] | str,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        progress_bar: bool | None = None,\n        save_results: bool | None = None,\n        language: str | list[str] | None = None,\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        device: Device | None = None,\n        batch_size: int | None = None,\n        raise_errors: bool | None = None,\n        cache_dir: str | None = None,\n        api_key: str | None = None,\n        force: bool | None = None,\n        verbose: bool | None = None,\n        trust_remote_code: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool | None = None,\n        evaluate_test_split: bool | None = None,\n        few_shot: bool | None = None,\n        num_iterations: int | None = None,\n        only_allow_safetensors: bool | None = None,\n    ) -&gt; list[BenchmarkResult]:\n        \"\"\"Benchmarks models on datasets.\n\n        Args:\n            model:\n                The full Hugging Face Hub path(s) to the pretrained transformer model.\n                The specific model version to use can be added after the suffix '@':\n                \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id,\n                and defaults to the latest version if not specified.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked. Defaults to None.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n                Defaults to None.\n            progress_bar:\n                Whether progress bars should be shown. Defaults to the value specified\n                when initialising the benchmarker.\n            save_results:\n                Whether to save the benchmark results to\n                'euroeval_benchmark_results.jsonl'. Defaults to the value specified\n                when initialising the benchmarker.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n                to 'all' if all languages (also non-Scandinavian) should be considered.\n                Defaults to the value specified when initialising the benchmarker.\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to the value specified when initialising the benchmarker.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to the value specified when initialising the\n                benchmarker.\n            device:\n                The device to use for benchmarking. Defaults to the value specified when\n                initialising the benchmarker.\n            batch_size:\n                The batch size to use. Defaults to the value specified when initialising\n                the benchmarker.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n            cache_dir:\n                Directory to store cached models. Defaults to the value specified when\n                initialising the benchmarker.\n            api_key:\n                The API key to use for a given inference server. Defaults to the value\n                specified when initialising the benchmarker.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to the value specified when initialising\n                the benchmarker.\n            verbose:\n                Whether to output additional output. Defaults to the value specified\n                when initialising the benchmarker.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to the value\n                specified when initialising the benchmarker.\n            use_flash_attention:\n                Whether to use Flash Attention. Defaults to the value specified when\n                initialising the benchmarker.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model. Defaults\n                to the value specified when initialising the benchmarker.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. Defaults to the\n                value specified when initialising the benchmarker.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to the value specified\n                when initialising the benchmarker.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to the value specified when\n                initialising the benchmarker.\n            only_allow_safetensors:\n                Whether to only allow models that use the safetensors format. Defaults\n                to the value specified when initialising the benchmarker.\n\n        Returns:\n            A list of benchmark results.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        if task is not None and dataset is not None:\n            raise ValueError(\"Only one of `task` and `dataset` can be specified.\")\n\n        benchmark_config = self._get_updated_benchmark_config(\n            task=task,\n            dataset=dataset,\n            progress_bar=progress_bar,\n            save_results=save_results,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            device=device,\n            batch_size=batch_size,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            api_key=api_key,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            evaluate_test_split=evaluate_test_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n            only_allow_safetensors=only_allow_safetensors,\n        )\n\n        adjust_logging_level(verbose=benchmark_config.verbose)\n\n        if benchmark_config.clear_model_cache:\n            clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        model_ids = self._prepare_model_ids(model_id=model)\n        dataset_configs = prepare_dataset_configs(\n            dataset_names=benchmark_config.datasets\n        )\n\n        current_benchmark_results: list[BenchmarkResult] = list()\n        for m_id in model_ids:\n            try:\n                model_config = get_model_config(\n                    model_id=m_id, benchmark_config=benchmark_config\n                )\n            except InvalidModel as e:\n                logger.info(e.message)\n                continue\n\n            loaded_model: BenchmarkModule | None = None\n            for dataset_config in dataset_configs:\n                # Skip if we have already benchmarked this model on this dataset and\n                # we are not forcing the benchmark\n                if not benchmark_config.force and model_has_been_benchmarked(\n                    model_id=m_id,\n                    dataset=dataset_config.name,\n                    few_shot=benchmark_config.few_shot,\n                    validation_split=not benchmark_config.evaluate_test_split,\n                    benchmark_results=self.benchmark_results,\n                ):\n                    logger.debug(\n                        f\"Skipping benchmarking {m_id} on {dataset_config.pretty_name},\"\n                        \" as it has already been benchmarked.\"\n                    )\n                    continue\n\n                # We do not re-initialise generative models as their architecture is not\n                # customised to specific datasets\n                if model_config.task in GENERATIVE_PIPELINE_TAGS:\n                    initial_logging(\n                        model_config=model_config,\n                        dataset_config=dataset_config,\n                        benchmark_config=benchmark_config,\n                    )\n                    if loaded_model is None:\n                        logger.info(\"Loading model...\")\n                        try:\n                            loaded_model = load_model(\n                                model_config=model_config,\n                                dataset_config=dataset_config,\n                                benchmark_config=benchmark_config,\n                            )\n                        except InvalidModel as e:\n                            if benchmark_config.raise_errors:\n                                raise e\n                            logger.info(e.message)\n                            break\n                    else:\n                        loaded_model.dataset_config = dataset_config\n\n                # Benchmark a single model on a single dataset\n                benchmark_output_or_err = self._benchmark_single(\n                    model=loaded_model,\n                    model_config=model_config,\n                    dataset_config=dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n\n                if (\n                    isinstance(benchmark_output_or_err, Exception)\n                    and benchmark_config.raise_errors\n                ):\n                    raise benchmark_output_or_err\n\n                elif isinstance(benchmark_output_or_err, InvalidBenchmark):\n                    if benchmark_config.raise_errors:\n                        raise benchmark_output_or_err\n                    logger.info(\n                        f\"{m_id} could not be benchmarked on \"\n                        f\"{dataset_config.pretty_name}. Skipping. The error message \"\n                        f\"raised was {benchmark_output_or_err.message!r}.\"\n                    )\n                    continue\n\n                elif isinstance(benchmark_output_or_err, InvalidModel):\n                    if benchmark_config.raise_errors:\n                        raise benchmark_output_or_err\n                    logger.info(benchmark_output_or_err.message)\n                    break\n\n                else:\n                    record = benchmark_output_or_err\n                    current_benchmark_results.append(record)\n                    if benchmark_config.save_results:\n                        record.append_to_results(results_path=self.results_path)\n\n            if benchmark_config.clear_model_cache:\n                clear_model_cache_fn(cache_dir=benchmark_config.cache_dir)\n\n        # This avoids the following warning at the end of the benchmarking:\n        #   Warning: WARNING: process group has NOT been destroyed before we destruct\n        #   ProcessGroupNCCL. On normal program exit, the application should call\n        #   destroy_process_group to ensure that any pending NCCL operations have\n        #   finished in this process. In rare cases this process can exit before this\n        #   point and block the progress of another member of the process group. This\n        #   constraint has always been present,  but this warning has only been added\n        #   since PyTorch 2.4 (function operator())\n        try:\n            destroy_process_group()\n        except AssertionError:\n            pass\n\n        return current_benchmark_results\n\n    def _get_updated_benchmark_config(\n        self,\n        progress_bar: bool | None = None,\n        save_results: bool | None = None,\n        task: str | list[str] | None | None = None,\n        dataset: str | list[str] | None | None = None,\n        language: str | list[str] | None = None,\n        model_language: str | list[str] | None | None = None,\n        dataset_language: str | list[str] | None | None = None,\n        device: Device | None | None = None,\n        batch_size: int | None = None,\n        raise_errors: bool | None = None,\n        cache_dir: str | None = None,\n        api_key: str | None | None = None,\n        force: bool | None = None,\n        verbose: bool | None = None,\n        trust_remote_code: bool | None = None,\n        use_flash_attention: bool | None | None = None,\n        clear_model_cache: bool | None = None,\n        evaluate_test_split: bool | None = None,\n        few_shot: bool | None = None,\n        num_iterations: int | None = None,\n        api_base: str | None | None = None,\n        api_version: str | None | None = None,\n        debug: bool | None = None,\n        run_with_cli: bool | None = None,\n        only_allow_safetensors: bool | None = None,\n    ) -&gt; \"BenchmarkConfig\":\n        \"\"\"Get an updated benchmark configuration.\n\n        Args:\n            progress_bar:\n                Whether progress bars should be shown. If None, then this value will not\n                be updated.\n            save_results:\n                Whether to save the benchmark results to\n                'euroeval_benchmark_results.jsonl'. If None, then this value will not\n                be updated.\n            task:\n                The tasks benchmark the model(s) on. If None, then this value will not\n                be updated.\n            dataset:\n                The datasets to benchmark on. If None, then this value will not be\n                updated.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. If None, then this value will not be updated.\n            model_language:\n                The language codes of the languages to include for models. If None, then\n                this value will not be updated.\n            dataset_language:\n                The language codes of the languages to include for datasets. If None,\n                then this value will not be updated.\n            device:\n                The device to use for benchmarking. If None, then this value will not be\n                updated.\n            batch_size:\n                The batch size to use. If None, then this value will not be updated.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation. If\n                None, then this value will not be updated.\n            cache_dir:\n                Directory to store cached models. If None, then this value will not be\n                updated.\n            api_key:\n                The API key to use for a given inference server. If None, then this\n                value will not be updated.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. If None, then this value will not be updated.\n            verbose:\n                Whether to output additional output. If None, then this value will not\n                be updated.\n            trust_remote_code:\n                Whether to trust remote code when loading models. If None, then this\n                value will not be updated.\n            use_flash_attention:\n                Whether to use Flash Attention. If None, then this value will not be\n                updated.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model. If None,\n                then this value will not be updated.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. If None, then this\n                value will not be updated.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. If None,\n                then this value will not be updated.\n            num_iterations:\n                The number of times each model should be evaluated. If None, then this\n                value will not be updated.\n            api_base:\n                The base URL for a given inference API. If None, then this value will\n                not be updated.\n            api_version:\n                The version of the API to use. If None, then this value will not be\n                updated.\n            debug:\n                Whether to output debug information. If None, then this value will not\n                be updated.\n            run_with_cli:\n                Whether the benchmarker is being run from the command-line interface.\n                If None, then this value will not be updated.\n            only_allow_safetensors:\n                Whether to only allow models that use the safetensors format. If None,\n                then this value will not be updated.\n\n        Returns:\n            The updated benchmark configuration.\n        \"\"\"\n        benchmark_config_params = deepcopy(self.benchmark_config_default_params)\n\n        if progress_bar is not None:\n            benchmark_config_params.progress_bar = progress_bar\n        if save_results is not None:\n            benchmark_config_params.save_results = save_results\n        if task is not None:\n            benchmark_config_params.task = task\n            benchmark_config_params.dataset = None\n        if dataset is not None:\n            benchmark_config_params.dataset = dataset\n            benchmark_config_params.task = None\n        if language is not None:\n            benchmark_config_params.language = language\n        if model_language is not None:\n            benchmark_config_params.model_language = model_language\n        if dataset_language is not None:\n            benchmark_config_params.dataset_language = dataset_language\n        if device is not None:\n            benchmark_config_params.device = device\n        if batch_size is not None:\n            benchmark_config_params.batch_size = batch_size\n        if raise_errors is not None:\n            benchmark_config_params.raise_errors = raise_errors\n        if cache_dir is not None:\n            benchmark_config_params.cache_dir = cache_dir\n        if api_key is not None:\n            benchmark_config_params.api_key = api_key\n        if force is not None:\n            benchmark_config_params.force = force\n        if verbose is not None:\n            benchmark_config_params.verbose = verbose\n        if trust_remote_code is not None:\n            benchmark_config_params.trust_remote_code = trust_remote_code\n        if use_flash_attention is not None:\n            benchmark_config_params.use_flash_attention = use_flash_attention\n        if clear_model_cache is not None:\n            benchmark_config_params.clear_model_cache = clear_model_cache\n        if evaluate_test_split is not None:\n            benchmark_config_params.evaluate_test_split = evaluate_test_split\n        if few_shot is not None:\n            benchmark_config_params.few_shot = few_shot\n        if num_iterations is not None:\n            benchmark_config_params.num_iterations = num_iterations\n        if api_base is not None:\n            benchmark_config_params.api_base = api_base\n        if api_version is not None:\n            benchmark_config_params.api_version = api_version\n        if debug is not None:\n            benchmark_config_params.debug = debug\n        if run_with_cli is not None:\n            benchmark_config_params.run_with_cli = run_with_cli\n        if only_allow_safetensors is not None:\n            benchmark_config_params.only_allow_safetensors = only_allow_safetensors\n\n        return build_benchmark_config(**benchmark_config_params.model_dump())\n\n    def _prepare_model_ids(self, model_id: list[str] | str) -&gt; list[str]:\n        \"\"\"Prepare the model ID(s) to be benchmarked.\n\n        Args:\n            model_id:\n                The model ID(s) of the models to benchmark.\n\n        Returns:\n            The prepared list of model IDs.\n        \"\"\"\n        model_ids = [model_id] if isinstance(model_id, str) else model_id\n\n        # Reorder the `model_ids` list to include the ones present in the benchmark\n        # results first\n        benchmarked_model_ids = [\n            re.sub(r\"\\(.+\\)\", \"\", record.model).strip()\n            for record in self.benchmark_results\n        ]\n        model_ids_sorted = [m_id for m_id in model_ids if m_id in benchmarked_model_ids]\n        model_ids_sorted += [\n            m_id for m_id in model_ids if m_id not in benchmarked_model_ids\n        ]\n\n        return [m_id.rstrip(\" /\") for m_id in model_ids_sorted]\n\n    def _benchmark_single(\n        self,\n        model: \"BenchmarkModule | None\",\n        model_config: \"ModelConfig\",\n        dataset_config: \"DatasetConfig\",\n        benchmark_config: \"BenchmarkConfig\",\n    ) -&gt; BenchmarkResult | InvalidBenchmark | InvalidModel:\n        \"\"\"Benchmark a single model on a single dataset.\n\n        Args:\n            model:\n                The model to benchmark.\n            model_config:\n                The configuration of the model we are evaluating.\n            dataset_config:\n                The configuration of the dataset we are evaluating on.\n            benchmark_config:\n                The general benchmark configuration.\n\n        Returns:\n            The benchmark result, or an error if the benchmark was unsuccessful.\n        \"\"\"\n        if model is None:\n            initial_logging(\n                model_config=model_config,\n                dataset_config=dataset_config,\n                benchmark_config=benchmark_config,\n            )\n\n        while True:\n            try:\n                # Set random seeds to enforce reproducibility of the randomly\n                # initialised weights\n                rng = enforce_reproducibility()\n\n                if model is None or model_config.model_type != ModelType.GENERATIVE:\n                    logger.info(\"Loading model...\")\n                    model = load_model(\n                        model_config=model_config,\n                        dataset_config=dataset_config,\n                        benchmark_config=benchmark_config,\n                    )\n                assert model is not None\n\n                if dataset_config.task == SPEED:\n                    scores = benchmark_speed(\n                        model=model, benchmark_config=self.benchmark_config\n                    )\n\n                else:\n                    bootstrapped_datasets = load_data(\n                        rng=rng,\n                        dataset_config=dataset_config,\n                        benchmark_config=benchmark_config,\n                    )\n                    prepared_datasets = model.prepare_datasets(\n                        datasets=bootstrapped_datasets, task=dataset_config.task\n                    )\n                    if model_config.model_type == ModelType.GENERATIVE:\n                        scores = generate(\n                            model=model,\n                            datasets=prepared_datasets,\n                            model_config=model_config,\n                            dataset_config=dataset_config,\n                            benchmark_config=self.benchmark_config,\n                        )\n                    else:\n                        scores = finetune(\n                            model=model,\n                            datasets=prepared_datasets,\n                            model_config=model_config,\n                            dataset_config=dataset_config,\n                            benchmark_config=benchmark_config,\n                        )\n\n                results = log_scores(\n                    dataset_name=dataset_config.pretty_name,\n                    metric_configs=dataset_config.task.metrics,\n                    scores=scores,\n                    model_id=model_config.model_id,\n                )\n\n                record = BenchmarkResult(\n                    dataset=dataset_config.name,\n                    task=dataset_config.task.name,\n                    dataset_languages=[\n                        language.code for language in dataset_config.languages\n                    ],\n                    model=model_config.model_id,\n                    results=results,\n                    num_model_parameters=model.num_params,\n                    max_sequence_length=model.model_max_length,\n                    vocabulary_size=model.vocab_size,\n                    merge=model_config.merge,\n                    generative=model_config.model_type == ModelType.GENERATIVE,\n                    generative_type=(\n                        model.generative_type.value\n                        if model.generative_type is not None\n                        else None\n                    ),\n                    few_shot=benchmark_config.few_shot,\n                    validation_split=not benchmark_config.evaluate_test_split,\n                )\n                logger.debug(f\"Results:\\n{results}\")\n                return record\n\n            except (InvalidBenchmark, InvalidModel) as e:\n                # If the model ID is not valid then raise an error\n                model_err_msg = \"does not exist on the Hugging Face Hub\"\n                if benchmark_config.raise_errors and model_err_msg in str(e):\n                    raise e\n\n                # Otherwise, if the error is due to Hugging Face Hub being down, then\n                # wait a bit and try again\n                elif \"The Hugging Face Hub seems to be down.\" in str(e):\n                    wait_time = 30\n                    logger.debug(\n                        \"The Hugging Face Hub seems to be down. Retrying in \"\n                        f\"{wait_time} seconds.\"\n                    )\n                    sleep(wait_time)\n                    continue\n\n                # Otherwise, if the error is due to the MPS fallback not being enabled,\n                # then raise an error asking the user to enable it\n                elif \"PYTORCH_ENABLE_MPS_FALLBACK\" in str(e):\n                    raise RuntimeError(\n                        \"The benchmark failed because the environment variable \"\n                        \"`PYTORCH_ENABLE_MPS_FALLBACK` is not set. Please set this \"\n                        \"environment variable to `1` and try again.\"\n                    )\n\n                elif benchmark_config.raise_errors:\n                    raise e\n                return e\n\n    def __call__(\n        self,\n        model: list[str] | str,\n        task: str | list[str] | None = None,\n        dataset: list[str] | str | None = None,\n        progress_bar: bool | None = None,\n        save_results: bool | None = None,\n        language: str | list[str] | None = None,\n        model_language: str | list[str] | None = None,\n        dataset_language: str | list[str] | None = None,\n        device: Device | None = None,\n        batch_size: int | None = None,\n        raise_errors: bool | None = None,\n        cache_dir: str | None = None,\n        api_key: str | None = None,\n        force: bool | None = None,\n        verbose: bool | None = None,\n        trust_remote_code: bool | None = None,\n        use_flash_attention: bool | None = None,\n        clear_model_cache: bool | None = None,\n        evaluate_test_split: bool | None = None,\n        few_shot: bool | None = None,\n        num_iterations: int | None = None,\n        only_allow_safetensors: bool | None = None,\n    ) -&gt; list[BenchmarkResult]:\n        \"\"\"Benchmarks models on datasets.\n\n        Args:\n            model:\n                The full Hugging Face Hub path(s) to the pretrained transformer model.\n                The specific model version to use can be added after the suffix '@':\n                \"model@v1.0.0\". It can be a branch name, a tag name, or a commit id,\n                and defaults to the latest version if not specified.\n            task:\n                The tasks benchmark the model(s) on. Mutually exclusive with `dataset`.\n                If both `task` and `dataset` are None then all datasets will be\n                benchmarked. Defaults to None.\n            dataset:\n                The datasets to benchmark on. Mutually exclusive with `task`. If both\n                `task` and `dataset` are None then all datasets will be benchmarked.\n                Defaults to None.\n            progress_bar:\n                Whether progress bars should be shown. Defaults to the value specified\n                when initialising the benchmarker.\n            save_results:\n                Whether to save the benchmark results to\n                'euroeval_benchmark_results.jsonl'. Defaults to the value specified\n                when initialising the benchmarker.\n            language:\n                The language codes of the languages to include, both for models and\n                datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n                to 'all' if all languages (also non-Scandinavian) should be considered.\n                Defaults to the value specified when initialising the benchmarker.\n            model_language:\n                The language codes of the languages to include for models. If specified\n                then this overrides the `language` parameter for model languages.\n                Defaults to the value specified when initialising the benchmarker.\n            dataset_language:\n                The language codes of the languages to include for datasets. If\n                specified then this overrides the `language` parameter for dataset\n                languages. Defaults to the value specified when initialising the\n                benchmarker.\n            device:\n                The device to use for benchmarking. Defaults to the value specified when\n                initialising the benchmarker.\n            batch_size:\n                The batch size to use. Defaults to the value specified when initialising\n                the benchmarker.\n            raise_errors:\n                Whether to raise errors instead of skipping the model evaluation.\n            cache_dir:\n                Directory to store cached models. Defaults to the value specified when\n                initialising the benchmarker.\n            api_key:\n                The API key to use for a given inference server. Defaults to the value\n                specified when initialising the benchmarker.\n            force:\n                Whether to force evaluations of models, even if they have been\n                benchmarked already. Defaults to the value specified when initialising\n                the benchmarker.\n            verbose:\n                Whether to output additional output. Defaults to the value specified\n                when initialising the benchmarker.\n            trust_remote_code:\n                Whether to trust remote code when loading models. Defaults to the value\n                specified when initialising the benchmarker.\n            use_flash_attention:\n                Whether to use Flash Attention. Defaults to the value specified when\n                initialising the benchmarker.\n            clear_model_cache:\n                Whether to clear the model cache after benchmarking each model. Defaults\n                to the value specified when initialising the benchmarker.\n            evaluate_test_split:\n                Whether to evaluate the test split of the datasets. Defaults to the\n                value specified when initialising the benchmarker.\n            few_shot:\n                Whether to only evaluate the model using few-shot evaluation. Only\n                relevant if the model is generative. Defaults to the value specified\n                when initialising the benchmarker.\n            num_iterations:\n                The number of times each model should be evaluated. This is only meant\n                to be used for power users, and scores will not be allowed on the\n                leaderboards if this is changed. Defaults to the value specified when\n                initialising the benchmarker.\n            only_allow_safetensors:\n                Whether to only allow models that use the safetensors format. Defaults\n                to the value specified when initialising the benchmarker.\n\n        Returns:\n            A list of benchmark results.\n\n        Raises:\n            ValueError:\n                If both `task` and `dataset` are specified.\n        \"\"\"\n        logger.warning(\n            \"Calling the `Benchmarker` class directly is deprecated. Please use the \"\n            \"`benchmark` function instead. This will be removed in a future version.\"\n        )\n        return self.benchmark(\n            model=model,\n            task=task,\n            dataset=dataset,\n            progress_bar=progress_bar,\n            save_results=save_results,\n            language=language,\n            model_language=model_language,\n            dataset_language=dataset_language,\n            device=device,\n            batch_size=batch_size,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n            api_key=api_key,\n            force=force,\n            verbose=verbose,\n            trust_remote_code=trust_remote_code,\n            use_flash_attention=use_flash_attention,\n            clear_model_cache=clear_model_cache,\n            evaluate_test_split=evaluate_test_split,\n            few_shot=few_shot,\n            num_iterations=num_iterations,\n            only_allow_safetensors=only_allow_safetensors,\n        )\n\n\ndef model_has_been_benchmarked(docs\n    model_id: str,\n    dataset: str,\n    few_shot: bool,\n    validation_split: bool,\n    benchmark_results: list[BenchmarkResult],\n) -&gt; bool:\n    \"\"\"Checks whether a model has already been benchmarked on a dataset.\n\n    Args:\n        model_id:\n            The model ID.\n        dataset:\n            The dataset.\n        few_shot:\n            Whether the model was evaluated using few-shot evaluation.\n        validation_split:\n            Whether the model was evaluated on the validation split.\n        benchmark_results:\n            The benchmark results.\n\n    Returns:\n        Whether the model has already been evaluated on the dataset.\n    \"\"\"\n    for record in benchmark_results:\n        same_evaluation = record.model == model_id and record.dataset == dataset\n        same_validation_split_setting = record.validation_split == validation_split\n        same_few_shot_setting = record.few_shot == few_shot or not record.generative\n        if same_evaluation and same_validation_split_setting and same_few_shot_setting:\n            return True\n    return False\n\ndocs\ndef adjust_logging_level(verbose: bool, ignore_testing: bool = False) -&gt; int:\n    \"\"\"Adjust the logging level based on verbosity.\n\n    Args:\n        verbose:\n            Whether to output additional output.\n        ignore_testing:\n            Whether to ignore the testing flag.\n\n    Returns:\n        The logging level that was set.\n    \"\"\"\n    if hasattr(sys, \"_called_from_test\") and not ignore_testing:\n        logging_level = logging.CRITICAL\n    elif verbose:\n        logging_level = logging.DEBUG\n    else:\n        logging_level = logging.INFO\n    logger.setLevel(logging_level)\n    return logging_level\n\n\ndef clear_model_cache_fn(cache_dir: str) -&gt; None:docs\n    \"\"\"Clear the model cache.\n\n    Note that this will not remove the stored completions.\n\n    Args:\n        cache_dir:\n            The path to the cache directory.\n    \"\"\"\n    model_cache_path = Path(cache_dir) / \"model_cache\"\n    model_cache_path.mkdir(parents=True, exist_ok=True)\n    for model_dir in model_cache_path.iterdir():\n        if model_dir.is_dir():\n            for sub_model_dir in model_dir.iterdir():\n                if sub_model_dir.is_dir():\n                    rmtree(sub_model_dir)\n\ndocs\ndef prepare_dataset_configs(dataset_names: list[str]) -&gt; list[\"DatasetConfig\"]:\n    \"\"\"Prepare the dataset configuration(s) to be benchmarked.\n\n    Args:\n        dataset_names:\n            The dataset names to benchmark.\n\n    Returns:\n        The prepared list of model IDs.\n    \"\"\"\n    return [\n        cfg for cfg in get_all_dataset_configs().values() if cfg.name in dataset_names\n    ]\n\n\ndef initial_logging(docs\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; None:\n    \"\"\"Initial logging at the start of the benchmarking process.\n\n    Args:\n        model_config:\n            The configuration of the model we are evaluating.\n        dataset_config:\n            The configuration of the dataset we are evaluating on.\n        benchmark_config:\n            The general benchmark configuration.\n    \"\"\"\n    split_type = \"validation\" if not benchmark_config.evaluate_test_split else \"test\"\n    if model_config.task in GENERATIVE_PIPELINE_TAGS:\n        if benchmark_config.few_shot:\n            eval_type = \"Few-shot benchmarking\"\n        else:\n            eval_type = \"Zero-shot benchmarking\"\n    else:\n        eval_type = \"Benchmarking\"\n    logger.info(\n        f\"{eval_type} {model_config.model_id} on the {split_type} split of \"\n        f\"{dataset_config.pretty_name}\"\n    )\n\n    if dataset_config.unofficial:\n        logger.info(\n            f\"Note that the {dataset_config.name!r} dataset is unofficial, \"\n            \"meaning that the resulting evaluation will not be included in the \"\n            \"official leaderboard.\"\n        )\n    if benchmark_config.debug:\n        logger.info(\n            \"Running in debug mode. This will output additional information, as \"\n            \"well as store the model outputs in the current directory after each \"\n            \"batch. For this reason, evaluation will be slower.\"\n        )\n</code></pre>"},{"location":"api/euroeval/benchmark_config_factory/","title":"euroeval.benchmark_config_factory","text":"euroeval.benchmark_config_factory<p> source module euroeval.benchmark_config_factory </p> <p>Factory class for creating dataset configurations.</p> <p> Functions </p> <ul> <li> <p>build_benchmark_config \u2014 Create a benchmark configuration.</p> </li> <li> <p>get_correct_language_codes \u2014 Get correct language code(s).</p> </li> <li> <p>prepare_languages \u2014 Prepare language(s) for benchmarking.</p> </li> <li> <p>prepare_tasks_and_datasets \u2014 Prepare task(s) and dataset(s) for benchmarking.</p> </li> <li> <p>prepare_device \u2014 Prepare device for benchmarking.</p> </li> </ul> <p> source build_benchmark_config(progress_bar: bool, save_results: bool, task: str | list[str] | None, dataset: str | list[str] | None, language: str | list[str], model_language: str | list[str] | None, dataset_language: str | list[str] | None, device: Device | None, batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, trust_remote_code: bool, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool, only_allow_safetensors: bool, first_time: bool = False) \u2192 BenchmarkConfig </p> <p>Create a benchmark configuration.</p> <p> Parameters </p> <ul> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar when running the benchmark.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to a file.</p> </li> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> parameter.</p> </li> <li> <p>language :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> <li> <p>model_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>dataset_language :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for datasets. If None then the <code>language</code> parameter will be used.</p> </li> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use for running the models.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors when running the benchmark.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>The directory to use for caching the models.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference server.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output when running the benchmark. This is automatically set if <code>debug</code> is True.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when running the benchmark.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention for the models. If None then it will be used if it is available.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache before running the benchmark.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to use the test split for the datasets.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to use few-shot learning for the models.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use for a given inference API.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014</p> <p>Whether to only allow evaluations of models stored as safetensors.</p> </li> <li> <p>first_time :  bool \u2014</p> <p>Whether this is the first time the benchmark configuration is being created. Defaults to False.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkConfig \u2014 The benchmark configuration.</p> </li> </ul> <p> source get_correct_language_codes(language_codes: str | list[str]) \u2192 list[str] </p> <p>Get correct language code(s).</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] \u2014</p> <p>The language codes of the languages to include, both for models and datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this to 'all' if all languages (also non-Scandinavian) should be considered.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[str] \u2014 The correct language codes.</p> </li> </ul> <p> source prepare_languages(language_codes: str | list[str] | None, default_language_codes: list[str]) \u2192 list[Language] </p> <p>Prepare language(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>language_codes :  str | list[str] | None \u2014</p> <p>The language codes of the languages to include for models or datasets. If specified then this overrides the <code>language</code> parameter for model or dataset languages.</p> </li> <li> <p>default_language_codes :  list[str] \u2014</p> <p>The default language codes of the languages to include.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[Language] \u2014 The prepared model or dataset languages.</p> </li> </ul> <p> source prepare_tasks_and_datasets(task: str | list[str] | None, dataset_languages: list[Language], dataset: str | list[str] | None) \u2192 tuple[list[Task], list[str]] </p> <p>Prepare task(s) and dataset(s) for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>task :  str | list[str] | None \u2014</p> <p>The tasks to include for dataset. If None then datasets will not be filtered based on their task.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>dataset :  str | list[str] | None \u2014</p> <p>The datasets to include for task. If None then all datasets will be included, limited by the <code>task</code> and <code>dataset_languages</code> parameters.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[list[Task], list[str]] \u2014 The prepared tasks and datasets.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark \u2014</p> <p>If the task or dataset is not found in the benchmark tasks or datasets.</p> </li> </ul> <p> source prepare_device(device: Device | None) \u2192 torch.device </p> <p>Prepare device for benchmarking.</p> <p> Parameters </p> <ul> <li> <p>device :  Device | None \u2014</p> <p>The device to use for running the models. If None then the device will be set automatically.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>torch.device \u2014 The prepared device.</p> </li> </ul>"},{"location":"src/euroeval/benchmark_config_factory/","title":"euroeval.benchmark_config_factory","text":"euroeval.benchmark_config_factory<p> docs module euroeval.benchmark_config_factory </p> <pre><code>\"\"\"Factory class for creating dataset configurations.\"\"\"\n\nimport importlib.util\nimport logging\nimport sys\nimport typing as t\n\nimport torch\n\nfrom .data_models import BenchmarkConfig\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device\nfrom .exceptions import InvalidBenchmark\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\nfrom .utils import log_once\n\nif t.TYPE_CHECKING:\n    from .data_models import Language, Task\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef build_benchmark_config(docs\n    progress_bar: bool,\n    save_results: bool,\n    task: str | list[str] | None,\n    dataset: str | list[str] | None,\n    language: str | list[str],\n    model_language: str | list[str] | None,\n    dataset_language: str | list[str] | None,\n    device: Device | None,\n    batch_size: int,\n    raise_errors: bool,\n    cache_dir: str,\n    api_key: str | None,\n    force: bool,\n    verbose: bool,\n    trust_remote_code: bool,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    evaluate_test_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    api_base: str | None,\n    api_version: str | None,\n    debug: bool,\n    run_with_cli: bool,\n    only_allow_safetensors: bool,\n    first_time: bool = False,\n) -&gt; BenchmarkConfig:\n    \"\"\"Create a benchmark configuration.\n\n    Args:\n        progress_bar:\n            Whether to show a progress bar when running the benchmark.\n        save_results:\n            Whether to save the benchmark results to a file.\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` parameter.\n        language:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n        model_language:\n            The language codes of the languages to include for models. If None then\n            the `language` parameter will be used.\n        dataset_language:\n            The language codes of the languages to include for datasets. If None then\n            the `language` parameter will be used.\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n        batch_size:\n            The batch size to use for running the models.\n        raise_errors:\n            Whether to raise errors when running the benchmark.\n        cache_dir:\n            The directory to use for caching the models.\n        api_key:\n            The API key to use for a given inference server.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        verbose:\n            Whether to print verbose output when running the benchmark. This is\n            automatically set if `debug` is True.\n        trust_remote_code:\n            Whether to trust remote code when running the benchmark.\n        use_flash_attention:\n            Whether to use Flash Attention for the models. If None then it will be used\n            if it is available.\n        clear_model_cache:\n            Whether to clear the model cache before running the benchmark.\n        evaluate_test_split:\n            Whether to use the test split for the datasets.\n        few_shot:\n            Whether to use few-shot learning for the models.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        api_base:\n            The base URL for a given inference API. Only relevant if `model` refers to a\n            model on an inference API.\n        api_version:\n            The version of the API to use for a given inference API.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n        only_allow_safetensors:\n            Whether to only allow evaluations of models stored as safetensors.\n        first_time:\n            Whether this is the first time the benchmark configuration is being created.\n            Defaults to False.\n\n    Returns:\n        The benchmark configuration.\n    \"\"\"\n    language_codes = get_correct_language_codes(language_codes=language)\n    model_languages = prepare_languages(\n        language_codes=model_language, default_language_codes=language_codes\n    )\n    dataset_languages = prepare_languages(\n        language_codes=dataset_language, default_language_codes=language_codes\n    )\n\n    tasks, datasets = prepare_tasks_and_datasets(\n        task=task, dataset=dataset, dataset_languages=dataset_languages\n    )\n\n    torch_device = prepare_device(device=device)\n\n    if use_flash_attention is None:\n        if torch_device.type != \"cuda\":\n            use_flash_attention = False\n        elif (\n            importlib.util.find_spec(\"flash_attn\") is None\n            and importlib.util.find_spec(\"vllm_flash_attn\") is None\n        ):\n            use_flash_attention = False\n            if first_time and torch_device.type == \"cuda\":\n                message = (\n                    \"Flash attention has not been installed, so this will not be used. \"\n                    \"To install it, run `pip install -U wheel &amp;&amp; \"\n                    \"FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn \"\n                    \"--no-build-isolation`. Alternatively, you can disable this \"\n                    \"message by setting \"\n                )\n                if run_with_cli:\n                    message += \"the flag `--no-use-flash-attention`.\"\n                else:\n                    message += (\n                        \"the argument `use_flash_attention=False` in the `Benchmarker`.\"\n                    )\n                log_once(message=message, level=logging.INFO)\n\n    # Set variable with number of iterations\n    if hasattr(sys, \"_called_from_test\"):\n        num_iterations = 1\n\n    return BenchmarkConfig(\n        model_languages=model_languages,\n        dataset_languages=dataset_languages,\n        tasks=tasks,\n        datasets=datasets,\n        batch_size=batch_size,\n        raise_errors=raise_errors,\n        cache_dir=cache_dir,\n        api_key=api_key,\n        force=force,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        verbose=verbose or debug,\n        device=torch_device,\n        trust_remote_code=trust_remote_code,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        evaluate_test_split=evaluate_test_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        api_base=api_base,\n        api_version=api_version,\n        debug=debug,\n        run_with_cli=run_with_cli,\n        only_allow_safetensors=only_allow_safetensors,\n    )\n\ndocs\ndef get_correct_language_codes(language_codes: str | list[str]) -&gt; list[str]:\n    \"\"\"Get correct language code(s).\n\n    Args:\n        language_codes:\n            The language codes of the languages to include, both for models and\n            datasets. Here 'no' means both Bokm\u00e5l (nb) and Nynorsk (nn). Set this\n            to 'all' if all languages (also non-Scandinavian) should be considered.\n\n    Returns:\n        The correct language codes.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages`\n    if \"all\" in language_codes:\n        languages = list(language_mapping.keys())\n    elif isinstance(language_codes, str):\n        languages = [language_codes]\n    else:\n        languages = language_codes\n\n    # If `languages` contains 'no' then also include 'nb' and 'nn'. Conversely, if\n    # either 'nb' or 'nn' are specified then also include 'no'.\n    if \"no\" in languages:\n        languages = list(set(languages) | {\"nb\", \"nn\"})\n    elif \"nb\" in languages or \"nn\" in languages:\n        languages = list(set(languages) | {\"no\"})\n\n    return languages\n\n\ndef prepare_languages(docs\n    language_codes: str | list[str] | None, default_language_codes: list[str]\n) -&gt; list[\"Language\"]:\n    \"\"\"Prepare language(s) for benchmarking.\n\n    Args:\n        language_codes:\n            The language codes of the languages to include for models or datasets.\n            If specified then this overrides the `language` parameter for model or\n            dataset languages.\n        default_language_codes:\n            The default language codes of the languages to include.\n\n    Returns:\n        The prepared model or dataset languages.\n    \"\"\"\n    # Create a dictionary that maps languages to their associated language objects\n    language_mapping = get_all_languages()\n\n    # Create the list `languages_str` of language codes to use for models or datasets\n    languages_str: list[str]\n    if language_codes is None:\n        languages_str = default_language_codes\n    elif isinstance(language_codes, str):\n        languages_str = [language_codes]\n    else:\n        languages_str = language_codes\n\n    # Convert the model languages to language objects\n    if \"all\" in languages_str:\n        prepared_languages = list(language_mapping.values())\n    else:\n        prepared_languages = [language_mapping[language] for language in languages_str]\n\n    return prepared_languages\n\n\ndef prepare_tasks_and_datasets(docs\n    task: str | list[str] | None,\n    dataset_languages: list[\"Language\"],\n    dataset: str | list[str] | None,\n) -&gt; tuple[list[\"Task\"], list[str]]:\n    \"\"\"Prepare task(s) and dataset(s) for benchmarking.\n\n    Args:\n        task:\n            The tasks to include for dataset. If None then datasets will not be\n            filtered based on their task.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        dataset:\n            The datasets to include for task. If None then all datasets will be\n            included, limited by the `task` and `dataset_languages` parameters.\n\n    Returns:\n        The prepared tasks and datasets.\n\n    Raises:\n        InvalidBenchmark:\n            If the task or dataset is not found in the benchmark tasks or datasets.\n    \"\"\"\n    # Create a dictionary that maps benchmark tasks to their associated benchmark\n    # task objects, and a dictionary that maps dataset names to their associated\n    # dataset configuration objects\n    task_mapping = get_all_tasks()\n    all_dataset_configs = get_all_dataset_configs()\n\n    # Create the list of dataset tasks\n    try:\n        if task is None:\n            tasks = list(task_mapping.values())\n        elif isinstance(task, str):\n            tasks = [task_mapping[task]]\n        else:\n            tasks = [task_mapping[t] for t in task]\n    except KeyError as e:\n        raise InvalidBenchmark(f\"Task {e} not found in the benchmark tasks.\") from e\n\n    all_official_datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if not dataset_config.unofficial\n    ]\n    if dataset is None:\n        dataset = all_official_datasets\n    elif isinstance(dataset, str):\n        dataset = [dataset]\n\n    all_datasets = list(all_dataset_configs.keys())\n    invalid_datasets = set(dataset) - set(all_datasets)\n    if invalid_datasets:\n        raise InvalidBenchmark(\n            f\"Dataset(s) {', '.join(invalid_datasets)} not found in the benchmark \"\n            \"datasets.\"\n        )\n\n    datasets = [\n        dataset_name\n        for dataset_name, dataset_config in all_dataset_configs.items()\n        if dataset_name in dataset\n        and dataset_config.task in tasks\n        and set(dataset_config.languages).intersection(dataset_languages)\n    ]\n\n    return tasks, datasets\n\n\ndef prepare_device(device: Device | None) -&gt; torch.device:docs\n    \"\"\"Prepare device for benchmarking.\n\n    Args:\n        device:\n            The device to use for running the models. If None then the device will be\n            set automatically.\n\n    Returns:\n        The prepared device.\n    \"\"\"\n    device_mapping = {\n        Device.CPU: torch.device(\"cpu\"),\n        Device.CUDA: torch.device(\"cuda\"),\n        Device.MPS: torch.device(\"mps\"),\n    }\n    if isinstance(device, Device):\n        return device_mapping[device]\n\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/euroeval/callbacks/","title":"euroeval.callbacks","text":"euroeval.callbacks<p> source module euroeval.callbacks </p> <p>Callbacks for the Hugging Face Trainer.</p> <p> Classes </p> <ul> <li> <p>NeverLeaveProgressCallback \u2014 Progress callback which never leaves the progress bar.</p> </li> </ul> <p> source class NeverLeaveProgressCallback(max_str_len: int = 100) </p> <p><p>Bases : ProgressCallback</p></p> <p>Progress callback which never leaves the progress bar.</p> <p>Initialise the callback.</p> <p> Methods </p> <ul> <li> <p>on_train_begin \u2014 Callback actions when training begins.</p> </li> <li> <p>on_step_end \u2014 Callback actions when a training step ends.</p> </li> <li> <p>on_prediction_step \u2014 Callback actions when a prediction step ends.</p> </li> </ul> <p> source method NeverLeaveProgressCallback.on_train_begin(args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs: str) \u2192 None </p> <p>Callback actions when training begins.</p> <p> source method NeverLeaveProgressCallback.on_step_end(args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs: str) \u2192 None </p> <p>Callback actions when a training step ends.</p> <p> source method NeverLeaveProgressCallback.on_prediction_step(args: TrainingArguments, state: TrainerState, control: TrainerControl, eval_dataloader: DataLoader | None = None, **kwargs: str) \u2192 None </p> <p>Callback actions when a prediction step ends.</p>"},{"location":"src/euroeval/callbacks/","title":"euroeval.callbacks","text":"euroeval.callbacks<p> docs module euroeval.callbacks </p> <pre><code>\"\"\"Callbacks for the Hugging Face Trainer.\"\"\"\n\nimport sys\nfrom collections.abc import Sized\n\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import TrainerControl, TrainerState, TrainingArguments\nfrom transformers.trainer_callback import ProgressCallback\n\n\nclass NeverLeaveProgressCallback(ProgressCallback):docs\n    \"\"\"Progress callback which never leaves the progress bar.\"\"\"\n\n    def __init__(self, max_str_len: int = 100) -&gt; None:\n        \"\"\"Initialise the callback.\"\"\"\n        super().__init__(max_str_len=max_str_len)\n        self.training_bar: tqdm | None = None\n        self.prediction_bar: tqdm | None = None\n\n    def on_train_begin(docs\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs: str,\n    ) -&gt; None:\n        \"\"\"Callback actions when training begins.\"\"\"\n        if state.is_local_process_zero:\n            desc = \"Finetuning model\"\n            self.training_bar = tqdm(\n                total=None,\n                leave=False,\n                desc=desc,\n                disable=hasattr(sys, \"_called_from_test\"),\n            )\n        self.current_step = 0\n\n    def on_step_end(docs\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs: str,\n    ) -&gt; None:\n        \"\"\"Callback actions when a training step ends.\"\"\"\n        if state.is_local_process_zero and self.training_bar is not None:\n            self.training_bar.update(state.global_step - self.current_step)\n            self.current_step = state.global_step\n\n    def on_prediction_step(docs\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        eval_dataloader: DataLoader | None = None,\n        **kwargs: str,\n    ) -&gt; None:\n        \"\"\"Callback actions when a prediction step ends.\"\"\"\n        if eval_dataloader is None:\n            return\n        correct_dtype = isinstance(eval_dataloader.dataset, Sized)\n        if state.is_local_process_zero and correct_dtype:\n            if self.prediction_bar is None:\n                desc = \"Evaluating model\"\n                self.prediction_bar = tqdm(\n                    total=len(eval_dataloader),\n                    leave=False,\n                    desc=desc,\n                    disable=hasattr(sys, \"_called_from_test\"),\n                )\n            self.prediction_bar.update(1)\n</code></pre>"},{"location":"api/euroeval/cli/","title":"euroeval.cli","text":"euroeval.cli<p> source module euroeval.cli </p> <p>Command-line interface for benchmarking.</p> <p> Functions </p> <ul> <li> <p>benchmark \u2014 Benchmark pretrained language models on language tasks.</p> </li> </ul> <p> source benchmark(model: tuple[str], dataset: tuple[str], language: tuple[str], model_language: tuple[str], dataset_language: tuple[str], raise_errors: bool, task: tuple[str], batch_size: str, progress_bar: bool, save_results: bool, cache_dir: str, api_key: str | None, force: bool, verbose: bool, device: str | None, trust_remote_code: bool, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, only_allow_safetensors: bool) \u2192 None </p> <p>Benchmark pretrained language models on language tasks.</p>"},{"location":"src/euroeval/cli/","title":"euroeval.cli","text":"euroeval.cli<p> docs module euroeval.cli </p> <pre><code>\"\"\"Command-line interface for benchmarking.\"\"\"\n\nimport click\n\nfrom .benchmarker import Benchmarker\nfrom .dataset_configs import get_all_dataset_configs\nfrom .enums import Device\nfrom .languages import get_all_languages\nfrom .tasks import get_all_tasks\n\n\n@click.command()\n@click.option(\n    \"--model\",\n    \"-m\",\n    required=True,\n    multiple=True,\n    help=\"The ID of the model to benchmark.\",\n)\n@click.option(\n    \"--task\",\n    \"-t\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_tasks().keys())),\n    help=\"The dataset tasks to benchmark the model(s) on.\",\n)\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=[\"all\"],\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The languages to benchmark, both for models and datasets. If \"all\" then all\n    models will be benchmarked on all datasets.\"\"\",\n)\n@click.option(\n    \"--model-language\",\n    \"-ml\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The model languages to benchmark. If not specified then this will use the\n    `language` value.\"\"\",\n)\n@click.option(\n    \"--dataset-language\",\n    \"-dl\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    metavar=\"ISO 639-1 LANGUAGE CODE\",\n    type=click.Choice([\"all\"] + list(get_all_languages().keys())),\n    help=\"\"\"The dataset languages to benchmark. If \"all\" then the models will be\n    benchmarked on all datasets. If not specified then this will use the `language`\n    value.\"\"\",\n)\n@click.option(\n    \"--dataset\",\n    default=None,\n    show_default=True,\n    multiple=True,\n    type=click.Choice(list(get_all_dataset_configs().keys())),\n    help=\"\"\"The name of the benchmark dataset. We recommend to use the `task` and\n    `language` options instead of this option.\"\"\",\n)\n@click.option(\n    \"--batch-size\",\n    default=\"32\",\n    type=click.Choice([\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]),\n    help=\"The batch size to use.\",\n)\n@click.option(\n    \"--progress-bar/--no-progress-bar\",\n    default=True,\n    show_default=True,\n    help=\"Whether to show a progress bar.\",\n)\n@click.option(\n    \"--raise-errors/--no-raise-errors\",\n    default=False,\n    show_default=True,\n    help=\"Whether to raise errors instead of skipping the evaluation.\",\n)\n@click.option(\n    \"--verbose/--no-verbose\",\n    \"-v/-nv\",\n    default=False,\n    show_default=True,\n    help=\"Whether extra input should be outputted during benchmarking\",\n)\n@click.option(\n    \"--save-results/--no-save-results\",\n    \"-s/-ns\",\n    default=True,\n    show_default=True,\n    help=\"Whether results should not be stored to disk.\",\n)\n@click.option(\n    \"--cache-dir\",\n    \"-c\",\n    default=\".euroeval_cache\",\n    show_default=True,\n    help=\"The directory where models are datasets are cached.\",\n)\n@click.option(\n    \"--api-key\",\n    type=str,\n    default=None,\n    show_default=True,\n    help=\"\"\"The API key to use for a given inference API. If you are benchmarking an \"\n    \"OpenAI model then this would be the OpenAI API key, if you are benchmarking a \"\n    \"model on the Hugging Face inference API then this would be the Hugging Face API \"\n    \"key, and so on.\"\"\",\n)\n@click.option(\n    \"--force/--no-force\",\n    \"-f\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to force evaluation of models which have already been evaluated,\n    with scores lying in the 'euroeval_benchmark_results.jsonl' file.\"\"\",\n)\n@click.option(\n    \"--device\",\n    default=None,\n    show_default=True,\n    type=click.Choice([device.lower() for device in Device.__members__]),\n    help=\"\"\"The device to use for evaluation. If not specified then the device will be\n    set automatically.\"\"\",\n)\n@click.option(\n    \"--trust-remote-code/--no-trust-remote-code\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to trust remote code. Only set this flag if you trust the supplier\n    of the model.\"\"\",\n)\n@click.option(\n    \"--use-flash-attention/--no-use-flash-attention\",\n    default=None,\n    show_default=True,\n    help=\"\"\"Whether to use Flash Attention. If not specified then the model will use\n    Flash Attention for generative models if a CUDA GPU is available and `flash-attn`\n    or `vllm-flash-attn` are installed.\"\"\",\n)\n@click.option(\n    \"--clear-model-cache/--no-clear-model-cache\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to clear the model cache after benchmarking each model. Note that\n    this will only remove the model files, and not the cached model outputs (which\n    don't take up a lot of disk space). This is useful when benchmarking many models,\n    to avoid running out of disk space.\"\"\",\n)\n@click.option(\n    \"--evaluate-test-split/--evaluate-val-split\",\n    default=False,\n    show_default=True,\n    help=\"\"\"Whether to only evaluate on the test split. Only use this for your final\n    evaluation, as the test split should not be used for development.\"\"\",\n)\n@click.option(\n    \"--few-shot/--zero-shot\",\n    default=True,\n    show_default=True,\n    help=\"Whether to only evaluate the model using few-shot evaluation. Only relevant \"\n    \"if the model is generative.\",\n)\n@click.option(\n    \"--num-iterations\",\n    default=10,\n    show_default=True,\n    help=\"\"\"The number of times each model should be evaluated. This is only meant to\n    be used for power users, and scores will not be allowed on the leaderboards if this\n    is changed.\"\"\",\n)\n@click.option(\n    \"--api-base\",\n    default=None,\n    show_default=True,\n    help=\"The base URL for a given inference API. Only relevant if `model` refers to a \"\n    \"model on an inference API.\",\n)\n@click.option(\n    \"--api-version\",\n    default=None,\n    show_default=True,\n    help=\"The version of the API to use. Only relevant if `model` refers to a model on \"\n    \"an inference API.\",\n)\n@click.option(\n    \"--debug/--no-debug\",\n    default=False,\n    show_default=True,\n    help=\"Whether to run the benchmark in debug mode. This prints out extra \"\n    \"information and stores all outputs to the current working directory. Only \"\n    \"relevant if the model is generative.\",\n)\n@click.option(\n    \"--only-allow-safetensors\",\n    is_flag=True,\n    help=\"Only allow loading models that have safetensors weights available\",\n    default=False,\n)\ndef benchmark(docs\n    model: tuple[str],\n    dataset: tuple[str],\n    language: tuple[str],\n    model_language: tuple[str],\n    dataset_language: tuple[str],\n    raise_errors: bool,\n    task: tuple[str],\n    batch_size: str,\n    progress_bar: bool,\n    save_results: bool,\n    cache_dir: str,\n    api_key: str | None,\n    force: bool,\n    verbose: bool,\n    device: str | None,\n    trust_remote_code: bool,\n    use_flash_attention: bool | None,\n    clear_model_cache: bool,\n    evaluate_test_split: bool,\n    few_shot: bool,\n    num_iterations: int,\n    api_base: str | None,\n    api_version: str | None,\n    debug: bool,\n    only_allow_safetensors: bool,\n) -&gt; None:\n    \"\"\"Benchmark pretrained language models on language tasks.\"\"\"\n    models = list(model)\n    datasets = None if len(dataset) == 0 else list(dataset)\n    languages: list[str] = list(language)\n    model_languages = None if len(model_language) == 0 else list(model_language)\n    dataset_languages = None if len(dataset_language) == 0 else list(dataset_language)\n    tasks = None if len(task) == 0 else list(task)\n    batch_size_int = int(batch_size)\n    device = Device[device.upper()] if device is not None else None\n\n    benchmarker = Benchmarker(\n        language=languages,\n        model_language=model_languages,\n        dataset_language=dataset_languages,\n        task=tasks,\n        dataset=datasets,\n        batch_size=batch_size_int,\n        progress_bar=progress_bar,\n        save_results=save_results,\n        raise_errors=raise_errors,\n        verbose=verbose,\n        api_key=api_key,\n        force=force,\n        cache_dir=cache_dir,\n        device=device,\n        trust_remote_code=trust_remote_code,\n        use_flash_attention=use_flash_attention,\n        clear_model_cache=clear_model_cache,\n        evaluate_test_split=evaluate_test_split,\n        few_shot=few_shot,\n        num_iterations=num_iterations,\n        api_base=api_base,\n        api_version=api_version,\n        debug=debug,\n        run_with_cli=True,\n        only_allow_safetensors=only_allow_safetensors,\n    )\n\n    # Perform the benchmark evaluation\n    benchmarker.benchmark(model=models)\n\n\nif __name__ == \"__main__\":\n    benchmark()\n</code></pre>"},{"location":"api/euroeval/constants/","title":"euroeval.constants","text":"euroeval.constants<p> source module euroeval.constants </p> <p>Constants used throughout the project.</p>"},{"location":"src/euroeval/constants/","title":"euroeval.constants","text":"euroeval.constants<p> docs module euroeval.constants </p> <pre><code>\"\"\"Constants used throughout the project.\"\"\"\n\nfrom .enums import TaskGroup\nfrom .tasks import NER\n\n# This is used as input to generative models; it cannot be a special token\nDUMMY_FILL_VALUE = 100\n\n\n# We need to raise the amount of tokens generated for reasoning models, to give them\n# time to think\nREASONING_MAX_TOKENS = 8_192\n\n\n# The Hugging Face Hub pipeline tags used to classify models as generative\nGENERATIVE_PIPELINE_TAGS = [\"text-generation\", \"text2text-generation\"]\n\n\n# Used to disallow non-generative models to be evaluated on these task groups\nGENERATIVE_DATASET_TASK_GROUPS = [TaskGroup.TEXT_TO_TEXT]\n\n\n# Local models are required to have these files in their directory\nLOCAL_MODELS_REQUIRED_FILES = [\"config.json\"]\n\n\n# Tasks where we use structured generation for generative models\nTASKS_USING_JSON = [NER]\n\n\n# Tasks where we use log probabilities for generative models, rather than the raw\n# completion\nTASK_GROUPS_USING_LOGPROBS = [\n    TaskGroup.SEQUENCE_CLASSIFICATION,\n    TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION,\n]\n\n\n# The number of top log probabilities to return for generative models. For several APIs\n# this is the maximum number of log probabilities that can be returned\nMAX_LOGPROBS = 10\n\n\n# We make sure to remove these metric attributed after each iteration, to avoid memory\n# leaks\nMETRIC_ATTRIBUTES_TAKING_UP_MEMORY = [\"cached_bertscorer\"]\n\n\n# Hugging Face Hub tags used to classify models as merge models\nMERGE_TAGS = [\"merge\", \"mergekit\"]\n</code></pre>"},{"location":"api/euroeval/dataset_configs/","title":"euroeval.dataset_configs","text":"euroeval.dataset_configs<p> source module euroeval.dataset_configs </p> <p>All dataset configurations used in EuroEval.</p> <p> Functions </p> <ul> <li> <p>get_all_dataset_configs \u2014 Get a mapping of all the dataset configurations.</p> </li> <li> <p>get_dataset_config \u2014 Get the dataset configuration for a dataset.</p> </li> </ul> <p> source get_all_dataset_configs() \u2192 dict[str, DatasetConfig] </p> <p>Get a mapping of all the dataset configurations.</p> <p> Returns </p> <ul> <li> <p>dict[str, DatasetConfig] \u2014 A mapping between names of datasets and their configurations.</p> </li> </ul> <p> source get_dataset_config(dataset_name: str) \u2192 DatasetConfig </p> <p>Get the dataset configuration for a dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>DatasetConfig \u2014 The dataset configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the dataset is not found.</p> </li> </ul>"},{"location":"src/euroeval/dataset_configs/","title":"euroeval.dataset_configs","text":"euroeval.dataset_configs<p> docs module euroeval.dataset_configs </p> <pre><code>\"\"\"All dataset configurations used in EuroEval.\"\"\"\n\nfrom .data_models import DatasetConfig\nfrom .languages import DA, DE, EN, FO, FR, IS, NB, NL, NN, NO, SV, get_all_languages\nfrom .tasks import COMMON_SENSE, KNOW, LA, MCRC, NER, RC, SENT, SPEED, SUMM\n\n\ndef get_all_dataset_configs() -&gt; dict[str, DatasetConfig]:docs\n    \"\"\"Get a mapping of all the dataset configurations.\n\n    Returns:\n        A mapping between names of datasets and their configurations.\n    \"\"\"\n    dataset_configs = [\n        cfg for cfg in globals().values() if isinstance(cfg, DatasetConfig)\n    ]\n    assert len(dataset_configs) == len({cfg.name for cfg in dataset_configs}), (\n        \"There are duplicate dataset configurations. Please ensure that each dataset \"\n        \"has a unique name.\"\n    )\n    return {cfg.name: cfg for cfg in dataset_configs}\n\n\ndef get_dataset_config(dataset_name: str) -&gt; DatasetConfig:docs\n    \"\"\"Get the dataset configuration for a dataset.\n\n    Args:\n        dataset_name:\n            The name of the dataset.\n\n    Returns:\n        The dataset configuration.\n\n    Raises:\n        ValueError:\n            If the dataset is not found.\n    \"\"\"\n    # Get mapping of all dataset configs\n    dataset_configs = get_all_dataset_configs()\n\n    # If there are no matches for the dataset name, raise an error\n    if dataset_name not in dataset_configs:\n        raise ValueError(f\"No dataset config found for dataset {dataset_name}.\")\n\n    # Otherwise, return the dataset configuration\n    return dataset_configs[dataset_name]\n\n\n### SENTIMENT DATASETS ###\n\nSWEREC_CONFIG = DatasetConfig(\n    name=\"swerec\",\n    pretty_name=\"the truncated version of the Swedish sentiment classification \"\n    \"dataset SweReC\",\n    huggingface_id=\"EuroEval/swerec-mini\",\n    task=SENT,\n    languages=[SV],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"F\u00f6ljande \u00e4r recensioner och deras sentiment, som kan vara \"\n    \"'positiv', 'neutral' eller 'negativ'.\",\n    prompt_template=\"Recension: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Recension: {text}\\n\\nKlassificera sentimentet i recensionen. \"\n    \"Svara med 'positiv', 'neutral' eller 'negativ', och inget annat.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nANGRY_TWEETS_CONFIG = DatasetConfig(\n    name=\"angry-tweets\",\n    pretty_name=\"the truncated version of the Danish sentiment classification \"\n    \"dataset AngryTweets\",\n    huggingface_id=\"EuroEval/angry-tweets-mini\",\n    task=SENT,\n    languages=[DA],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"F\u00f8lgende er tweets og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'neutral' eller 'negativ'.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassificer sentimentet i tweetet. Svar kun \"\n    \"med 'positiv', 'neutral' eller 'negativ', og intet andet.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nNOREC_CONFIG = DatasetConfig(\n    name=\"norec\",\n    pretty_name=\"the truncated version of the Norwegian sentiment classification \"\n    \"dataset NoReC\",\n    huggingface_id=\"EuroEval/norec-mini\",\n    task=SENT,\n    languages=[NB, NN, NO],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"F\u00f8lgende er anmeldelser og deres sentiment, som kan v\u00e6re 'positiv', \"\n    \"'n\u00f8ytral' eller 'negativ'.\",\n    prompt_template=\"Anmeldelse: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"n\u00f8ytral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Anmeldelse: {text}\\n\\nKlassifiser sentimentet i anmeldelsen. \"\n    \"Svar med 'positiv', 'n\u00f8ytral' eller 'negativ', og ikke noe annet.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nHOTTER_AND_COLDER_SENTIMENT_CONFIG = DatasetConfig(\n    name=\"hotter-and-colder-sentiment\",\n    pretty_name=\"the sentiment classification part of the Icelandic dataset Hotter \"\n    \"and Colder\",\n    huggingface_id=\"EuroEval/hotter-and-colder-sentiment\",\n    task=SENT,\n    languages=[IS],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"Eftirfarandi eru yfirfer\u00f0ir \u00e1samt lyndisgildi \u00feeirra, sem getur \"\n    \"veri\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt'.\",\n    prompt_template=\"Yfirfer\u00f0: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"j\u00e1kv\u00e6tt\", neutral=\"hlutlaust\", negative=\"neikv\u00e6tt\"\n    ),\n    instruction_prompt=\"Texti: {text}\\n\\nFlokka\u00f0u tilfinninguna \u00ed textanum. \"\n    \"Svara\u00f0u me\u00f0 'j\u00e1kv\u00e6tt', 'hlutlaust' e\u00f0a 'neikv\u00e6tt', og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSB10K_CONFIG = DatasetConfig(\n    name=\"sb10k\",\n    pretty_name=\"the truncated version of the German sentiment classification \"\n    \"dataset SB10k\",\n    huggingface_id=\"EuroEval/sb10k-mini\",\n    task=SENT,\n    languages=[DE],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"Im Folgenden sind Tweets und ihre Stimmung aufgef\u00fchrt, die \"\n    \"'positiv', 'neutral' oder 'negativ' sein kann.\",\n    prompt_template=\"Tweet: {text}\\nStimmungslage: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positiv\", neutral=\"neutral\", negative=\"negativ\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nKlassifizieren Sie die Stimmung im Tweet. \"\n    \"Antworten Sie mit 'positiv', 'neutral' oder 'negativ', und nichts anderes.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nDUTCH_SOCIAL_CONFIG = DatasetConfig(\n    name=\"dutch-social\",\n    pretty_name=\"the truncated version of the Dutch sentiment classification \"\n    \"dataset Dutch Social\",\n    huggingface_id=\"EuroEval/dutch-social-mini\",\n    task=SENT,\n    languages=[NL],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"Hieronder staan tweets en hun sentiment, dat 'positief', \"\n    \"'neutraal' of 'negatief' kan zijn.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positief\", neutral=\"neutraal\", negative=\"negatief\"\n    ),\n    instruction_prompt=\"Tweet: {text}\\n\\nClassificeer het sentiment in de tweet. \"\n    \"Antwoord met 'positief', 'neutraal' of 'negatief', en niets anders.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nDBRD_CONFIG = DatasetConfig(\n    name=\"dbrd\",\n    pretty_name=\"the truncated version of the Dutch sentiment classification \"\n    \"dataset DBRD\",\n    huggingface_id=\"EuroEval/dbrd-mini\",\n    task=SENT,\n    languages=[NL],\n    labels=[\"negative\", \"positive\"],\n    prompt_prefix=\"Hieronder staan tweets en hun sentiment, dat 'positief' of \"\n    \"'negatief' kan zijn.\",\n    prompt_template=\"Tweet: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(positive=\"positief\", negative=\"negatief\"),\n    instruction_prompt=\"Tweet: {text}\\n\\nClassificeer het sentiment in de tweet. \"\n    \"Antwoord met 'positief' of 'negatief', en niets anders.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nSST5_CONFIG = DatasetConfig(\n    name=\"sst5\",\n    pretty_name=\"the truncated version of the English sentiment classification \"\n    \"dataset SST5\",\n    huggingface_id=\"EuroEval/sst5-mini\",\n    task=SENT,\n    languages=[EN],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"The following are texts and their sentiment, which can be \"\n    \"'positive', 'neutral' or 'negative'.\",\n    prompt_template=\"Text: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positive\", neutral=\"neutral\", negative=\"negative\"\n    ),\n    instruction_prompt=\"Text: {text}\\n\\nClassify the sentiment in the text. Answer \"\n    \"with 'positive', 'neutral' or 'negative', and nothing else.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nFOSENT_CONFIG = DatasetConfig(\n    name=\"fosent\",\n    pretty_name=\"the Faroese sentiment classification dataset FoSent\",\n    huggingface_id=\"EuroEval/fosent\",\n    task=SENT,\n    languages=[FO],\n    labels=[\"negative\", \"neutral\", \"positive\"],\n    prompt_prefix=\"Her eru nakrir tekstir flokka\u00f0ir eftir lyndi, sum kann vera \"\n    \"'positivt', 'neutralt' ella 'negativt'.\",\n    prompt_template=\"Text: {text}\\nLyndi: {label}\",\n    prompt_label_mapping=dict(\n        positive=\"positivt\", neutral=\"neutralt\", negative=\"negativt\"\n    ),\n    instruction_prompt=\"Tekstur: {text}\\n\\nFlokka lyndi\u00f0 \u00ed tekstinum. Svara vi\u00f0 \"\n    \"'positivt', 'neutralt' ella 'negativt', og einki anna\u00f0.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nALLOCINE_CONFIG = DatasetConfig(\n    name=\"allocine\",\n    pretty_name=\"the truncated version of the French sentiment classification \"\n    \"dataset Allocine\",\n    huggingface_id=\"EuroEval/allocine-mini\",\n    task=SENT,\n    languages=[FR],\n    labels=[\"negative\", \"positive\"],\n    prompt_prefix=\"Voici des textes et leur sentiment, qui peut \u00eatre 'positif' ou \"\n    \"'n\u00e9gatif'.\",\n    prompt_template=\"Texte: {text}\\nSentiment: {label}\",\n    prompt_label_mapping=dict(positive=\"positif\", negative=\"n\u00e9gatif\"),\n    instruction_prompt=\"Texte : {text}\\nClassez le sentiment dans le texte. R\u00e9pondez \"\n    \"par \u2018positif' ou \u2018n\u00e9gatif', et rien d'autre.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\n\n### NAMED ENTITY RECOGNITION DATASETS ###\n\nSUC3_CONFIG = DatasetConfig(\n    name=\"suc3\",\n    pretty_name=\"the truncated version of the Swedish named entity recognition \"\n    \"dataset SUC 3.0\",\n    huggingface_id=\"EuroEval/suc3-mini\",\n    task=NER,\n    languages=[SV],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och JSON-ordb\u00f6cker med de namngivna enheter \"\n    \"som f\u00f6rekommer i den givna meningen.\",\n    prompt_template=\"Mening: {text}\\nNamngivna entiteter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"plats\",\n        \"i-loc\": \"plats\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Mening: {text}\\n\\nIdentifiera de namngivna enheterna i \"\n    \"meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', \"\n    \"'plats', 'organisation' och 'diverse'. V\u00e4rdena ska vara listor \u00f6ver de namngivna \"\n    \"enheter av den typen, precis som de f\u00f6rekommer i meningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANSK_CONFIG = DatasetConfig(\n    name=\"dansk\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DANSK\",\n    huggingface_id=\"EuroEval/dansk-mini\",\n    task=NER,\n    languages=[DA],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NB_CONFIG = DatasetConfig(\n    name=\"norne-nb\",\n    pretty_name=\"the truncated version of the Bokm\u00e5l part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"EuroEval/norne-nb-mini\",\n    task=NER,\n    languages=[NB, NO],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nNORNE_NN_CONFIG = DatasetConfig(\n    name=\"norne-nn\",\n    pretty_name=\"the truncated version of the Nynorsk part of the Norwegian named \"\n    \"entity recognition dataset NorNE\",\n    huggingface_id=\"EuroEval/norne-nn-mini\",\n    task=NER,\n    languages=[NN],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"F\u00f8lgende er fraser og JSON-ordb\u00f8ker med de navngitte enhetene \"\n    \"som forekommer i den gitte frasen.\",\n    prompt_template=\"Frase: {text}\\nNavngitte enheter: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisasjon\",\n        \"i-org\": \"organisasjon\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"Frase: {text}\\n\\nIdentifiser de navngitte enhetene i frasen. \"\n    \"Du b\u00f8r outputte dette som en JSON-ordbok med n\u00f8klene 'person', 'sted', \"\n    \"'organisasjon' og 'diverse'. Verdiene skal v\u00e6re lister over de navngitte enhetene \"\n    \"av den typen, akkurat som de vises i frasen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nMIM_GOLD_NER_CONFIG = DatasetConfig(\n    name=\"mim-gold-ner\",\n    pretty_name=\"the truncated version of the Icelandic named entity recognition \"\n    \"dataset MIM-GOLD-NER\",\n    huggingface_id=\"EuroEval/mim-gold-ner-mini\",\n    task=NER,\n    languages=[IS],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Eftirfarandi eru setningar \u00e1samt JSON lyklum me\u00f0 nefndum einingum \"\n    \"sem koma fyrir \u00ed setningunum.\",\n    prompt_template=\"Setning: {text}\\nNefndar einingar: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"einstaklingur\",\n        \"i-per\": \"einstaklingur\",\n        \"b-loc\": \"sta\u00f0setning\",\n        \"i-loc\": \"sta\u00f0setning\",\n        \"b-org\": \"stofnun\",\n        \"i-org\": \"stofnun\",\n        \"b-misc\": \"\u00fdmislegt\",\n        \"i-misc\": \"\u00fdmislegt\",\n    },\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 nefndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'einstaklingur', \"\n    \"'sta\u00f0setning', 'stofnun' og '\u00fdmislegt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nefndu \"\n    \"einingarnar af \u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nFONE_CONFIG = DatasetConfig(\n    name=\"fone\",\n    pretty_name=\"the truncated version of the Faroese named entity recognition \"\n    \"dataset FoNE\",\n    huggingface_id=\"EuroEval/fone-mini\",\n    task=NER,\n    languages=[FO],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nGERMEVAL_CONFIG = DatasetConfig(\n    name=\"germeval\",\n    pretty_name=\"the truncated version of the German named entity recognition \"\n    \"dataset GermEval\",\n    huggingface_id=\"EuroEval/germeval-mini\",\n    task=NER,\n    languages=[DE],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Es folgen S\u00e4tze und JSON-W\u00f6rterb\u00fccher mit den benannten \"\n    \"Entit\u00e4ten, die in der angegebenen Phrase vorkommen.\",\n    prompt_template=\"Satz: {text}\\nBenannte Entit\u00e4ten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"ort\",\n        \"i-loc\": \"ort\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"verschiedenes\",\n        \"i-misc\": \"verschiedenes\",\n    },\n    instruction_prompt=\"Satz: {text}\\n\\nIdentifizieren Sie die benannten Entit\u00e4ten im \"\n    \"Satz. Sie sollten dies als JSON-W\u00f6rterbuch mit den Schl\u00fcsseln 'person', 'ort', \"\n    \"'organisation' und 'verschiedenes' ausgeben. Die Werte sollten Listen der \"\n    \"benannten Entit\u00e4ten dieses Typs sein, genau wie sie im Satz erscheinen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_NL_CONFIG = DatasetConfig(\n    name=\"conll-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the named entity \"\n    \"recognition dataset CoNLL 2002\",\n    huggingface_id=\"EuroEval/conll-nl-mini\",\n    task=NER,\n    languages=[NL],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Hieronder staan zinnen en JSON woordenboeken met de genoemde \"\n    \"entiteiten die voorkomen in de gegeven zin.\",\n    prompt_template=\"Zin: {text}\\nGenoemde entiteiten: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"persoon\",\n        \"i-per\": \"persoon\",\n        \"b-loc\": \"locatie\",\n        \"i-loc\": \"locatie\",\n        \"b-org\": \"organisatie\",\n        \"i-org\": \"organisatie\",\n        \"b-misc\": \"diversen\",\n        \"i-misc\": \"diversen\",\n    },\n    instruction_prompt=\"Zin: {text}\\n\\nIdentificeer de genoemde entiteiten in de zin. \"\n    \"Je moet dit uitvoeren als een JSON-woordenboek met de sleutels 'persoon', \"\n    \"'locatie', 'organisatie' en 'diversen'. De waarden moeten lijsten zijn van de \"\n    \"genoemde entiteiten van dat type, precies zoals ze voorkomen in de zin.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nCONLL_EN_CONFIG = DatasetConfig(\n    name=\"conll-en\",\n    pretty_name=\"the truncated version of the English named entity recognition \"\n    \"dataset CoNLL 2003\",\n    huggingface_id=\"EuroEval/conll-en-mini\",\n    task=NER,\n    languages=[EN],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Below are sentences and JSON dictionaries with the named entities \"\n    \"that occur in the given sentence.\",\n    prompt_template=\"Sentence: {text}\\nNamed entities: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"location\",\n        \"i-loc\": \"location\",\n        \"b-org\": \"organization\",\n        \"i-org\": \"organization\",\n        \"b-misc\": \"miscellaneous\",\n        \"i-misc\": \"miscellaneous\",\n    },\n    instruction_prompt=\"Sentence: {text}\\n\\nIdentify the named entities in the \"\n    \"sentence. You should output this as a JSON dictionary with the keys being \"\n    \"'person', 'location', 'organization' and 'miscellaneous'. The values should be \"\n    \"lists of the named entities of that type, exactly as they appear in the sentence.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nELTEC_CONFIG = DatasetConfig(\n    name=\"eltec\",\n    pretty_name=\"the truncated version of the French named entity recognition \"\n    \"dataset ELTeC\",\n    huggingface_id=\"EuroEval/eltec-mini\",\n    task=NER,\n    languages=[FR],\n    labels=[\n        \"o\",\n        \"b-per\",\n        \"i-per\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Vous trouverez ci-dessous des phrases et des dictionnaires JSON \"\n    \"avec les entit\u00e9s nomm\u00e9es qui apparaissent dans la phrase donn\u00e9e.\",\n    prompt_template=\"Sentence: {text}\\nEntit\u00e9s nomm\u00e9es: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"personne\",\n        \"i-per\": \"personne\",\n        \"b-loc\": \"lieu\",\n        \"i-loc\": \"lieu\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"divers\",\n        \"i-misc\": \"divers\",\n    },\n    instruction_prompt=\"Sentence: {text}\\n\\nIdentifiez les entit\u00e9s nomm\u00e9es dans la \"\n    \"phrase. Vous devez produire ceci sous forme de dictionnaire JSON avec les cl\u00e9s \"\n    \"'personne', 'lieu', 'organisation', et 'divers'. Les valeurs doivent \u00eatre des \"\n    \"listes des entit\u00e9s nomm\u00e9es de ce type, exactement comme elles apparaissent dans \"\n    \"la phrase.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n)\n\nDANE_CONFIG = DatasetConfig(\n    name=\"dane\",\n    pretty_name=\"the truncated version of the Danish named entity recognition \"\n    \"dataset DaNE\",\n    huggingface_id=\"EuroEval/dane-mini\",\n    task=NER,\n    languages=[DA],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og JSON-ordb\u00f8ger med de navngivne enheder, \"\n    \"som forekommer i den givne s\u00e6tning.\",\n    prompt_template=\"S\u00e6tning: {text}\\nNavngivne enheder: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"person\",\n        \"i-per\": \"person\",\n        \"b-loc\": \"sted\",\n        \"i-loc\": \"sted\",\n        \"b-org\": \"organisation\",\n        \"i-org\": \"organisation\",\n        \"b-misc\": \"diverse\",\n        \"i-misc\": \"diverse\",\n    },\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nIdentific\u00e9r de navngivne enheder i \"\n    \"s\u00e6tningen. Du skal outputte dette som en JSON-ordbog med n\u00f8glerne 'person', \"\n    \"'sted', 'organisation' og 'diverse'. V\u00e6rdierne skal v\u00e6re lister over de navngivne \"\n    \"enheder af den type, pr\u00e6cis som de forekommer i s\u00e6tningen.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\nWIKIANN_FO_CONFIG = DatasetConfig(\n    name=\"wikiann-fo\",\n    pretty_name=\"the truncated version of the Faroese part of the named entity \"\n    \"recognition dataset WikiANN\",\n    huggingface_id=\"EuroEval/wikiann-fo-mini\",\n    task=NER,\n    languages=[FO],\n    labels=[\n        \"o\",\n        \"b-loc\",\n        \"i-loc\",\n        \"b-org\",\n        \"i-org\",\n        \"b-per\",\n        \"i-per\",\n        \"b-misc\",\n        \"i-misc\",\n    ],\n    prompt_prefix=\"Her eru nakrir setningar og nakrar JSON or\u00f0ab\u00f8kur vi\u00f0 nevndar \"\n    \"eindir, sum eru \u00ed setningunum.\",\n    prompt_template=\"Setningur: {text}\\nNevndar eindir: {label}\",\n    prompt_label_mapping={\n        \"b-per\": \"pers\u00f3nur\",\n        \"i-per\": \"pers\u00f3nur\",\n        \"b-loc\": \"sta\u00f0ur\",\n        \"i-loc\": \"sta\u00f0ur\",\n        \"b-org\": \"felagsskapur\",\n        \"i-org\": \"felagsskapur\",\n        \"b-misc\": \"ymiskt\",\n        \"i-misc\": \"ymiskt\",\n    },\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 nevndu einingarnar \u00ed setningunni. \"\n    \"\u00de\u00fa \u00e6ttir a\u00f0 skila \u00feessu sem JSON or\u00f0ab\u00f3k me\u00f0 lyklunum 'pers\u00f3nur', 'sta\u00f0ur', \"\n    \"'felagsskapur' og 'ymiskt'. Gildin \u00e6ttu a\u00f0 vera listi yfir nevndu einingarnar af \"\n    \"\u00feeirri ger\u00f0, n\u00e1kv\u00e6mlega eins og \u00fe\u00e6r koma fram \u00ed setningunni.\",\n    num_few_shot_examples=8,\n    max_generated_tokens=128,\n    unofficial=True,\n)\n\n\n### LINGUISTIC ACCEPTABILITY DATASETS ###\n\nSCALA_SV_CONFIG = DatasetConfig(\n    name=\"scala-sv\",\n    pretty_name=\"The Swedish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-sv\",\n    task=LA,\n    languages=[SV],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"F\u00f6ljande \u00e4r meningar och huruvida de \u00e4r grammatiskt korrekta.\",\n    prompt_template=\"Mening: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"Mening: {text}\\n\\nBest\u00e4m om meningen \u00e4r grammatiskt korrekt \"\n    \"eller inte. Svara med 'ja' om meningen \u00e4r korrekt och 'nej' om den inte \u00e4r, \"\n    \"och inget annat.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_DA_CONFIG = DatasetConfig(\n    name=\"scala-da\",\n    pretty_name=\"the Danish part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-da\",\n    task=LA,\n    languages=[DA],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"F\u00f8lgende er s\u00e6tninger og om de er grammatisk korrekte.\",\n    prompt_template=\"S\u00e6tning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nej\"),\n    instruction_prompt=\"S\u00e6tning: {text}\\n\\nBestem om s\u00e6tningen er grammatisk korrekt \"\n    \"eller ej. Svar med 'ja', hvis s\u00e6tningen er korrekt, og 'nej', hvis den ikke er, \"\n    \"og intet andet.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NB_CONFIG = DatasetConfig(\n    name=\"scala-nb\",\n    pretty_name=\"the Bokm\u00e5l part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-nb\",\n    task=LA,\n    languages=[NB, NO],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er, \"\n    \"og ikke noe annet.\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NN_CONFIG = DatasetConfig(\n    name=\"scala-nn\",\n    pretty_name=\"the Nynorsk part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-nn\",\n    task=LA,\n    languages=[NN],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"F\u00f8lgende er setninger og hvorvidt de er grammatisk korrekte.\",\n    prompt_template=\"Setning: {text}\\nGrammatisk korrekt: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nBestem om setningen er grammatisk korrekt \"\n    \"eller ikke. Svar med 'ja' hvis setningen er korrekt og 'nei' hvis den ikke er, \"\n    \"og ikke noe annet.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_IS_CONFIG = DatasetConfig(\n    name=\"scala-is\",\n    pretty_name=\"the Icelandic part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-is\",\n    task=LA,\n    languages=[IS],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki, og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_FO_CONFIG = DatasetConfig(\n    name=\"scala-fo\",\n    pretty_name=\"the Faroese part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-fo\",\n    task=LA,\n    languages=[FO],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Hetta eru nakrir setningar og um teir eru m\u00e1ll\u00e6ruliga r\u00e6ttir.\",\n    prompt_template=\"Setningur: {text}\\nM\u00e1ll\u00e6ruliga r\u00e6ttur: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nei\"),\n    instruction_prompt=\"Setningur: {text}\\n\\nGreini\u00f0 hvort setningurin er m\u00e1ll\u00e6ruliga \"\n    \"r\u00e6ttur ella ikki. Svari\u00f0 skal vera 'ja' um setningurin er r\u00e6ttur og 'nei' um \"\n    \"hann ikki er, og einki anna\u00f0.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_DE_CONFIG = DatasetConfig(\n    name=\"scala-de\",\n    pretty_name=\"the German part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-de\",\n    task=LA,\n    languages=[DE],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Die folgenden S\u00e4tze und ob sie grammatikalisch korrekt sind.\",\n    prompt_template=\"Satz: {text}\\nGrammatikalisch richtig: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nein\"),\n    instruction_prompt=\"Satz: {text}\\n\\nBestimmen Sie, ob der Satz grammatikalisch \"\n    \"korrekt ist oder nicht. Antworten Sie mit 'ja', wenn der Satz korrekt ist und \"\n    \"'nein', wenn er es nicht ist, und nichts anderes.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_NL_CONFIG = DatasetConfig(\n    name=\"scala-nl\",\n    pretty_name=\"the Dutch part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-nl\",\n    task=LA,\n    languages=[NL],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is, \"\n    \"en niets anders.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_EN_CONFIG = DatasetConfig(\n    name=\"scala-en\",\n    pretty_name=\"the English part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-en\",\n    task=LA,\n    languages=[EN],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"The following are sentences and whether they are grammatically \"\n    \"correct.\",\n    prompt_template=\"Sentence: {text}\\nGrammatically correct: {label}\",\n    prompt_label_mapping=dict(correct=\"yes\", incorrect=\"no\"),\n    instruction_prompt=\"Sentence: {text}\\n\\nDetermine whether the sentence is \"\n    \"grammatically correct or not. Reply with 'yes' if the sentence is correct and \"\n    \"'no' if it is not, and nothing else.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nSCALA_FR_CONFIG = DatasetConfig(\n    name=\"scala-fr\",\n    pretty_name=\"the French part of the linguistic acceptability dataset ScaLA\",\n    huggingface_id=\"EuroEval/scala-fr\",\n    task=LA,\n    languages=[FR],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Les phrases suivantes indiquent si elles sont grammaticalement \"\n    \"correctes.\",\n    prompt_template=\"Phrase : {text}\\nCorrect du point de vue grammatical: {label}\",\n    prompt_label_mapping=dict(correct=\"oui\", incorrect=\"non\"),\n    instruction_prompt=\"Phrase: {text}\\n\\nD\u00e9terminez si la phrase est grammaticalement \"\n    \"correcte ou non. R\u00e9pondez par 'oui' si la phrase est correcte et par 'non' si \"\n    \"elle ne l'est pas, et rien d'autre.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n)\n\nDUTCH_COLA_CONFIG = DatasetConfig(\n    name=\"dutch-cola\",\n    pretty_name=\"the truncated version of the Dutch linguistic acceptability dataset \"\n    \"Dutch CoLA\",\n    huggingface_id=\"EuroEval/dutch-cola\",\n    task=LA,\n    languages=[NL],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is, \"\n    \"en niets anders.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nDUTCH_COLA_FULL_CONFIG = DatasetConfig(\n    name=\"dutch-cola-full\",\n    pretty_name=\"the Dutch linguistic acceptability dataset Dutch CoLA\",\n    huggingface_id=\"EuroEval/dutch-cola-full\",\n    task=LA,\n    languages=[NL],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Hieronder staan zinnen en of ze grammaticaal correct ('ja') of \"\n    \"incorrect ('nee') zijn.\",\n    prompt_template=\"Zin: {text}\\nGrammaticaal correct: {label}\",\n    prompt_label_mapping=dict(correct=\"ja\", incorrect=\"nee\"),\n    instruction_prompt=\"Zin: {text}\\n\\nBepaal of de zin grammaticaal correct is of \"\n    \"niet. Antwoord met 'ja' als de zin correct is en 'nee' als dat niet het geval is, \"\n    \"en niets anders.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=3,\n    unofficial=True,\n)\n\nICE_EC_CONFIG = DatasetConfig(\n    name=\"ice-ec\",\n    pretty_name=\"the truncated version of the Icelandic Error Corpus\",\n    huggingface_id=\"EuroEval/ice-ec\",\n    task=LA,\n    languages=[IS],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki, og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nICE_EC_FULL_CONFIG = DatasetConfig(\n    name=\"ice-ec-full\",\n    pretty_name=\"the Icelandic Error Corpus\",\n    huggingface_id=\"EuroEval/ice-ec-full\",\n    task=LA,\n    languages=[IS],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki, og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nICE_LINGUISTIC_CONFIG = DatasetConfig(\n    name=\"ice-linguistic\",\n    pretty_name=\"the Icelandic linguistic acceptability dataset IceLinguistic\",\n    huggingface_id=\"EuroEval/ice-linguistic\",\n    task=LA,\n    languages=[IS],\n    labels=[\"incorrect\", \"correct\"],\n    prompt_prefix=\"Eftirfarandi eru setningar og hvort \u00fe\u00e6r eru m\u00e1lfr\u00e6\u00f0ilega r\u00e9ttar.\",\n    prompt_template=\"Setning: {text}\\nM\u00e1lfr\u00e6\u00f0ilega r\u00e9tt: {label}\",\n    prompt_label_mapping=dict(correct=\"j\u00e1\", incorrect=\"nei\"),\n    instruction_prompt=\"Setning: {text}\\n\\nGreini\u00f0 hvort setningin er m\u00e1lfr\u00e6\u00f0ilega \"\n    \"r\u00e9tt e\u00f0a ekki. Svari\u00f0 skal vera 'j\u00e1' ef setningin er r\u00e9tt og 'nei' ef h\u00fan er \"\n    \"ekki, og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=12,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n\n### READING COMPREHENSION DATASETS ###\n\nSCANDIQA_DA_CONFIG = DatasetConfig(\n    name=\"scandiqa-da\",\n    pretty_name=\"the Danish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"EuroEval/scandiqa-da-mini\",\n    task=RC,\n    languages=[DA],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende sp\u00f8rgsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rgsm\u00e5l: {question}\\nSvar med maks. 3 ord: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rgsm\u00e5l om teksten ovenfor \"\n    \"med maks. 3 ord.\\n\\nSp\u00f8rgsm\u00e5l: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNORQUAD_CONFIG = DatasetConfig(\n    name=\"norquad\",\n    pretty_name=\"the truncated version of the Norwegian question answering \"\n    \"dataset NorQuAD\",\n    huggingface_id=\"EuroEval/norquad-mini\",\n    task=RC,\n    languages=[NB, NN, NO],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n)\n\nNORGLM_MULTI_QA = DatasetConfig(\n    name=\"norglm-multi-qa\",\n    pretty_name=\"the question answering part of the Norwegian NorGLM multi-task human \"\n    \"annotated dataset NO-Multi-QA-Sum\",\n    huggingface_id=\"EuroEval/norglm-multi-qa\",\n    task=RC,\n    languages=[NB, NN, NO],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"Tekst: {text}\\nSp\u00f8rsm\u00e5l: {question}\\nSvar p\u00e5 maks 3 ord: {label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l om teksten ovenfor \"\n    \"med maks 3 ord.\\n\\nSp\u00f8rsm\u00e5l: {question}\",\n    num_few_shot_examples=2,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\nSCANDIQA_SV_CONFIG = DatasetConfig(\n    name=\"scandiqa-sv\",\n    pretty_name=\"the Swedish part of the truncated version of the question answering \"\n    \"dataset ScandiQA\",\n    huggingface_id=\"EuroEval/scandiqa-sv-mini\",\n    task=RC,\n    languages=[SV],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande fr\u00e5gor och svar.\",\n    prompt_template=\"Text: {text}\\nFr\u00e5ga: {question}\\nSvar p\u00e5 max 3 ord: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga om texten ovan med \"\n    \"h\u00f6gst 3 ord.\\n\\nFr\u00e5ga: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nNQII_CONFIG = DatasetConfig(\n    name=\"nqii\",\n    pretty_name=\"the truncated version of the Icelandic reading comprehension dataset \"\n    \"Natural Questions in Icelandic\",\n    huggingface_id=\"EuroEval/nqii-mini\",\n    task=RC,\n    languages=[IS],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nFOQA_CONFIG = DatasetConfig(\n    name=\"foqa\",\n    pretty_name=\"the Faroese reading comprehension dataset FoQA\",\n    huggingface_id=\"EuroEval/foqa\",\n    task=RC,\n    languages=[FO],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Hetta eru tekstir saman vi\u00f0 spurningum og svar.\",\n    prompt_template=\"Tekstur: {text}\\nSpurningur: {question}\\nSvara vi\u00f0 \u00ed mesta lagi \"\n    \"trimum or\u00f0um: {label}\",\n    instruction_prompt=\"Tekstur: {text}\\n\\nSvara hesum spurninginum um tekstin \"\n    \"uppiyvir vi\u00f0 \u00ed mesta lagi trimum or\u00f0um.\\n\\nSpurningur: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nGERMANQUAD_CONFIG = DatasetConfig(\n    name=\"germanquad\",\n    pretty_name=\"the truncated version of the German reading comprehension dataset \"\n    \"GermanQuAD\",\n    huggingface_id=\"EuroEval/germanquad-mini\",\n    task=RC,\n    languages=[DE],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Im Folgenden finden Sie Texte mit den dazugeh\u00f6rigen Fragen und \"\n    \"Antworten.\",\n    prompt_template=\"Text: {text}\\nFragen: {question}\\nFragen Antwort in maximal 3 \"\n    \"W\u00f6rtern: {label}\",\n    instruction_prompt=\"Text: {text}\\n\\nBeantworten Sie die folgende Frage zum obigen \"\n    \"Text in h\u00f6chstens 3 W\u00f6rtern.\\n\\nFrage: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_CONFIG = DatasetConfig(\n    name=\"squad\",\n    pretty_name=\"the truncated version of the English question answering dataset SQuAD\",\n    huggingface_id=\"EuroEval/squad-mini\",\n    task=RC,\n    languages=[EN],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"The following are texts with accompanying questions and answers.\",\n    prompt_template=\"Text: {text}\\nQuestion: {question}\\nAnswer in max 3 words: \"\n    \"{label}\",\n    instruction_prompt=\"Text: {text}\\n\\nAnswer the following question about the \"\n    \"above text in at most 3 words.\\n\\nQuestion: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nSQUAD_NL_CONFIG = DatasetConfig(\n    name=\"squad-nl\",\n    pretty_name=\"the truncated version of the Dutch reading comprehension dataset \"\n    \"SQuAD-nl, translated from the English SQuAD dataset\",\n    huggingface_id=\"EuroEval/squad-nl-v2-mini\",\n    task=RC,\n    languages=[NL],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Hieronder volgen teksten met bijbehorende vragen en antwoorden.\",\n    prompt_template=\"Tekst: {text}\\nVraag: {question}\\nAntwoord in max 3 woorden: \"\n    \"{label}\",\n    instruction_prompt=\"Tekst: {text}\\n\\nBeantwoord de volgende vraag over de \"\n    \"bovenstaande tekst in maximaal 3 woorden.\\n\\nVraag: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\nICELANDIC_QA_CONFIG = DatasetConfig(\n    name=\"icelandic-qa\",\n    pretty_name=\"the Icelandic reading comprehension dataset about Icelandic culture \"\n    \"and history\",\n    huggingface_id=\"EuroEval/icelandic-qa\",\n    task=RC,\n    languages=[IS],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi spurningum og sv\u00f6rum.\",\n    prompt_template=\"Texti: {text}\\nSpurning: {question}\\nSvara\u00f0u me\u00f0 a\u00f0 h\u00e1marki 3 \"\n    \"or\u00f0um: {label}\",\n    instruction_prompt=\"Texti: {text}\\n\\nSvara\u00f0u eftirfarandi spurningu um textann a\u00f0 \"\n    \"h\u00e1marki \u00ed 3 or\u00f0um.\\n\\nSpurning: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n    unofficial=True,\n)\n\nFQUAD_CONFIG = DatasetConfig(\n    name=\"fquad\",\n    pretty_name=\"the truncated version of the French reading comprehension dataset \"\n    \"FQuAD\",\n    huggingface_id=\"EuroEval/fquad-mini\",\n    task=RC,\n    languages=[FR],\n    labels=[\"start_positions\", \"end_positions\"],\n    prompt_prefix=\"Les textes suivants sont accompagn\u00e9s de questions et de r\u00e9ponses.\",\n    prompt_template=\"Texte: {text}\\nQuestion: {question}\\nR\u00e9ponse en 3 mots maximum: \"\n    \"{label}\",\n    instruction_prompt=\"Texte: {text}\\n\\nR\u00e9pondez \u00e0 la question suivante sur le \"\n    \"texte ci-dessus en 3 mots maximum.\\n\\nQuestion: {question}\",\n    num_few_shot_examples=4,\n    max_generated_tokens=32,\n)\n\n### SUMMARIZATION DATASETS ###\n\nNORDJYLLAND_NEWS_CONFIG = DatasetConfig(\n    name=\"nordjylland-news\",\n    pretty_name=\"the truncated version of the Danish summarisation dataset \"\n    \"Nordjylland News\",\n    huggingface_id=\"EuroEval/nordjylland-news-mini\",\n    task=SUMM,\n    languages=[DA],\n    prompt_prefix=\"F\u00f8lgende er nyhedsartikler med tilh\u00f8rende resum\u00e9er.\",\n    prompt_template=\"Nyhedsartikel: {text}\\nResum\u00e9: {target_text}\",\n    instruction_prompt=\"Nyhedsartikel: {text}\\n\\nSkriv et resum\u00e9 af ovenst\u00e5ende \"\n    \"artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nMLSUM_CONFIG = DatasetConfig(\n    name=\"mlsum\",\n    pretty_name=\"the truncated version of the German summarisation dataset MLSum\",\n    huggingface_id=\"EuroEval/mlsum-mini\",\n    task=SUMM,\n    languages=[DE],\n    prompt_prefix=\"Im Folgenden finden Sie Nachrichtenartikel mit den dazugeh\u00f6rigen \"\n    \"Zusammenfassungen.\",\n    prompt_template=\"Nachrichtenartikel: {text}\\nZusammenfassung: {target_text}\",\n    instruction_prompt=\"Nachrichtenartikel: {text}\\n\\nSchreiben Sie eine \"\n    \"Zusammenfassung des obigen Artikels.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nRRN_CONFIG = DatasetConfig(\n    name=\"rrn\",\n    pretty_name=\"the truncated version of the Icelandic summarisation dataset \"\n    \"R\u00daV Radio News\",\n    huggingface_id=\"EuroEval/rrn-mini\",\n    task=SUMM,\n    languages=[IS],\n    prompt_prefix=\"Eftirfarandi eru fr\u00e9ttagreinar me\u00f0 tilheyrandi samantektum.\",\n    prompt_template=\"Fr\u00e9ttagrein: {text}\\nSamantekt: {target_text}\",\n    instruction_prompt=\"Fr\u00e9ttagrein: {text}\\n\\nSkrifa\u00f0u samantekt um ofangreindu \"\n    \"grein.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nNO_SAMMENDRAG_CONFIG = DatasetConfig(\n    name=\"no-sammendrag\",\n    pretty_name=\"the truncated version of the Norwegian summarisation dataset \"\n    \"Norske Sammendrag\",\n    huggingface_id=\"EuroEval/no-sammendrag-mini\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nNORGLM_MULTI_SUM = DatasetConfig(\n    name=\"norglm-multi-sum\",\n    pretty_name=\"the summarisation part of the Norwegian NorGLM multi-task human \"\n    \"annotated dataset NO-Multi-QA-Sum\",\n    huggingface_id=\"EuroEval/norglm-multi-sum\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nWIKI_LINGUA_NL_CONFIG = DatasetConfig(\n    name=\"wiki-lingua-nl\",\n    pretty_name=\"the Dutch part of the truncated version of the summarisation dataset \"\n    \"WikiLingua\",\n    huggingface_id=\"EuroEval/wiki-lingua-nl-mini\",\n    task=SUMM,\n    languages=[NL],\n    prompt_prefix=\"Hieronder volgen artikelen met bijbehorende samenvattingen.\",\n    prompt_template=\"Artikel: {text}\\nSamenvatting: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSchrijf een samenvatting van het \"\n    \"bovenstaande artikel.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSWEDN_CONFIG = DatasetConfig(\n    name=\"swedn\",\n    pretty_name=\"the truncated version of the Swedish summarisation dataset SweDN\",\n    huggingface_id=\"EuroEval/swedn-mini\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nCNN_DAILYMAIL_CONFIG = DatasetConfig(\n    name=\"cnn-dailymail\",\n    pretty_name=\"the truncated version of the English summarisation dataset \"\n    \"CNN-DailyMail\",\n    huggingface_id=\"EuroEval/cnn-dailymail-mini\",\n    task=SUMM,\n    languages=[EN],\n    prompt_prefix=\"The following are articles with accompanying summaries.\",\n    prompt_template=\"News article: {text}\\nSummary: {target_text}\",\n    instruction_prompt=\"News article: {text}\\n\\nWrite a summary of the above article.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\nSCHIBSTED_SV_CONFIG = DatasetConfig(\n    name=\"schibsted-sv\",\n    pretty_name=\"the Swedish summarisation dataset Schibsted-sv\",\n    huggingface_id=\"EuroEval/schibsted-article-summaries-sv\",\n    task=SUMM,\n    languages=[SV],\n    prompt_prefix=\"Nedan f\u00f6ljer artiklar med tillh\u00f6rande sammanfattningar.\",\n    prompt_template=\"Artikel: {text}\\nSammanfattning: {target_text}\",\n    instruction_prompt=\"Artikel: {text}\\n\\nSkriv en sammanfattning av artikeln ovan.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nSCHIBSTED_NO_CONFIG = DatasetConfig(\n    name=\"schibsted-no\",\n    pretty_name=\"the Norwegian summarisation dataset Schibsted-no\",\n    huggingface_id=\"EuroEval/schibsted-article-summaries-no\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nPERSONAL_SUM_CONFIG = DatasetConfig(\n    name=\"personal-sum\",\n    pretty_name=\"the Norwegian summarisation dataset personal-sum\",\n    huggingface_id=\"EuroEval/personal-sum\",\n    task=SUMM,\n    languages=[NB, NN, NO],\n    prompt_prefix=\"Her f\u00f8lger nyhetsartikler med tilh\u00f8rende sammendrag.\",\n    prompt_template=\"Nyhetsartikkel: {text}\\nSammendrag: {target_text}\",\n    instruction_prompt=\"Nyhetsartikkel: {text}\\n\\nSkriv et sammendrag av den \"\n    \"ovennevnte artikkelen.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n    unofficial=True,\n)\n\nORANGE_SUM_CONFIG = DatasetConfig(\n    name=\"orange-sum\",\n    pretty_name=\"the truncated version of the French summarisation dataset OrangeSum\",\n    huggingface_id=\"EuroEval/orange-sum-mini\",\n    task=SUMM,\n    languages=[FR],\n    prompt_prefix=\"Les articles suivants sont accompagn\u00e9s d'un r\u00e9sum\u00e9.\",\n    prompt_template=\"Article de presse: {text}\\nR\u00e9sum\u00e9: {target_text}\",\n    instruction_prompt=\"Article de presse: {text}\\n\\nR\u00e9digez un r\u00e9sum\u00e9 de l'article \"\n    \"ci-dessus.\",\n    num_few_shot_examples=1,\n    max_generated_tokens=256,\n)\n\n# TODO: Faroese summarization\n\n\n### KNOWLEDGE DATASETS ###\n\nDANSKE_TALEMAADER_CONFIG = DatasetConfig(\n    name=\"danske-talemaader\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset Danske \"\n    \"Talem\u00e5der\",\n    huggingface_id=\"EuroEval/danske-talemaader\",\n    task=KNOW,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nDANISH_CITIZEN_TESTS_CONFIG = DatasetConfig(\n    name=\"danish-citizen-tests\",\n    pretty_name=\"the Danish knowledge dataset Danish Citizen Tests\",\n    huggingface_id=\"EuroEval/danish-citizen-tests\",\n    task=KNOW,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_NO_CONFIG = DatasetConfig(\n    name=\"mmlu-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset MMLU-no, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd', og ikke noe annet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_SV_CONFIG = DatasetConfig(\n    name=\"mmlu-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset MMLU-sv, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd', och inget annat.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_IS_CONFIG = DatasetConfig(\n    name=\"mmlu-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset MMLU-is, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nICELANDIC_KNOWLEDGE_CONFIG = DatasetConfig(\n    name=\"icelandic-knowledge\",\n    pretty_name=\"the IcelandicQA dataset phrased as a knowledge dataset\",\n    huggingface_id=\"EuroEval/icelandic-knowledge\",\n    task=KNOW,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd'.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nMMLU_DE_CONFIG = DatasetConfig(\n    name=\"mmlu-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset MMLU-de, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd', und nichts anderes.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_NL_CONFIG = DatasetConfig(\n    name=\"mmlu-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset MMLU-nl, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd', en niets anders.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_CONFIG = DatasetConfig(\n    name=\"mmlu\",\n    pretty_name=\"the truncated version of the English knowledge dataset MMLU\",\n    huggingface_id=\"EuroEval/mmlu-mini\",\n    task=KNOW,\n    languages=[EN],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by replying \"\n    \"with 'a', 'b', 'c' or 'd', and nothing else.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nMMLU_DA_CONFIG = DatasetConfig(\n    name=\"mmlu-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset MMLU-da, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nMMLU_FR_CONFIG = DatasetConfig(\n    name=\"mmlu-fr\",\n    pretty_name=\"the truncated version of the French knowledge dataset MMLU-fr, \"\n    \"translated from the English MMLU dataset\",\n    huggingface_id=\"EuroEval/mmlu-fr-mini\",\n    task=KNOW,\n    languages=[FR],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Les questions suivantes sont des questions \u00e0 choix multiples \"\n    \"(avec r\u00e9ponses).\",\n    prompt_template=\"Question: {text}\\nR\u00e9ponse: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', \"\n    \"'b', 'c' ou 'd', et rien d'autre.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nARC_DA_CONFIG = DatasetConfig(\n    name=\"arc-da\",\n    pretty_name=\"the truncated version of the Danish knowledge dataset ARC-da, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-da-mini\",\n    task=KNOW,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_NO_CONFIG = DatasetConfig(\n    name=\"arc-no\",\n    pretty_name=\"the truncated version of the Norwegian knowledge dataset ARC-no, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-no-mini\",\n    task=KNOW,\n    languages=[NB, NN, NO],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd', og ikke noe annet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_SV_CONFIG = DatasetConfig(\n    name=\"arc-sv\",\n    pretty_name=\"the truncated version of the Swedish knowledge dataset ARC-sv, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-sv-mini\",\n    task=KNOW,\n    languages=[SV],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd', och inget annat.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_IS_CONFIG = DatasetConfig(\n    name=\"arc-is\",\n    pretty_name=\"the truncated version of the Icelandic knowledge dataset ARC-is, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-is-mini\",\n    task=KNOW,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nARC_DE_CONFIG = DatasetConfig(\n    name=\"arc-de\",\n    pretty_name=\"the truncated version of the German knowledge dataset ARC-de, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-de-mini\",\n    task=KNOW,\n    languages=[DE],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd', und nichts anderes.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_NL_CONFIG = DatasetConfig(\n    name=\"arc-nl\",\n    pretty_name=\"the truncated version of the Dutch knowledge dataset ARC-nl, \"\n    \"translated from the English ARC dataset\",\n    huggingface_id=\"EuroEval/arc-nl-mini\",\n    task=KNOW,\n    languages=[NL],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd', en niets anders.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nARC_CONFIG = DatasetConfig(\n    name=\"arc\",\n    pretty_name=\"the truncated version of the English knowledge dataset ARC\",\n    huggingface_id=\"EuroEval/arc-mini\",\n    task=KNOW,\n    languages=[EN],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by replying \"\n    \"with 'a', 'b', 'c' or 'd', and nothing else.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n# TODO: Faroese knowledge\n\n\n### COMMON SENSE REASONING DATASETS ###\n\nHELLASWAG_DA_CONFIG = DatasetConfig(\n    name=\"hellaswag-da\",\n    pretty_name=\"the truncated version of the Danish common-sense reasoning dataset \"\n    \"HellaSwag-da, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-da-mini\",\n    task=COMMON_SENSE,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er multiple choice sp\u00f8rgsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rgsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rgsm\u00e5l: {text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_NO_CONFIG = DatasetConfig(\n    name=\"hellaswag-no\",\n    pretty_name=\"the truncated version of the Norwegian common-sense reasoning dataset \"\n    \"HellaSwag-no, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-no-mini\",\n    task=COMMON_SENSE,\n    languages=[NB, NN, NO],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er flervalgssp\u00f8rsm\u00e5l (med svar).\",\n    prompt_template=\"Sp\u00f8rsm\u00e5l: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Sp\u00f8rsm\u00e5l: {text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd', og ikke noe annet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_SV_CONFIG = DatasetConfig(\n    name=\"hellaswag-sv\",\n    pretty_name=\"the truncated version of the Swedish common-sense reasoning dataset \"\n    \"HellaSwag-sv, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-sv-mini\",\n    task=COMMON_SENSE,\n    languages=[SV],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f6ljande \u00e4r flervalsfr\u00e5gor (med svar).\",\n    prompt_template=\"Fr\u00e5ga: {text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Fr\u00e5ga: {text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd', och inget annat.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_IS_CONFIG = DatasetConfig(\n    name=\"hellaswag-is\",\n    pretty_name=\"the truncated version of the Icelandic common-sense reasoning dataset \"\n    \"HellaSwag-is, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-is-mini\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nWINOGRANDE_IS_CONFIG = DatasetConfig(\n    name=\"winogrande-is\",\n    pretty_name=\"the Icelandic common-sense reasoning dataset \"\n    \"Winogrande-is, manually translated from the English Winogrande dataset\",\n    huggingface_id=\"EuroEval/winogrande-is\",\n    task=COMMON_SENSE,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru fj\u00f6lvalsspurningar (me\u00f0 sv\u00f6rum).\",\n    prompt_template=\"Spurningar: {text}\\nSvara: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Spurningar: {text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_DE_CONFIG = DatasetConfig(\n    name=\"hellaswag-de\",\n    pretty_name=\"the truncated version of the German common-sense reasoning dataset \"\n    \"HellaSwag-de, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-de-mini\",\n    task=COMMON_SENSE,\n    languages=[DE],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Die folgenden Fragen sind Multiple-Choice-Fragen (mit Antworten).\",\n    prompt_template=\"Frage: {text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Frage: {text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd', und nichts anderes.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_NL_CONFIG = DatasetConfig(\n    name=\"hellaswag-nl\",\n    pretty_name=\"the truncated version of the Dutch common-sense reasoning dataset \"\n    \"HellaSwag-nl, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-nl-mini\",\n    task=COMMON_SENSE,\n    languages=[NL],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Hieronder staan meerkeuzevragen (met antwoorden).\",\n    prompt_template=\"Vraag: {text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Vraag: {text}\\n\\nBeantwoord de bovenstaande vraag met 'a', \"\n    \"'b', 'c' of 'd', en niets anders.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_CONFIG = DatasetConfig(\n    name=\"hellaswag\",\n    pretty_name=\"the truncated version of the English common-sense reasoning \"\n    \"dataset HellaSwag\",\n    huggingface_id=\"EuroEval/hellaswag-mini\",\n    task=COMMON_SENSE,\n    languages=[EN],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"The following are multiple choice questions (with answers).\",\n    prompt_template=\"Question: {text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nAnswer the above question by replying \"\n    \"with 'a', 'b', 'c' or 'd', and nothing else.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\nHELLASWAG_FR_CONFIG = DatasetConfig(\n    name=\"hellaswag-fr\",\n    pretty_name=\"the truncated version of the French common-sense reasoning dataset \"\n    \"HellaSwag-fr, translated from the English HellaSwag dataset\",\n    huggingface_id=\"EuroEval/hellaswag-fr-mini\",\n    task=COMMON_SENSE,\n    languages=[FR],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Les questions suivantes sont des questions \u00e0 choix multiples \"\n    \"(avec r\u00e9ponses).\",\n    prompt_template=\"Question: {text}\\nR\u00e9ponse: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"Question: {text}\\n\\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', \"\n    \"'b', 'c' ou 'd', et rien d'autre.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n)\n\n# TODO: Faroese common sense reasoning\n\n\n### MULTIPLE CHOICE READING COMPREHENSION DATASETS ###\n\nBELEBELE_DA_CONFIG = DatasetConfig(\n    name=\"belebele-da\",\n    pretty_name=\"the Danish multiple choice reading comprehension dataset BeleBele-da, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-da-mini\",\n    task=MCRC,\n    languages=[DA],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"F\u00f8lgende er tekster med tilh\u00f8rende multiple choice sp\u00f8rgsm\u00e5l og \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nBesvar ovenst\u00e5ende sp\u00f8rgsm\u00e5l ved at \"\n    \"svare med 'a', 'b', 'c' eller 'd', og intet andet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_SV_CONFIG = DatasetConfig(\n    name=\"belebele-sv\",\n    pretty_name=\"the Swedish multiple choice reading comprehension dataset \"\n    \"BeleBele-sv, translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-sv-mini\",\n    task=MCRC,\n    languages=[SV],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Nedan f\u00f6ljer texter med tillh\u00f6rande multiple choice fr\u00e5gor och \"\n    \"svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nBesvara f\u00f6ljande fr\u00e5ga med 'a', 'b', 'c' \"\n    \"eller 'd', och inget annat.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_NO_CONFIG = DatasetConfig(\n    name=\"belebele-no\",\n    pretty_name=\"the Norwegian multiple choice reading comprehension dataset \"\n    \"BeleBele-no, translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-no-mini\",\n    task=MCRC,\n    languages=[NB, NN, NO],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Her f\u00f8lger tekster med tilh\u00f8rende multiple choice sp\u00f8rsm\u00e5l og svar.\",\n    prompt_template=\"{text}\\nSvar: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nBesvar f\u00f8lgende sp\u00f8rsm\u00e5l med 'a', 'b', \"\n    \"'c' eller 'd', og ikke noe annet.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_IS_CONFIG = DatasetConfig(\n    name=\"belebele-is\",\n    pretty_name=\"the Icelandic multiple choice reading comprehension dataset \"\n    \"BeleBele-is, translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-is-mini\",\n    task=MCRC,\n    languages=[IS],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Eftirfarandi eru textar me\u00f0 tilheyrandi fj\u00f6lvalsspurningum og \"\n    \"sv\u00f6rum.\",\n    prompt_template=\"{text}\\nSvara: {label}\",\n    instruction_prompt=\"{text}\\n\\nSvara\u00f0u eftirfarandi spurningum me\u00f0 'a', \"\n    \"'b', 'c' e\u00f0a 'd', og engu \u00f6\u00f0ru.\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_DE_CONFIG = DatasetConfig(\n    name=\"belebele-de\",\n    pretty_name=\"the German multiple choice reading comprehension dataset BeleBele-de, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-de-mini\",\n    task=MCRC,\n    languages=[DE],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Die folgenden Texte sind mit dazugeh\u00f6rigen Multiple-Choice-Fragen \"\n    \"und Antworten.\",\n    prompt_template=\"{text}\\nAntwort: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nBeantworten Sie die obige Frage mit 'a', 'b', \"\n    \"'c' oder 'd', und nichts anderes.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_NL_CONFIG = DatasetConfig(\n    name=\"belebele-nl\",\n    pretty_name=\"the Dutch multiple choice reading comprehension dataset BeleBele-nl, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-nl-mini\",\n    task=MCRC,\n    languages=[NL],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Hieronder staan teksten met bijbehorende multiple choice vragen en \"\n    \"antwoorden.\",\n    prompt_template=\"{text}\\nAntwoord: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nBeantwoord de bovenstaande vraag met 'a', 'b', \"\n    \"'c' of 'd', en niets anders.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_FR_CONFIG = DatasetConfig(\n    name=\"belebele-fr\",\n    pretty_name=\"the French multiple choice reading comprehension dataset BeleBele-fr, \"\n    \"translated from the English BeleBele dataset\",\n    huggingface_id=\"EuroEval/belebele-fr-mini\",\n    task=MCRC,\n    languages=[FR],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"Les textes suivants sont accompagn\u00e9s de questions \u00e0 choix \"\n    \"multiples et de r\u00e9ponses.\",\n    prompt_template=\"{text}\\nR\u00e9ponse: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nR\u00e9pondez \u00e0 la question ci-dessus par 'a', \"\n    \"'b', 'c' ou 'd', et rien d'autre.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\nBELEBELE_CONFIG = DatasetConfig(\n    name=\"belebele\",\n    pretty_name=\"the English multiple choice reading comprehension dataset BeleBele\",\n    huggingface_id=\"EuroEval/belebele-mini\",\n    task=MCRC,\n    languages=[EN],\n    labels=[\"a\", \"b\", \"c\", \"d\"],\n    prompt_prefix=\"The following are texts with accompanying multiple choice questions \"\n    \"and answers.\",\n    prompt_template=\"{text}\\nAnswer: {label}\",\n    prompt_label_mapping=dict(a=\"a\", b=\"b\", c=\"c\", d=\"d\"),\n    instruction_prompt=\"{text}\\n\\nAnswer the above question by replying \"\n    \"with 'a', 'b', 'c' or 'd', and nothing else.\",\n    num_few_shot_examples=5,\n    max_generated_tokens=5,\n    unofficial=True,\n)\n\n\n### SPEED ESTIMATION DATASETS ###\n\nSPEED_CONFIG = DatasetConfig(\n    name=\"speed\",\n    pretty_name=\"the speed estimation benchmark\",\n    huggingface_id=\"\",\n    task=SPEED,\n    languages=list(get_all_languages().values()),\n    prompt_prefix=\"\",\n    prompt_template=\"\",\n    instruction_prompt=\"\",\n    num_few_shot_examples=0,\n    max_generated_tokens=5,\n)\n</code></pre>"},{"location":"api/euroeval/data_loading/","title":"euroeval.data_loading","text":"euroeval.data_loading<p> source module euroeval.data_loading </p> <p>Functions related to the loading of the data.</p> <p> Functions </p> <ul> <li> <p>load_data \u2014 Load the raw bootstrapped datasets.</p> </li> </ul> <p> source load_data(rng: Generator, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[DatasetDict] </p> <p>Load the raw bootstrapped datasets.</p> <p> Parameters </p> <ul> <li> <p>rng :  Generator \u2014</p> <p>The random number generator to use.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration for the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[DatasetDict] \u2014 A list of bootstrapped datasets, one for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/euroeval/data_loading/","title":"euroeval.data_loading","text":"euroeval.data_loading<p> docs module euroeval.data_loading </p> <pre><code>\"\"\"Functions related to the loading of the data.\"\"\"\n\nimport logging\nimport sys\nimport time\n\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom datasets.exceptions import DatasetsError\nfrom huggingface_hub.errors import HfHubHTTPError\nfrom numpy.random import Generator\n\nfrom .data_models import BenchmarkConfig, DatasetConfig\nfrom .exceptions import InvalidBenchmark\nfrom .utils import unscramble\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef load_data(docs\n    rng: Generator, dataset_config: \"DatasetConfig\", benchmark_config: \"BenchmarkConfig\"\n) -&gt; list[DatasetDict]:\n    \"\"\"Load the raw bootstrapped datasets.\n\n    Args:\n        rng:\n            The random number generator to use.\n        dataset_config:\n            The configuration for the dataset.\n        benchmark_config:\n            The configuration for the benchmark.\n\n    Returns:\n        A list of bootstrapped datasets, one for each iteration.\n    \"\"\"\n    num_attempts = 5\n    for _ in range(num_attempts):\n        try:\n            dataset = load_dataset(\n                path=dataset_config.huggingface_id,\n                cache_dir=benchmark_config.cache_dir,\n                token=unscramble(\"HjccJFhIozVymqXDVqTUTXKvYhZMTbfIjMxG_\"),\n            )\n            break\n        except (FileNotFoundError, DatasetsError):\n            logger.warning(\n                f\"Failed to load dataset {dataset_config.huggingface_id!r}. Retrying...\"\n            )\n            time.sleep(1)\n            continue\n        except HfHubHTTPError:\n            raise InvalidBenchmark(\"The Hugging Face Hub seems to be down.\")\n    else:\n        raise InvalidBenchmark(\n            f\"Failed to load dataset {dataset_config.huggingface_id!r} after \"\n            f\"{num_attempts} attempts.\"\n        )\n\n    assert isinstance(dataset, DatasetDict)  # type: ignore[used-before-def]\n\n    dataset = DatasetDict({key: dataset[key] for key in [\"train\", \"val\", \"test\"]})\n\n    if not benchmark_config.evaluate_test_split:\n        dataset[\"test\"] = dataset[\"val\"]\n\n    # Remove empty examples from the datasets\n    for text_feature in [\"tokens\", \"text\"]:\n        if text_feature in dataset[\"train\"].features:\n            dataset = dataset.filter(lambda x: len(x[text_feature]) &gt; 0)\n\n    # If we are testing then truncate the test set\n    if hasattr(sys, \"_called_from_test\"):\n        dataset[\"test\"] = dataset[\"test\"].select(range(1))\n\n    # Bootstrap the splits\n    bootstrapped_splits: dict[str, list[Dataset]] = dict()\n    for split in [\"train\", \"val\", \"test\"]:\n        bootstrap_indices = rng.integers(\n            0,\n            len(dataset[split]),\n            size=(benchmark_config.num_iterations, len(dataset[split])),\n        )\n        bootstrapped_splits[split] = [\n            dataset[split].select(bootstrap_indices[idx])\n            for idx in range(benchmark_config.num_iterations)\n        ]\n\n    datasets = [\n        DatasetDict(\n            {\n                split: bootstrapped_splits[split][idx]\n                for split in [\"train\", \"val\", \"test\"]\n            }\n        )\n        for idx in range(benchmark_config.num_iterations)\n    ]\n    return datasets\n</code></pre>"},{"location":"api/euroeval/data_models/","title":"euroeval.data_models","text":"euroeval.data_models<p> source module euroeval.data_models </p> <p>Data models used in EuroEval.</p> <p> Classes </p> <ul> <li> <p>MetricConfig \u2014 Configuration for a metric.</p> </li> <li> <p>Task \u2014 A dataset task.</p> </li> <li> <p>Language \u2014 A benchmarkable language.</p> </li> <li> <p>BenchmarkConfig \u2014 General benchmarking configuration, across datasets and models.</p> </li> <li> <p>BenchmarkConfigParams \u2014 The parameters for the benchmark configuration.</p> </li> <li> <p>BenchmarkResult \u2014 A benchmark result.</p> </li> <li> <p>DatasetConfig \u2014 Configuration for a dataset.</p> </li> <li> <p>ModelConfig \u2014 Configuration for a model.</p> </li> <li> <p>PreparedModelInputs \u2014 The inputs to a model.</p> </li> <li> <p>GenerativeModelOutput \u2014 The output of a generative model.</p> </li> <li> <p>SingleGenerativeModelOutput \u2014 A single output of a generative model.</p> </li> <li> <p>HFModelInfo \u2014 Information about a Hugging Face model.</p> </li> </ul> <p> source dataclass MetricConfig(name: str, pretty_name: str, huggingface_id: str, results_key: str, compute_kwargs: dict[str, t.Any] = field(default_factory=dict), postprocessing_fn: c.Callable[[float], tuple[float, str]] = field(default_factory=lambda: lambda raw_score: (100 * raw_score, f'{raw_score:.2%}'))) </p> <p>Configuration for a metric.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the metric.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the metric, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the metric.</p> </li> <li> <p>results_key :  str \u2014</p> <p>The name of the key used to extract the metric scores from the results dictionary.</p> </li> <li> <p>compute_kwargs :  dict[str, t.Any] \u2014</p> <p>Keyword arguments to pass to the metric's compute function. Defaults to an empty dictionary.</p> </li> <li> <p>postprocessing_fn :  c.Callable[[float], tuple[float, str]] \u2014</p> <p>A function to apply to the metric scores after they are computed, taking the score to the postprocessed score along with its string representation. Defaults to x -&gt; (100 * x, f\"{x:.2%}\").</p> </li> </ul> <p> source dataclass Task(name: str, task_group: TaskGroup, metrics: list[MetricConfig]) </p> <p>A dataset task.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the task.</p> </li> <li> <p>task_group :  TaskGroup \u2014</p> <p>The task group of the task.</p> </li> <li> <p>metrics :  list[MetricConfig] \u2014</p> <p>The metrics used to evaluate the task.</p> </li> </ul> <p> source dataclass Language(code: str, name: str) </p> <p>A benchmarkable language.</p> <p> Attributes </p> <ul> <li> <p>code :  str \u2014</p> <p>The ISO 639-1 language code of the language.</p> </li> <li> <p>name :  str \u2014</p> <p>The name of the language.</p> </li> </ul> <p> source dataclass BenchmarkConfig(model_languages: list[Language], dataset_languages: list[Language], tasks: list[Task], datasets: list[str], batch_size: int, raise_errors: bool, cache_dir: str, api_key: str | None, force: bool, progress_bar: bool, save_results: bool, device: torch.device, verbose: bool, trust_remote_code: bool, use_flash_attention: bool | None, clear_model_cache: bool, evaluate_test_split: bool, few_shot: bool, num_iterations: int, api_base: str | None, api_version: str | None, debug: bool, run_with_cli: bool, only_allow_safetensors: bool) </p> <p>General benchmarking configuration, across datasets and models.</p> <p> Attributes </p> <ul> <li> <p>model_languages :  list[Language] \u2014</p> <p>The languages of the models to benchmark.</p> </li> <li> <p>dataset_languages :  list[Language] \u2014</p> <p>The languages of the datasets in the benchmark.</p> </li> <li> <p>tasks :  list[Task] \u2014</p> <p>The tasks benchmark the model(s) on.</p> </li> <li> <p>datasets :  list[str] \u2014</p> <p>The datasets to benchmark on.</p> </li> <li> <p>batch_size :  int \u2014</p> <p>The batch size to use.</p> </li> <li> <p>raise_errors :  bool \u2014</p> <p>Whether to raise errors instead of skipping them.</p> </li> <li> <p>cache_dir :  str \u2014</p> <p>Directory to store cached models and datasets.</p> </li> <li> <p>api_key :  str | None \u2014</p> <p>The API key to use for a given inference API.</p> </li> <li> <p>force :  bool \u2014</p> <p>Whether to force the benchmark to run even if the results are already cached.</p> </li> <li> <p>progress_bar :  bool \u2014</p> <p>Whether to show a progress bar.</p> </li> <li> <p>save_results :  bool \u2014</p> <p>Whether to save the benchmark results to 'euroeval_benchmark_results.json'.</p> </li> <li> <p>device :  torch.device \u2014</p> <p>The device to use for benchmarking.</p> </li> <li> <p>verbose :  bool \u2014</p> <p>Whether to print verbose output.</p> </li> <li> <p>trust_remote_code :  bool \u2014</p> <p>Whether to trust remote code when loading models from the Hugging Face Hub.</p> </li> <li> <p>use_flash_attention :  bool | None \u2014</p> <p>Whether to use Flash Attention. If None then this will be used for generative models.</p> </li> <li> <p>clear_model_cache :  bool \u2014</p> <p>Whether to clear the model cache after benchmarking each model.</p> </li> <li> <p>evaluate_test_split :  bool \u2014</p> <p>Whether to evaluate on the test split.</p> </li> <li> <p>few_shot :  bool \u2014</p> <p>Whether to only evaluate the model using few-shot evaluation. Only relevant if the model is generative.</p> </li> <li> <p>num_iterations :  int \u2014</p> <p>The number of iterations each model should be evaluated for.</p> </li> <li> <p>api_base :  str | None \u2014</p> <p>The base URL for a given inference API. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>api_version :  str | None \u2014</p> <p>The version of the API to use. Only relevant if <code>model</code> refers to a model on an inference API.</p> </li> <li> <p>debug :  bool \u2014</p> <p>Whether to run the benchmark in debug mode.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> <li> <p>only_allow_safetensors :  bool \u2014</p> <p>Whether to only allow models that use the safetensors format.</p> </li> </ul> <p> source class BenchmarkConfigParams() </p> <p><p>Bases : pydantic.BaseModel</p></p> <p>The parameters for the benchmark configuration.</p> <p> Attributes </p> <ul> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> source class BenchmarkResult() </p> <p><p>Bases : pydantic.BaseModel</p></p> <p>A benchmark result.</p> <p> Attributes </p> <ul> <li> <p>model_config :  ClassVar[ConfigDict] \u2014 Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p> </li> <li> <p>model_extra :  dict[str, Any] | None \u2014 Get extra fields set during validation.</p> </li> <li> <p>model_fields_set :  set[str] \u2014 Returns the set of fields that have been explicitly set on this model instance.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>from_dict \u2014 Create a benchmark result from a dictionary.</p> </li> <li> <p>append_to_results \u2014 Append the benchmark result to the results file.</p> </li> </ul> <p> source classmethod BenchmarkResult.from_dict(config: dict) \u2192 BenchmarkResult </p> <p>Create a benchmark result from a dictionary.</p> <p> Parameters </p> <ul> <li> <p>config :  dict \u2014</p> <p>The configuration dictionary.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkResult \u2014 The benchmark result.</p> </li> </ul> <p> source method BenchmarkResult.append_to_results(results_path: pathlib.Path) \u2192 None </p> <p>Append the benchmark result to the results file.</p> <p> Parameters </p> <ul> <li> <p>results_path :  pathlib.Path \u2014</p> <p>The path to the results file.</p> </li> </ul> <p> source dataclass DatasetConfig(name: str, pretty_name: str, huggingface_id: str, task: Task, languages: list[Language], prompt_template: str, max_generated_tokens: int, prompt_prefix: str, num_few_shot_examples: int, instruction_prompt: str, labels: list[str] = field(default_factory=list), prompt_label_mapping: dict[str, str] = field(default_factory=dict), unofficial: bool = False) </p> <p>Configuration for a dataset.</p> <p> Attributes </p> <ul> <li> <p>name :  str \u2014</p> <p>The name of the dataset. Must be lower case with no spaces.</p> </li> <li> <p>pretty_name :  str \u2014</p> <p>A longer prettier name for the dataset, which allows cases and spaces. Used for logging.</p> </li> <li> <p>huggingface_id :  str \u2014</p> <p>The Hugging Face ID of the dataset.</p> </li> <li> <p>task :  Task \u2014</p> <p>The task of the dataset.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The ISO 639-1 language codes of the entries in the dataset.</p> </li> <li> <p>id2label :  dict[int, str] \u2014</p> <p>The mapping from ID to label.</p> </li> <li> <p>label2id :  dict[str, int] \u2014</p> <p>The mapping from label to ID.</p> </li> <li> <p>num_labels :  int \u2014</p> <p>The number of labels in the dataset.</p> </li> <li> <p>prompt_template :  str \u2014</p> <p>The template for the prompt to use when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate when benchmarking the dataset using few-shot evaluation.</p> </li> <li> <p>prompt_prefix :  str \u2014</p> <p>The prefix to use in the few-shot prompt.</p> </li> <li> <p>num_few_shot_examples :  int \u2014</p> <p>The number of examples to use when benchmarking the dataset using few-shot evaluation. For a classification task, these will be drawn evenly from each label.</p> </li> <li> <p>instruction_prompt :  str \u2014</p> <p>The prompt to use when benchmarking the dataset using instruction-based evaluation.</p> </li> <li> <p>labels :  optional \u2014</p> <p>The labels in the dataset. Defaults to an empty list.</p> </li> <li> <p>prompt_label_mapping :  optional \u2014</p> <p>A mapping from the labels to another phrase which is used as a substitute for the label in few-shot evaluation. Defaults to an empty dictionary.</p> </li> <li> <p>unofficial :  optional \u2014</p> <p>Whether the dataset is unofficial. Defaults to False.</p> </li> </ul> <p> source property DatasetConfig.id2label: dict[int, str] </p> <p>The mapping from ID to label.</p> <p> source property DatasetConfig.label2id: dict[str, int] </p> <p>The mapping from label to ID.</p> <p> source property DatasetConfig.num_labels: int </p> <p>The number of labels in the dataset.</p> <p> source dataclass ModelConfig(model_id: str, revision: str, task: str, languages: list[Language], inference_backend: InferenceBackend, merge: bool, model_type: ModelType, fresh: bool, model_cache_dir: str, adapter_base_model_id: str | None) </p> <p>Configuration for a model.</p> <p> Attributes </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The ID of the model.</p> </li> <li> <p>revision :  str \u2014</p> <p>The revision of the model.</p> </li> <li> <p>task :  str \u2014</p> <p>The task that the model was trained on.</p> </li> <li> <p>languages :  list[Language] \u2014</p> <p>The languages of the model.</p> </li> <li> <p>inference_backend :  InferenceBackend \u2014</p> <p>The backend used to perform inference with the model.</p> </li> <li> <p>merge :  bool \u2014</p> <p>Whether the model is a merged model.</p> </li> <li> <p>model_type :  ModelType \u2014</p> <p>The type of the model (e.g., encoder, base decoder, instruction tuned).</p> </li> <li> <p>fresh :  bool \u2014</p> <p>Whether the model is freshly initialised.</p> </li> <li> <p>model_cache_dir :  str \u2014</p> <p>The directory to cache the model in.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul> <p> source dataclass PreparedModelInputs(texts: list[str] | None = None, input_ids: torch.Tensor | None = None, attention_mask: torch.Tensor | None = None) </p> <p>The inputs to a model.</p> <p> Attributes </p> <ul> <li> <p>texts :  list[str] | None \u2014</p> <p>The texts to input to the model. Can be None if the input IDs and attention mask are provided instead.</p> </li> <li> <p>input_ids :  torch.Tensor | None \u2014</p> <p>The input IDs of the texts. Can be None if the texts are provided instead.</p> </li> <li> <p>attention_mask :  torch.Tensor | None \u2014</p> <p>The attention mask of the texts. Can be None if the texts are provided instead.</p> </li> </ul> <p> source dataclass GenerativeModelOutput(sequences: list[str], scores: list[list[list[tuple[str, float]]]] | None = None) </p> <p>The output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequences :  list[str] \u2014</p> <p>The generated sequences.</p> </li> <li> <p>scores :  list[list[list[tuple[str, float]]]] | None \u2014</p> <p>The scores of the sequences. This is an array of shape (batch_size, num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass SingleGenerativeModelOutput(sequence: str, scores: list[list[tuple[str, float]]] | None = None) </p> <p>A single output of a generative model.</p> <p> Attributes </p> <ul> <li> <p>sequence :  str \u2014</p> <p>The generated sequence.</p> </li> <li> <p>scores :  list[list[tuple[str, float]]] | None \u2014</p> <p>The scores of the sequence. This is an array of shape (num_tokens, num_logprobs, 2), where the last dimension contains the token and its logprob. Can be None if the scores are not available.</p> </li> </ul> <p> source dataclass HFModelInfo(pipeline_tag: str, tags: list[str], adapter_base_model_id: str | None) </p> <p>Information about a Hugging Face model.</p> <p> Attributes </p> <ul> <li> <p>pipeline_tag :  str \u2014</p> <p>The pipeline tag of the model.</p> </li> <li> <p>tags :  list[str] \u2014</p> <p>The other tags of the model.</p> </li> <li> <p>adapter_base_model_id :  str | None \u2014</p> <p>The model ID of the base model if the model is an adapter model. Can be None if the model is not an adapter model.</p> </li> </ul>"},{"location":"src/euroeval/data_models/","title":"euroeval.data_models","text":"euroeval.data_models<p> docs module euroeval.data_models </p> <pre><code>\"\"\"Data models used in EuroEval.\"\"\"\n\nimport collections.abc as c\nimport importlib.metadata\nimport json\nimport pathlib\nimport re\nimport typing as t\nfrom dataclasses import dataclass, field\n\nimport pydantic\nimport torch\n\nfrom .enums import Device, InferenceBackend, ModelType, TaskGroup\nfrom .types import ScoreDict\n\n\n@dataclass\nclass MetricConfig:docs\n    \"\"\"Configuration for a metric.\n\n    Attributes:\n        name:\n            The name of the metric.\n        pretty_name:\n            A longer prettier name for the metric, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the metric.\n        results_key:\n            The name of the key used to extract the metric scores from the results\n            dictionary.\n        compute_kwargs:\n            Keyword arguments to pass to the metric's compute function. Defaults to\n            an empty dictionary.\n        postprocessing_fn:\n            A function to apply to the metric scores after they are computed, taking\n            the score to the postprocessed score along with its string representation.\n            Defaults to x -&gt; (100 * x, f\"{x:.2%}\").\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    results_key: str\n    compute_kwargs: dict[str, t.Any] = field(default_factory=dict)\n    postprocessing_fn: c.Callable[[float], tuple[float, str]] = field(\n        default_factory=lambda: lambda raw_score: (100 * raw_score, f\"{raw_score:.2%}\")\n    )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the metric configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Task:docs\n    \"\"\"A dataset task.\n\n    Attributes:\n        name:\n            The name of the task.\n        task_group:\n            The task group of the task.\n        metrics:\n            The metrics used to evaluate the task.\n    \"\"\"\n\n    name: str\n    task_group: TaskGroup\n    metrics: list[MetricConfig]\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the task.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass Language:docs\n    \"\"\"A benchmarkable language.\n\n    Attributes:\n        code:\n            The ISO 639-1 language code of the language.\n        name:\n            The name of the language.\n    \"\"\"\n\n    code: str\n    name: str\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the language.\"\"\"\n        return hash(self.code)\n\n\n@dataclass\nclass BenchmarkConfig:docs\n    \"\"\"General benchmarking configuration, across datasets and models.\n\n    Attributes:\n        model_languages:\n            The languages of the models to benchmark.\n        dataset_languages:\n            The languages of the datasets in the benchmark.\n        tasks:\n            The tasks benchmark the model(s) on.\n        datasets:\n            The datasets to benchmark on.\n        batch_size:\n            The batch size to use.\n        raise_errors:\n            Whether to raise errors instead of skipping them.\n        cache_dir:\n            Directory to store cached models and datasets.\n        api_key:\n            The API key to use for a given inference API.\n        force:\n            Whether to force the benchmark to run even if the results are already\n            cached.\n        progress_bar:\n            Whether to show a progress bar.\n        save_results:\n            Whether to save the benchmark results to 'euroeval_benchmark_results.json'.\n        device:\n            The device to use for benchmarking.\n        verbose:\n            Whether to print verbose output.\n        trust_remote_code:\n            Whether to trust remote code when loading models from the Hugging Face Hub.\n        use_flash_attention:\n            Whether to use Flash Attention. If None then this will be used for\n            generative models.\n        clear_model_cache:\n            Whether to clear the model cache after benchmarking each model.\n        evaluate_test_split:\n            Whether to evaluate on the test split.\n        few_shot:\n            Whether to only evaluate the model using few-shot evaluation. Only relevant\n            if the model is generative.\n        num_iterations:\n            The number of iterations each model should be evaluated for.\n        api_base:\n            The base URL for a given inference API. Only relevant if `model` refers to a\n            model on an inference API.\n        api_version:\n            The version of the API to use. Only relevant if `model` refers to a model on\n            an inference API.\n        debug:\n            Whether to run the benchmark in debug mode.\n        run_with_cli:\n            Whether the benchmark is being run with the CLI.\n        only_allow_safetensors:\n            Whether to only allow models that use the safetensors format.\n    \"\"\"\n\n    model_languages: list[Language]\n    dataset_languages: list[Language]\n    tasks: list[Task]\n    datasets: list[str]\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    api_key: str | None\n    force: bool\n    progress_bar: bool\n    save_results: bool\n    device: torch.device\n    verbose: bool\n    trust_remote_code: bool\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    evaluate_test_split: bool\n    few_shot: bool\n    num_iterations: int\n    api_base: str | None\n    api_version: str | None\n    debug: bool\n    run_with_cli: bool\n    only_allow_safetensors: bool\n\n\nclass BenchmarkConfigParams(pydantic.BaseModel):docs\n    \"\"\"The parameters for the benchmark configuration.\"\"\"\n\n    model_config = pydantic.ConfigDict(protected_namespaces=())\n\n    progress_bar: bool\n    save_results: bool\n    task: str | list[str] | None\n    dataset: str | list[str] | None\n    language: str | list[str]\n    model_language: str | list[str] | None\n    dataset_language: str | list[str] | None\n    device: Device | None\n    batch_size: int\n    raise_errors: bool\n    cache_dir: str\n    api_key: str | None\n    force: bool\n    verbose: bool\n    trust_remote_code: bool\n    use_flash_attention: bool | None\n    clear_model_cache: bool\n    evaluate_test_split: bool\n    few_shot: bool\n    num_iterations: int\n    api_base: str | None\n    api_version: str | None\n    debug: bool\n    run_with_cli: bool\n    only_allow_safetensors: bool\n\n\nclass BenchmarkResult(pydantic.BaseModel):docs\n    \"\"\"A benchmark result.\"\"\"\n\n    dataset: str\n    task: str\n    dataset_languages: list[str]\n    model: str\n    results: ScoreDict\n    num_model_parameters: int\n    max_sequence_length: int\n    vocabulary_size: int\n    merge: bool\n    generative: bool\n    generative_type: str | None\n    few_shot: bool\n    validation_split: bool\n    euroeval_version: str = importlib.metadata.version(\"euroeval\")\n\n    @classmethod\n    def from_dict(cls, config: dict) -&gt; \"BenchmarkResult\":docs\n        \"\"\"Create a benchmark result from a dictionary.\n\n        Args:\n            config:\n                The configuration dictionary.\n\n        Returns:\n            The benchmark result.\n        \"\"\"\n        # To be backwards compatible, we accept old results which changed the model\n        # name with parameters rather than adding them as explicit parameters\n        val_matches = re.search(r\"\\(.*val.*\\)$\", config[\"model\"])\n        few_shot_matches = re.search(r\"\\(.*few-shot.*\\)$\", config[\"model\"])\n        zero_shot_matches = re.search(r\"\\(.*zero-shot.*\\)$\", config[\"model\"])\n        config[\"model\"] = re.sub(\n            r\"\\(.*(few-shot|val).*\\)$\", \"\", config[\"model\"]\n        ).strip()\n\n        if \"merge\" not in config:\n            config[\"merge\"] = False\n        if \"generative\" not in config:\n            config[\"generative\"] = (\n                few_shot_matches is not None or zero_shot_matches is not None\n            )\n        if \"generative_type\" not in config:\n            config[\"generative_type\"] = None\n        if \"few_shot\" not in config:\n            config[\"few_shot\"] = zero_shot_matches is None\n        if \"validation_split\" not in config:\n            config[\"validation_split\"] = val_matches is not None\n\n        return cls(**config)\n\n    def append_to_results(self, results_path: pathlib.Path) -&gt; None:docs\n        \"\"\"Append the benchmark result to the results file.\n\n        Args:\n            results_path:\n                The path to the results file.\n        \"\"\"\n        json_str = json.dumps(self.model_dump())\n        with results_path.open(\"a\") as f:\n            f.write(\"\\n\" + json_str)\n\n\n@dataclass\nclass DatasetConfig:docs\n    \"\"\"Configuration for a dataset.\n\n    Attributes:\n        name:\n            The name of the dataset. Must be lower case with no spaces.\n        pretty_name:\n            A longer prettier name for the dataset, which allows cases and spaces. Used\n            for logging.\n        huggingface_id:\n            The Hugging Face ID of the dataset.\n        task:\n            The task of the dataset.\n        languages:\n            The ISO 639-1 language codes of the entries in the dataset.\n        id2label:\n            The mapping from ID to label.\n        label2id:\n            The mapping from label to ID.\n        num_labels:\n            The number of labels in the dataset.\n        prompt_template:\n            The template for the prompt to use when benchmarking the dataset using\n            few-shot evaluation.\n        max_generated_tokens:\n            The maximum number of tokens to generate when benchmarking the dataset\n            using few-shot evaluation.\n        prompt_prefix:\n            The prefix to use in the few-shot prompt.\n        num_few_shot_examples:\n            The number of examples to use when benchmarking the dataset using few-shot\n            evaluation. For a classification task, these will be drawn evenly from\n            each label.\n        instruction_prompt:\n            The prompt to use when benchmarking the dataset using instruction-based\n            evaluation.\n        labels (optional):\n            The labels in the dataset. Defaults to an empty list.\n        prompt_label_mapping (optional):\n            A mapping from the labels to another phrase which is used as a substitute\n            for the label in few-shot evaluation. Defaults to an empty dictionary.\n        unofficial (optional):\n            Whether the dataset is unofficial. Defaults to False.\n    \"\"\"\n\n    name: str\n    pretty_name: str\n    huggingface_id: str\n    task: Task\n    languages: list[Language]\n    prompt_template: str\n    max_generated_tokens: int\n    prompt_prefix: str\n    num_few_shot_examples: int\n    instruction_prompt: str\n    labels: list[str] = field(default_factory=list)\n    prompt_label_mapping: dict[str, str] = field(default_factory=dict)\n    unofficial: bool = False\n\n    @property\n    def id2label(self) -&gt; dict[int, str]:docs\n        \"\"\"The mapping from ID to label.\"\"\"\n        return {idx: label for idx, label in enumerate(self.labels)}\n\n    @property\n    def label2id(self) -&gt; dict[str, int]:docs\n        \"\"\"The mapping from label to ID.\"\"\"\n        return {label: i for i, label in enumerate(self.labels)}\n\n    @property\n    def num_labels(self) -&gt; int:docs\n        \"\"\"The number of labels in the dataset.\"\"\"\n        return len(self.labels)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the dataset configuration.\"\"\"\n        return hash(self.name)\n\n\n@dataclass\nclass ModelConfig:docs\n    \"\"\"Configuration for a model.\n\n    Attributes:\n        model_id:\n            The ID of the model.\n        revision:\n            The revision of the model.\n        task:\n            The task that the model was trained on.\n        languages:\n            The languages of the model.\n        inference_backend:\n            The backend used to perform inference with the model.\n        merge:\n            Whether the model is a merged model.\n        model_type:\n            The type of the model (e.g., encoder, base decoder, instruction tuned).\n        fresh:\n            Whether the model is freshly initialised.\n        model_cache_dir:\n            The directory to cache the model in.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    model_id: str\n    revision: str\n    task: str\n    languages: list[Language]\n    inference_backend: InferenceBackend\n    merge: bool\n    model_type: ModelType\n    fresh: bool\n    model_cache_dir: str\n    adapter_base_model_id: str | None\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hash of the model configuration.\"\"\"\n        return hash(self.model_id)\n\n\n@dataclass\nclass PreparedModelInputs:docs\n    \"\"\"The inputs to a model.\n\n    Attributes:\n        texts:\n            The texts to input to the model. Can be None if the input IDs and attention\n            mask are provided instead.\n        input_ids:\n            The input IDs of the texts. Can be None if the texts are provided instead.\n        attention_mask:\n            The attention mask of the texts. Can be None if the texts are provided\n            instead.\n    \"\"\"\n\n    texts: list[str] | None = None\n    input_ids: torch.Tensor | None = None\n    attention_mask: torch.Tensor | None = None\n\n\n@dataclass\nclass GenerativeModelOutput:docs\n    \"\"\"The output of a generative model.\n\n    Attributes:\n        sequences:\n            The generated sequences.\n        scores:\n            The scores of the sequences. This is an array of shape (batch_size,\n            num_tokens, num_logprobs, 2), where the last dimension contains the\n            token and its logprob. Can be None if the scores are not available.\n    \"\"\"\n\n    sequences: list[str]\n    scores: list[list[list[tuple[str, float]]]] | None = None\n\n\n@dataclass\nclass SingleGenerativeModelOutput:docs\n    \"\"\"A single output of a generative model.\n\n    Attributes:\n        sequence:\n            The generated sequence.\n        scores:\n            The scores of the sequence. This is an array of shape (num_tokens,\n            num_logprobs, 2), where the last dimension contains the token and its\n            logprob. Can be None if the scores are not available.\n    \"\"\"\n\n    sequence: str\n    scores: list[list[tuple[str, float]]] | None = None\n\n\n@dataclass\nclass HFModelInfo:docs\n    \"\"\"Information about a Hugging Face model.\n\n    Attributes:\n        pipeline_tag:\n            The pipeline tag of the model.\n        tags:\n            The other tags of the model.\n        adapter_base_model_id:\n            The model ID of the base model if the model is an adapter model. Can be None\n            if the model is not an adapter model.\n    \"\"\"\n\n    pipeline_tag: str\n    tags: list[str]\n    adapter_base_model_id: str | None\n</code></pre>"},{"location":"api/euroeval/enums/","title":"euroeval.enums","text":"euroeval.enums<p> source module euroeval.enums </p> <p>Enums used in the project.</p> <p> Classes </p> <ul> <li> <p>AutoStrEnum \u2014 StrEnum where auto() returns the field name in lower case.</p> </li> <li> <p>Device \u2014 The compute device to use for the evaluation.</p> </li> <li> <p>InferenceBackend \u2014 The backend used for model inference.</p> </li> <li> <p>ModelType \u2014 The type of a model.</p> </li> <li> <p>GenerativeType \u2014 The type of a generative model.</p> </li> <li> <p>DataType \u2014 The data type of the model weights.</p> </li> <li> <p>BatchingPreference \u2014 The preference for batching.</p> </li> <li> <p>TaskGroup \u2014 The overall task group of a task.</p> </li> </ul> <p> source enum AutoStrEnum() </p> <p><p>Bases : str, Enum</p></p> <p>StrEnum where auto() returns the field name in lower case.</p> <p> source enum Device() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The compute device to use for the evaluation.</p> <p> Attributes </p> <ul> <li> <p>CPU \u2014</p> <p>CPU device.</p> </li> <li> <p>MPS \u2014</p> <p>MPS GPU, used in M-series MacBooks.</p> </li> <li> <p>CUDA \u2014</p> <p>CUDA GPU, used with NVIDIA GPUs.</p> </li> </ul> <p> source enum InferenceBackend() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The backend used for model inference.</p> <p> Attributes </p> <ul> <li> <p>TRANSFORMERS \u2014</p> <p>Hugging Face <code>transformers</code> library.</p> </li> <li> <p>VLLM \u2014</p> <p>VLLM library.</p> </li> <li> <p>LITELLM \u2014</p> <p>LiteLLM library.</p> </li> <li> <p>NONE \u2014</p> <p>No inference backend used (e.g., for human evaluation).</p> </li> </ul> <p> source enum ModelType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The type of a model.</p> <p> Attributes </p> <ul> <li> <p>ENCODER \u2014</p> <p>An encoder (i.e., BERT-style) model.</p> </li> <li> <p>GENERATIVE \u2014</p> <p>A generative model. Can be either decoder or encoder-decoder (aka seq2seq).</p> </li> <li> <p>HUMAN \u2014</p> <p>Human evaluator.</p> </li> </ul> <p> source enum GenerativeType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The type of a generative model.</p> <p> Attributes </p> <ul> <li> <p>BASE \u2014</p> <p>A base (i.e., pretrained) generative model.</p> </li> <li> <p>INSTRUCTION_TUNED \u2014</p> <p>An instruction-tuned generative model.</p> </li> <li> <p>REASONING \u2014</p> <p>A generative reasoning model.</p> </li> </ul> <p> source enum DataType() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The data type of the model weights.</p> <p> Attributes </p> <ul> <li> <p>FP32 \u2014</p> <p>32-bit floating point.</p> </li> <li> <p>FP16 \u2014</p> <p>16-bit floating point.</p> </li> <li> <p>BF16 \u2014</p> <p>16-bit bfloat.</p> </li> </ul> <p> source enum BatchingPreference() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The preference for batching.</p> <p> Attributes </p> <ul> <li> <p>NO_PREFERENCE \u2014</p> <p>No preference for batching.</p> </li> <li> <p>SINGLE_SAMPLE \u2014</p> <p>Single sample batching.</p> </li> <li> <p>ALL_AT_ONCE \u2014</p> <p>All samples at once batching.</p> </li> </ul> <p> source enum TaskGroup() </p> <p><p>Bases : AutoStrEnum</p></p> <p>The overall task group of a task.</p> <p> Attributes </p> <ul> <li> <p>SEQUENCE_CLASSIFICATION \u2014</p> <p>Classification of documents.</p> </li> <li> <p>MULTIPLE_CHOICE_CLASSIFICATION \u2014</p> <p>Classification of documents with multiple-choice options.</p> </li> <li> <p>TOKEN_CLASSIFICATION \u2014</p> <p>Token-level classification.</p> </li> <li> <p>QUESTION_ANSWERING \u2014</p> <p>Extractive question answering.</p> </li> <li> <p>TEXT_TO_TEXT \u2014</p> <p>Text-to-text generation.</p> </li> <li> <p>SPEED \u2014</p> <p>Speed benchmark.</p> </li> </ul>"},{"location":"src/euroeval/enums/","title":"euroeval.enums","text":"euroeval.enums<p> docs module euroeval.enums </p> <pre><code>\"\"\"Enums used in the project.\"\"\"\n\nfrom enum import Enum, auto\n\n\nclass AutoStrEnum(str, Enum):docs\n    \"\"\"StrEnum where auto() returns the field name in lower case.\"\"\"\n\n    @staticmethod\n    def _generate_next_value_(\n        name: str, start: int, count: int, last_values: list\n    ) -&gt; str:\n        return name.lower()\n\n\nclass Device(AutoStrEnum):docs\n    \"\"\"The compute device to use for the evaluation.\n\n    Attributes:\n        CPU:\n            CPU device.\n        MPS:\n            MPS GPU, used in M-series MacBooks.\n        CUDA:\n            CUDA GPU, used with NVIDIA GPUs.\n    \"\"\"\n\n    CPU = auto()\n    MPS = auto()\n    CUDA = auto()\n\n\nclass InferenceBackend(AutoStrEnum):docs\n    \"\"\"The backend used for model inference.\n\n    Attributes:\n        TRANSFORMERS:\n            Hugging Face `transformers` library.\n        VLLM:\n            VLLM library.\n        LITELLM:\n            LiteLLM library.\n        NONE:\n            No inference backend used (e.g., for human evaluation).\n    \"\"\"\n\n    TRANSFORMERS = auto()\n    VLLM = auto()\n    LITELLM = auto()\n    NONE = auto()\n\n\nclass ModelType(AutoStrEnum):docs\n    \"\"\"The type of a model.\n\n    Attributes:\n        ENCODER:\n            An encoder (i.e., BERT-style) model.\n        GENERATIVE:\n            A generative model. Can be either decoder or encoder-decoder (aka seq2seq).\n        HUMAN:\n            Human evaluator.\n    \"\"\"\n\n    ENCODER = auto()\n    GENERATIVE = auto()\n    HUMAN = auto()\n\n\nclass GenerativeType(AutoStrEnum):docs\n    \"\"\"The type of a generative model.\n\n    Attributes:\n        BASE:\n            A base (i.e., pretrained) generative model.\n        INSTRUCTION_TUNED:\n            An instruction-tuned generative model.\n        REASONING:\n            A generative reasoning model.\n    \"\"\"\n\n    BASE = auto()\n    INSTRUCTION_TUNED = auto()\n    REASONING = auto()\n\n\nclass DataType(AutoStrEnum):docs\n    \"\"\"The data type of the model weights.\n\n    Attributes:\n        FP32:\n            32-bit floating point.\n        FP16:\n            16-bit floating point.\n        BF16:\n            16-bit bfloat.\n    \"\"\"\n\n    FP32 = auto()\n    FP16 = auto()\n    BF16 = auto()\n\n\nclass BatchingPreference(AutoStrEnum):docs\n    \"\"\"The preference for batching.\n\n    Attributes:\n        NO_PREFERENCE:\n            No preference for batching.\n        SINGLE_SAMPLE:\n            Single sample batching.\n        ALL_AT_ONCE:\n            All samples at once batching.\n    \"\"\"\n\n    NO_PREFERENCE = auto()\n    SINGLE_SAMPLE = auto()\n    ALL_AT_ONCE = auto()\n\n\nclass TaskGroup(AutoStrEnum):docs\n    \"\"\"The overall task group of a task.\n\n    Attributes:\n        SEQUENCE_CLASSIFICATION:\n            Classification of documents.\n        MULTIPLE_CHOICE_CLASSIFICATION:\n            Classification of documents with multiple-choice options.\n        TOKEN_CLASSIFICATION:\n            Token-level classification.\n        QUESTION_ANSWERING:\n            Extractive question answering.\n        TEXT_TO_TEXT:\n            Text-to-text generation.\n        SPEED:\n            Speed benchmark.\n    \"\"\"\n\n    SEQUENCE_CLASSIFICATION = auto()\n    MULTIPLE_CHOICE_CLASSIFICATION = auto()\n    TOKEN_CLASSIFICATION = auto()\n    QUESTION_ANSWERING = auto()\n    TEXT_TO_TEXT = auto()\n    SPEED = auto()\n</code></pre>"},{"location":"api/euroeval/exceptions/","title":"euroeval.exceptions","text":"euroeval.exceptions<p> source module euroeval.exceptions </p> <p>Exceptions to used by other functions.</p> <p> Classes </p> <ul> <li> <p>InvalidBenchmark \u2014 The (model, dataset) combination cannot be benchmarked.</p> </li> <li> <p>InvalidModel \u2014 The model cannot be benchmarked on any datasets.</p> </li> <li> <p>HuggingFaceHubDown \u2014 The Hugging Face Hub seems to be down.</p> </li> <li> <p>NoInternetConnection \u2014 There seems to be no internet connection.</p> </li> <li> <p>NaNValueInModelOutput \u2014 There is a NaN value in the model output.</p> </li> <li> <p>FlashAttentionNotInstalled \u2014 The <code>flash-attn</code> package has not been installed.</p> </li> <li> <p>NeedsExtraInstalled \u2014 The evaluation requires extra to be installed.</p> </li> <li> <p>NeedsManualDependency \u2014 The evaluation requires a dependency to be manually installed.</p> </li> <li> <p>NeedsAdditionalArgument \u2014 The evaluation requires additional arguments to the <code>euroeval</code> command.</p> </li> <li> <p>NeedsEnvironmentVariable \u2014 The evaluation requires an environment variable to be set.</p> </li> </ul> <p> source class InvalidBenchmark(message: str = 'This model cannot be benchmarked on the given dataset.') </p> <p><p>Bases : Exception</p></p> <p>The (model, dataset) combination cannot be benchmarked.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class InvalidModel(message: str = 'The model cannot be benchmarked on any datasets.') </p> <p><p>Bases : Exception</p></p> <p>The model cannot be benchmarked on any datasets.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class HuggingFaceHubDown(message: str = 'The Hugging Face Hub is currently down.') </p> <p><p>Bases : Exception</p></p> <p>The Hugging Face Hub seems to be down.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NoInternetConnection(message: str = 'There is currently no internet connection.') </p> <p><p>Bases : Exception</p></p> <p>There seems to be no internet connection.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NaNValueInModelOutput(message: str = 'There is a NaN value in the model output.') </p> <p><p>Bases : Exception</p></p> <p>There is a NaN value in the model output.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class FlashAttentionNotInstalled(message: str = 'The model you are trying to load requires Flash Attention. To use Flash Attention, please install the <code>flash-attn</code> package, which can be done by running <code>pip install -U wheel &amp;&amp; FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation</code>.') </p> <p><p>Bases : Exception</p></p> <p>The <code>flash-attn</code> package has not been installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to display.</p> </li> </ul> <p> source class NeedsExtraInstalled(extra: str) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires extra to be installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>extra :  str \u2014</p> <p>The extra that needs to be installed.</p> </li> </ul> <p> source class NeedsManualDependency(package: str) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires a dependency to be manually installed.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>package :  str \u2014</p> <p>The package that needs to be manually installed.</p> </li> </ul> <p> source class NeedsAdditionalArgument(cli_argument: str, script_argument: str, run_with_cli: bool) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires additional arguments to the <code>euroeval</code> command.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>cli_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>euroeval</code> command.</p> </li> <li> <p>script_argument :  str \u2014</p> <p>The argument that needs to be passed to the <code>Benchmarker</code> class.</p> </li> <li> <p>run_with_cli :  bool \u2014</p> <p>Whether the benchmark is being run with the CLI.</p> </li> </ul> <p> source class NeedsEnvironmentVariable(env_var: str) </p> <p><p>Bases : InvalidModel</p></p> <p>The evaluation requires an environment variable to be set.</p> <p>Initialize the exception.</p> <p> Parameters </p> <ul> <li> <p>env_var :  str \u2014</p> <p>The environment variable that needs to be set.</p> </li> </ul>"},{"location":"src/euroeval/exceptions/","title":"euroeval.exceptions","text":"euroeval.exceptions<p> docs module euroeval.exceptions </p> <pre><code>\"\"\"Exceptions to used by other functions.\"\"\"\n\n\nclass InvalidBenchmark(Exception):docs\n    \"\"\"The (model, dataset) combination cannot be benchmarked.\"\"\"\n\n    def __init__(\n        self, message: str = \"This model cannot be benchmarked on the given dataset.\"\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass InvalidModel(Exception):docs\n    \"\"\"The model cannot be benchmarked on any datasets.\"\"\"\n\n    def __init__(\n        self, message: str = \"The model cannot be benchmarked on any datasets.\"\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass HuggingFaceHubDown(Exception):docs\n    \"\"\"The Hugging Face Hub seems to be down.\"\"\"\n\n    def __init__(\n        self, message: str = \"The Hugging Face Hub is currently down.\"\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NoInternetConnection(Exception):docs\n    \"\"\"There seems to be no internet connection.\"\"\"\n\n    def __init__(\n        self, message: str = \"There is currently no internet connection.\"\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NaNValueInModelOutput(Exception):docs\n    \"\"\"There is a NaN value in the model output.\"\"\"\n\n    def __init__(\n        self, message: str = \"There is a NaN value in the model output.\"\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass FlashAttentionNotInstalled(Exception):docs\n    \"\"\"The `flash-attn` package has not been installed.\"\"\"\n\n    def __init__(\n        self,\n        message: str = (\n            \"The model you are trying to load requires Flash Attention. To use Flash \"\n            \"Attention, please install the `flash-attn` package, which can be done by \"\n            \"running `pip install -U wheel &amp;&amp; FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE \"\n            \"pip install flash-attn --no-build-isolation`.\"\n        ),\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            message:\n                The message to display.\n        \"\"\"\n        self.message = message\n        super().__init__(self.message)\n\n\nclass NeedsExtraInstalled(InvalidModel):docs\n    \"\"\"The evaluation requires extra to be installed.\"\"\"\n\n    def __init__(self, extra: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            extra:\n                The extra that needs to be installed.\n        \"\"\"\n        self.extra = extra\n        self.message = (\n            f\"The model you are trying to load requires the `{extra}` extra to be \"\n            f\"installed. To install the `{extra}` extra, please run `pip install \"\n            f\"euroeval[{extra}]` or `pip install euroeval[all]`.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsManualDependency(InvalidModel):docs\n    \"\"\"The evaluation requires a dependency to be manually installed.\"\"\"\n\n    def __init__(self, package: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            package:\n                The package that needs to be manually installed.\n        \"\"\"\n        self.package = package\n        self.message = (\n            f\"The model you are trying to load requires the `{package}` package to be \"\n            f\"installed - please run `pip install {package}` and try again.\"\n        )\n        super().__init__(self.message)\n\n\nclass NeedsAdditionalArgument(InvalidModel):docs\n    \"\"\"The evaluation requires additional arguments to the `euroeval` command.\"\"\"\n\n    def __init__(\n        self, cli_argument: str, script_argument: str, run_with_cli: bool\n    ) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            cli_argument:\n                The argument that needs to be passed to the `euroeval` command.\n            script_argument:\n                The argument that needs to be passed to the `Benchmarker` class.\n            run_with_cli:\n                Whether the benchmark is being run with the CLI.\n        \"\"\"\n        self.cli_argument = cli_argument\n        self.script_argument = script_argument\n        if run_with_cli:\n            self.message = (\n                f\"The model you are trying to load requires the `{cli_argument}` \"\n                \"argument to be passed to the `euroeval` command. Please pass the \"\n                \"argument and try again.\"\n            )\n        else:\n            self.message = (\n                f\"The model you are trying to load requires the `{script_argument}` \"\n                \"argument  to be passed to the `Benchmarker` class. Please pass the \"\n                \"argument and try again.\"\n            )\n        super().__init__(self.message)\n\n\nclass NeedsEnvironmentVariable(InvalidModel):docs\n    \"\"\"The evaluation requires an environment variable to be set.\"\"\"\n\n    def __init__(self, env_var: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            env_var:\n                The environment variable that needs to be set.\n        \"\"\"\n        self.env_var = env_var\n        self.message = (\n            f\"The model you are trying to load requires the `{env_var}` environment \"\n            \"variable to be set. Please set the environment variable and try again.\"\n        )\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/euroeval/finetuning/","title":"euroeval.finetuning","text":"euroeval.finetuning<p> source module euroeval.finetuning </p> <p>Functions related to the finetuning of models.</p> <p> Functions </p> <ul> <li> <p>finetune \u2014 Evaluate a model on a dataset through finetuning.</p> </li> <li> <p>finetune_single_iteration \u2014 Run a single iteration of a benchmark.</p> </li> <li> <p>get_training_args \u2014 Get the training arguments for the current iteration.</p> </li> </ul> <p> source finetune(model: BenchmarkModule, datasets: list[DatasetDict], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through finetuning.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to use for training and evaluation.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dicts containing the scores for each metric for each iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source finetune_single_iteration(model: BenchmarkModule | None, dataset: DatasetDict, iteration_idx: int, training_args: TrainingArguments, model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 dict[str, float] </p> <p>Run a single iteration of a benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule | None \u2014</p> <p>The model to use in the benchmark. If None then a new model will be loaded.</p> </li> <li> <p>dataset :  DatasetDict \u2014</p> <p>The dataset to use for training and evaluation.</p> </li> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> <li> <p>training_args :  TrainingArguments \u2014</p> <p>The training arguments.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 The scores for the test dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>e</p> </li> <li> <p>InvalidBenchmark</p> </li> </ul> <p> source get_training_args(benchmark_config: BenchmarkConfig, model_config: ModelConfig, iteration_idx: int, dtype: DataType, batch_size: int | None = None) \u2192 TrainingArguments </p> <p>Get the training arguments for the current iteration.</p> <p> Parameters </p> <ul> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>iteration_idx :  int \u2014</p> <p>The index of the current iteration. This is only used to generate a unique random seed for the current iteration.</p> </li> <li> <p>dtype :  DataType \u2014</p> <p>The data type to use for the model weights.</p> </li> <li> <p>batch_size :  int | None \u2014</p> <p>The batch size to use for the current iteration, or None if the batch size in the benchmark config should be used.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>TrainingArguments \u2014 The training arguments for the current iteration.</p> </li> </ul>"},{"location":"src/euroeval/finetuning/","title":"euroeval.finetuning","text":"euroeval.finetuning<p> docs module euroeval.finetuning </p> <pre><code>\"\"\"Functions related to the finetuning of models.\"\"\"\n\nimport logging\nimport sys\nimport typing as t\n\nimport torch\nfrom datasets import DatasetDict\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    EarlyStoppingCallback,\n    IntervalStrategy,\n    PrinterCallback,\n    ProgressCallback,\n    TrainingArguments,\n)\nfrom transformers.trainer import OptimizerNames\n\nfrom .benchmark_modules import BenchmarkModule\nfrom .callbacks import NeverLeaveProgressCallback\nfrom .enums import DataType\nfrom .exceptions import InvalidBenchmark, NaNValueInModelOutput\nfrom .model_loading import load_model\nfrom .utils import (\n    block_terminal_output,\n    clear_memory,\n    enforce_reproducibility,\n    log_once,\n)\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef finetune(docs\n    model: BenchmarkModule,\n    datasets: list[DatasetDict],\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; list[dict[str, float]]:\n    \"\"\"Evaluate a model on a dataset through finetuning.\n\n    Args:\n        model:\n            The model to evaluate.\n        datasets:\n            The datasets to use for training and evaluation.\n        model_config:\n            The configuration of the model.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        A list of dicts containing the scores for each metric for each iteration.\n    \"\"\"\n    # Set the data type to use for the model weights\n    using_cuda = benchmark_config.device == torch.device(\"cuda\")\n    if using_cuda and torch.cuda.is_bf16_supported():\n        dtype = DataType.BF16\n    elif using_cuda:\n        dtype = DataType.FP16\n    else:\n        dtype = DataType.FP32\n\n    # TEMP\n    dtype = DataType.FP32\n\n    bs: int = benchmark_config.batch_size\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        # Set variable that tracks whether we need to initialize new models in\n        # the single iteration call\n        model_already_initialized = idx == 0\n\n        # Run a loop here to deal with automatic reduction of batch size\n        while True:\n            # Clear GPU memory\n            if not model_already_initialized:\n                try:\n                    del model\n                except UnboundLocalError:\n                    pass\n                clear_memory()\n\n            try:\n                # Re-block terminal output, as it gets unblocked by the `transformers`\n                # package before training\n                block_terminal_output()\n\n                training_args = get_training_args(\n                    benchmark_config=benchmark_config,\n                    model_config=model_config,\n                    iteration_idx=idx,\n                    dtype=dtype,\n                    batch_size=bs,\n                )\n\n                itr_scores = finetune_single_iteration(\n                    model=model if model_already_initialized else None,\n                    dataset=datasets[idx],\n                    iteration_idx=idx,\n                    training_args=training_args,\n                    model_config=model_config,\n                    dataset_config=dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n\n                scores.append(itr_scores)\n                logger.debug(f\"Test scores for iteration {idx}: {itr_scores}\")\n\n                break\n\n            # NaN values can appear in the model output when using mixed precision, as\n            # the hidden states get overflowed. In this case we try to disable mixed\n            # precision and try again.\n            except NaNValueInModelOutput:\n                if dtype != DataType.FP32:\n                    dtype = DataType.FP32\n                    model_already_initialized = False\n                    logger.debug(\n                        \"NaN value detected in model outputs while using mixed \"\n                        \"precision. Retrying with full fp32 precision.\"\n                    )\n                else:\n                    raise InvalidBenchmark(\n                        \"NaN value detected in model outputs, even with mixed \"\n                        \"precision disabled.\"\n                    )\n\n            except Exception as e:\n                if \"CUDA\" not in str(e) and \"out of memory\" not in str(e):\n                    raise InvalidBenchmark(str(e))\n\n                if bs &lt;= 1:\n                    msg = \"Could not benchmark the model, even with a batch size of 1!\"\n                    if \"MPS\" in str(e):\n                        msg += (\n                            \" As you are using MPS, you can try running the evaluation \"\n                            \"with the `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` \"\n                            \"environment variable set, as this removes the upper bound \"\n                            \"on the memory usage.\"\n                        )\n                    raise InvalidBenchmark(msg)\n\n                model_already_initialized = False\n\n                bs //= 2\n                logger.debug(f\"Reduced batch size to {bs}\")\n\n    return scores\n\n\ndef finetune_single_iteration(docs\n    model: BenchmarkModule | None,\n    dataset: DatasetDict,\n    iteration_idx: int,\n    training_args: TrainingArguments,\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; dict[str, float]:\n    \"\"\"Run a single iteration of a benchmark.\n\n    Args:\n        model:\n            The model to use in the benchmark. If None then a new model will be loaded.\n        dataset:\n            The dataset to use for training and evaluation.\n        iteration_idx:\n            The index of the iteration.\n        training_args:\n            The training arguments.\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The scores for the test dataset.\n    \"\"\"\n    # Set random seeds to enforce reproducibility of the randomly initialised weights\n    enforce_reproducibility(seed=training_args.seed)\n\n    if model is None:\n        model = load_model(\n            model_config=model_config,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n    trainer = model.trainer_class(\n        model=model.get_pytorch_module(),\n        processing_class=model.get_tokenizer(),\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"val\"],\n        compute_metrics=model.compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n        data_collator=model.data_collator,\n    )\n\n    if not benchmark_config.verbose:\n\n        def no_logging(logs: dict[str, float]) -&gt; None:\n            return\n\n        trainer.log = no_logging\n\n    # Re-block terminal output, as it gets unblocked by the `transformers`\n    # package before training\n    block_terminal_output()\n\n    # Sort out callbacks. We remove the callbacks that are producing unnecessary\n    # output, to avoid cluttering the terminal output\n    if not benchmark_config.verbose:\n        trainer.remove_callback(PrinterCallback)\n    trainer.remove_callback(ProgressCallback)\n    if benchmark_config.progress_bar:\n        trainer.add_callback(NeverLeaveProgressCallback)\n\n    try:\n        trainer.train()\n        with torch.inference_mode():\n            try:\n                test_scores = trainer.evaluate(\n                    eval_dataset=dataset[\"test\"],\n                    orig_eval_dataset=dataset[\"original_test\"],\n                    metric_key_prefix=\"test\",\n                )\n            except TypeError:\n                test_scores = trainer.evaluate(\n                    eval_dataset=dataset[\"test\"], metric_key_prefix=\"test\"\n                )\n        return test_scores\n\n    except NaNValueInModelOutput as e:\n        del trainer\n        del model\n        clear_memory()\n        raise e\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        raise InvalidBenchmark(str(e))\n\n\ndef get_training_args(docs\n    benchmark_config: \"BenchmarkConfig\",\n    model_config: \"ModelConfig\",\n    iteration_idx: int,\n    dtype: DataType,\n    batch_size: int | None = None,\n) -&gt; TrainingArguments:\n    \"\"\"Get the training arguments for the current iteration.\n\n    Args:\n        benchmark_config:\n            The benchmark configuration.\n        model_config:\n            The model configuration.\n        iteration_idx:\n            The index of the current iteration. This is only used to generate a\n            unique random seed for the current iteration.\n        dtype:\n            The data type to use for the model weights.\n        batch_size:\n            The batch size to use for the current iteration, or None if the batch size\n            in the benchmark config should be used.\n\n    Returns:\n        The training arguments for the current iteration.\n    \"\"\"\n    log_once(message=f\"Using {dtype} data type.\", level=logging.DEBUG)\n\n    if benchmark_config.verbose:\n        logging_strategy = IntervalStrategy.STEPS\n    else:\n        logging_strategy = IntervalStrategy.NO\n\n    if batch_size is None:\n        batch_size = benchmark_config.batch_size\n\n    training_args = TrainingArguments(\n        output_dir=model_config.model_cache_dir,\n        evaluation_strategy=IntervalStrategy.STEPS,\n        logging_strategy=logging_strategy,\n        save_strategy=IntervalStrategy.STEPS,\n        eval_steps=30,\n        logging_steps=30,\n        save_steps=30,\n        max_steps=1 if hasattr(sys, \"_called_from_test\") else 10_000,\n        use_cpu=benchmark_config.device == torch.device(\"cpu\"),\n        report_to=[],\n        save_total_limit=1,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=2e-5,\n        warmup_ratio=0.01,\n        gradient_accumulation_steps=32 // batch_size,\n        load_best_model_at_end=True,\n        optim=OptimizerNames.ADAMW_TORCH,\n        seed=4242 + iteration_idx,\n        fp16=dtype == DataType.FP16,\n        bf16=dtype == DataType.BF16,\n        disable_tqdm=not benchmark_config.progress_bar,\n        ddp_find_unused_parameters=False,\n        save_safetensors=False,\n    )\n\n    # TEMP: Use only 1 GPU for now for finetuning\n    if benchmark_config.device == torch.device(\"cuda\"):\n        training_args._n_gpu = 1\n\n    return training_args\n</code></pre>"},{"location":"api/euroeval/generation/","title":"euroeval.generation","text":"euroeval.generation<p> source module euroeval.generation </p> <p>Functions related to text generation of models.</p> <p> Functions </p> <ul> <li> <p>generate \u2014 Evaluate a model on a dataset through generation.</p> </li> <li> <p>generate_single_iteration \u2014 Evaluate a model on a dataset in a single iteration through generation.</p> </li> <li> <p>debug_log \u2014 Log inputs and outputs for debugging purposes.</p> </li> </ul> <p> source generate(model: BenchmarkModule, datasets: list[DatasetDict], model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Evaluate a model on a dataset through generation.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>datasets :  list[DatasetDict] \u2014</p> <p>The datasets to evaluate on.</p> </li> <li> <p>model_config :  ModelConfig \u2014</p> <p>The configuration of the model.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 A list of dictionaries containing the test scores.</p> </li> </ul> <p> source generate_single_iteration(dataset: Dataset, model: BenchmarkModule, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig, cache: ModelCache) \u2192 dict[str, float] </p> <p>Evaluate a model on a dataset in a single iteration through generation.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset to evaluate on.</p> </li> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to evaluate.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A list of dictionaries containing the scores for each metric.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source debug_log(batch: dict[str, t.Any], extracted_labels: list[dict | str | list[str]], dataset_config: DatasetConfig) \u2192 None </p> <p>Log inputs and outputs for debugging purposes.</p> <p> Parameters </p> <ul> <li> <p>batch :  dict[str, t.Any] \u2014</p> <p>The batch of examples to evaluate on.</p> </li> <li> <p>extracted_labels :  list[dict | str | list[str]] \u2014</p> <p>The extracted labels from the model output.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The configuration of the dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> </ul>"},{"location":"src/euroeval/generation/","title":"euroeval.generation","text":"euroeval.generation<p> docs module euroeval.generation </p> <pre><code>\"\"\"Functions related to text generation of models.\"\"\"\n\nimport logging\nimport sys\nimport typing as t\nfrom pathlib import Path\n\nimport more_itertools as mit\nfrom datasets import Dataset, DatasetDict\nfrom tqdm.auto import tqdm\n\nfrom .benchmark_modules import BenchmarkModule\nfrom .enums import BatchingPreference, TaskGroup\nfrom .exceptions import InvalidBenchmark\nfrom .model_cache import (\n    ModelCache,\n    load_cached_model_outputs,\n    split_dataset_into_cached_and_non_cached,\n)\nfrom .utils import clear_memory\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef generate(docs\n    model: \"BenchmarkModule\",\n    datasets: list[DatasetDict],\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; list[dict[str, float]]:\n    \"\"\"Evaluate a model on a dataset through generation.\n\n    Args:\n        model:\n            The model to evaluate.\n        datasets:\n            The datasets to evaluate on.\n        model_config:\n            The configuration of the model.\n        benchmark_config:\n            The configuration of the benchmark.\n        dataset_config:\n            The configuration of the dataset.\n\n    Returns:\n        A list of dictionaries containing the test scores.\n    \"\"\"\n    # Set up the name of the model output cache. If we are testing then we save the\n    # model outputs to a different cache and ensure that that cache is deleted before\n    # the next test, to ensure that the tests are independent of each other\n    if benchmark_config.debug:\n        model_cache_dir = Path.cwd()\n    else:\n        model_cache_dir = Path(model_config.model_cache_dir)\n    if hasattr(sys, \"_called_from_test\"):\n        cache_name = f\"{dataset_config.name}-model-outputs-test.json\"\n        (model_cache_dir / cache_name).unlink(missing_ok=True)\n    elif benchmark_config.debug:\n        cache_name = f\"{model_config.model_id}-{dataset_config.name}-model-outputs.json\"\n    else:\n        cache_name = f\"{dataset_config.name}-model-outputs.json\"\n\n    cache = ModelCache(\n        model_cache_dir=model_cache_dir,\n        cache_name=cache_name,\n        max_generated_tokens=dataset_config.max_generated_tokens,\n    )\n\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        test_scores = generate_single_iteration(\n            model=model,\n            dataset=datasets[idx][\"test\"],\n            cache=cache,\n            dataset_config=dataset_config,\n            benchmark_config=benchmark_config,\n        )\n\n        logger.debug(f\"Test scores for iteration {idx}: {test_scores}\")\n        scores.append(test_scores)\n        clear_memory()\n\n    if not benchmark_config.debug:\n        cache.remove()\n\n    return scores\n\n\ndef generate_single_iteration(docs\n    dataset: Dataset,\n    model: \"BenchmarkModule\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n    cache: ModelCache,\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate a model on a dataset in a single iteration through generation.\n\n    Args:\n        dataset:\n            The dataset to evaluate on.\n        model:\n            The model to evaluate.\n        dataset_config:\n            The configuration of the dataset.\n        benchmark_config:\n            The configuration of the benchmark.\n        cache:\n            The model output cache.\n\n    Returns:\n        A list of dictionaries containing the scores for each metric.\n    \"\"\"\n    cache.load()\n\n    # Split up the dataset into a cached and non-cached part\n    cached_dataset, non_cached_dataset = split_dataset_into_cached_and_non_cached(\n        dataset=dataset, cache=cache\n    )\n\n    all_preds: list[str] = list()\n\n    if len(non_cached_dataset) &gt; 0:\n        match model.batching_preference:\n            case BatchingPreference.SINGLE_SAMPLE:\n                itr = tqdm(iterable=non_cached_dataset, leave=False)\n            case BatchingPreference.ALL_AT_ONCE:\n                itr = [non_cached_dataset[:]]\n            case _:\n                num_batches = len(non_cached_dataset) // benchmark_config.batch_size\n                if len(non_cached_dataset) % benchmark_config.batch_size != 0:\n                    num_batches += 1\n                itr = tqdm(\n                    iterable=mit.batched(\n                        iterable=non_cached_dataset, n=benchmark_config.batch_size\n                    ),\n                    total=len(non_cached_dataset) // benchmark_config.batch_size,\n                )\n\n        # Generate the completions for the non-cached examples\n        for batch in itr:\n            assert isinstance(batch, dict)\n\n            single_sample_batch = (\n                \"text\" in batch and isinstance(batch[\"text\"], str)\n            ) or (\"messages\" in batch and isinstance(batch[\"messages\"][0], dict))\n            if single_sample_batch:\n                batch = {key: [value] for key, value in batch.items()}\n\n            model_output = model.generate(inputs=batch)\n            extracted_labels = model.extract_labels_from_generation(\n                input_batch=batch, model_output=model_output\n            )\n\n            # Extended logging if we are running in debug mode\n            if benchmark_config.debug:\n                debug_log(\n                    batch=batch,\n                    extracted_labels=extracted_labels,  # type: ignore[arg-type]\n                    dataset_config=dataset_config,\n                )\n\n            cache.add_to_cache(model_inputs=batch, model_output=model_output)\n            all_preds.extend(extracted_labels)\n\n            # If we are debugging then we save the cache often, but since this makes\n            # evaluation slower, we do not do this by default\n            if benchmark_config.debug:\n                cache.save()\n\n        if isinstance(itr, tqdm):\n            itr.close()\n\n        # Store the cache to disk\n        cache.save()\n\n    # Fetch the cached predictions for the cached examples\n    if len(cached_dataset) &gt; 0:\n        model_output = load_cached_model_outputs(\n            cached_dataset=cached_dataset, cache=cache\n        )\n        extracted_labels = model.extract_labels_from_generation(\n            input_batch=cached_dataset[:], model_output=model_output\n        )\n        all_preds.extend(extracted_labels)\n\n    if \"label\" in non_cached_dataset.column_names:\n        ground_truth = [\n            label.lower() if isinstance(label, str) else label\n            for label in non_cached_dataset[\"label\"] + cached_dataset[\"label\"]\n        ]\n    elif \"labels\" in non_cached_dataset.column_names:\n        ground_truth = [\n            [label.lower() if isinstance(label, str) else label for label in label_list]\n            for label_list in non_cached_dataset[\"labels\"] + cached_dataset[\"labels\"]\n        ]\n    elif \"target_text\" in non_cached_dataset.column_names:\n        ground_truth = non_cached_dataset[\"target_text\"] + cached_dataset[\"target_text\"]\n    else:\n        raise ValueError(\n            \"The dataset must have either a 'label', 'labels', or 'target_text' column\"\n        )\n\n    itr_scores: dict[str, float] = model.compute_metrics(\n        model_outputs_and_labels=(all_preds, ground_truth)\n    )\n\n    return itr_scores\n\n\ndef debug_log(docs\n    batch: dict[str, t.Any],\n    extracted_labels: list[dict | str | list[str]],\n    dataset_config: \"DatasetConfig\",\n) -&gt; None:\n    \"\"\"Log inputs and outputs for debugging purposes.\n\n    Args:\n        batch:\n            The batch of examples to evaluate on.\n        extracted_labels:\n            The extracted labels from the model output.\n        dataset_config:\n            The configuration of the dataset.\n    \"\"\"\n    match dataset_config.task.task_group:\n        case TaskGroup.TOKEN_CLASSIFICATION:\n            log_msgs = [\"\"]\n            for tokens, predictions, labels in zip(\n                batch[\"tokens\"], extracted_labels, batch[\"labels\"]\n            ):\n                predictions = [tag.upper() for tag in predictions]\n                sample = list(zip(tokens, predictions, labels))\n                log_batches = [\n                    [(\"Tokens: \", \"Predictions: \", \"Labels: \")] + sample[i : i + 10]\n                    for i in range(0, len(sample), 10)\n                ]\n                for log_batch in log_batches:\n                    lengths = [len(max(triple, key=len)) for triple in log_batch]\n                    log_batch = [\n                        [f\"{x:&lt;{length}}\" for x in triple]\n                        for triple, length in zip(log_batch, lengths)\n                    ]\n                    tokens = [triple[0] for triple in log_batch]\n                    predictions = [triple[1] for triple in log_batch]\n                    labels = [triple[2] for triple in log_batch]\n                    log_msgs.append(\n                        \"\\t\".join(tokens)\n                        + \"\\n\"\n                        + \"\\t\".join(predictions)\n                        + \"\\n\"\n                        + \"\\t\".join(labels)\n                    )\n            logger.info(\"\\n\\n\".join(log_msgs))\n            return\n\n        case (\n            TaskGroup.SEQUENCE_CLASSIFICATION | TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION\n        ):\n            labels = [\n                dataset_config.prompt_label_mapping.get(label, label).lower()\n                for label in batch[\"label\"]\n            ]\n\n        case TaskGroup.QUESTION_ANSWERING:\n            extracted_labels = [\n                prediction[\"prediction_text\"]\n                for prediction in extracted_labels\n                if isinstance(prediction, dict)\n            ]\n            labels = [label[\"answers\"][\"text\"][0] for label in batch[\"label\"]]\n\n        case TaskGroup.TEXT_TO_TEXT:\n            labels = batch[\"target_text\"]\n\n        case _:\n            raise InvalidBenchmark(\n                f\"The task group '{dataset_config.task.task_group}' is not supported.\"\n            )\n\n    if \"messages\" in batch:\n        input_texts = [messages[-1][\"content\"] for messages in batch[\"messages\"]]\n    else:\n        input_texts = batch[\"text\"]\n\n    for input_text, prediction, label in zip(input_texts, extracted_labels, labels):\n        logger.info(\n            f\"Input: '{input_text}'\\nPrediction: '{prediction}'\\nLabel: '{label}'\"\n        )\n</code></pre>"},{"location":"api/euroeval/human_evaluation/","title":"euroeval.human_evaluation","text":"euroeval.human_evaluation<p> source module euroeval.human_evaluation </p> <p>Gradio app for conducting human evaluation of the tasks.</p> <p> Classes </p> <ul> <li> <p>HumanEvaluator \u2014 An app for evaluating human performance on the EuroEval benchmark.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>main \u2014 Start the Gradio app for human evaluation.</p> </li> </ul> <p> source class HumanEvaluator(annotator_id: int, title: str, description: str, dummy_model_id: str = 'mistralai/Mistral-7B-v0.1') </p> <p>An app for evaluating human performance on the EuroEval benchmark.</p> <p>Initialize the HumanEvaluator.</p> <p> Parameters </p> <ul> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> <li> <p>title :  str \u2014</p> <p>The title of the app.</p> </li> <li> <p>description :  str \u2014</p> <p>The description of the app.</p> </li> <li> <p>dummy_model_id :  str \u2014</p> <p>The model ID to use for generating prompts.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>create_app \u2014 Create the Gradio app for human evaluation.</p> </li> <li> <p>update_dataset_choices \u2014 Update the dataset choices based on the selected language and task.</p> </li> <li> <p>update_dataset \u2014 Update the dataset based on a selected dataset name.</p> </li> <li> <p>add_entity_to_answer \u2014 Add an entity to the answer.</p> </li> <li> <p>reset_entities \u2014 Reset the entities in the answer.</p> </li> <li> <p>submit_answer \u2014 Submit an answer to the dataset.</p> </li> <li> <p>example_to_markdown \u2014 Convert an example to a Markdown string.</p> </li> <li> <p>compute_and_log_scores \u2014 Computes and logs the scores for the dataset.</p> </li> </ul> <p> source method HumanEvaluator.create_app() \u2192 gr.Blocks </p> <p>Create the Gradio app for human evaluation.</p> <p> Returns </p> <ul> <li> <p>gr.Blocks \u2014 The Gradio app for human evaluation.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset_choices(language: str | None, task: str | None) \u2192 Dropdown </p> <p>Update the dataset choices based on the selected language and task.</p> <p> Parameters </p> <ul> <li> <p>language :  str | None \u2014</p> <p>The language selected by the user.</p> </li> <li> <p>task :  str | None \u2014</p> <p>The task selected by the user.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Dropdown \u2014 A list of dataset names that match the selected language and task.</p> </li> </ul> <p> source method HumanEvaluator.update_dataset(dataset_name: str, iteration: int) \u2192 tuple[Markdown, Markdown, Dropdown, Textbox, Button, Button, Textbox, Button] </p> <p>Update the dataset based on a selected dataset name.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The dataset name selected by the user.</p> </li> <li> <p>iteration :  int \u2014</p> <p>The iteration index of the datasets to evaluate.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Markdown, Markdown, Dropdown, Textbox, Button, Button, Textbox, Button] \u2014 A tuple (task_examples, question, entity_type, entity, entity_add_button, entity_reset_button, answer, submit_button) for the selected dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method HumanEvaluator.add_entity_to_answer(question: str, entity_type: str, entity: str, answer: str) \u2192 tuple[Textbox, Textbox] </p> <p>Add an entity to the answer.</p> <p> Parameters </p> <ul> <li> <p>question :  str \u2014</p> <p>The current question.</p> </li> <li> <p>entity_type :  str \u2014</p> <p>The entity type selected by the user.</p> </li> <li> <p>entity :  str \u2014</p> <p>The entity provided by the user.</p> </li> <li> <p>answer :  str \u2014</p> <p>The current answer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Textbox, Textbox] \u2014 A tuple (entity, answer) with a (blank) entity and answer.</p> </li> </ul> <p> source method HumanEvaluator.reset_entities() \u2192 Textbox </p> <p>Reset the entities in the answer.</p> <p> Returns </p> <ul> <li> <p>Textbox \u2014 A blank answer.</p> </li> </ul> <p> source method HumanEvaluator.submit_answer(dataset_name: str, question: str, answer: str, annotator_id: int) \u2192 tuple[str, str] </p> <p>Submit an answer to the dataset.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>The name of the dataset.</p> </li> <li> <p>question :  str \u2014</p> <p>The question for the dataset.</p> </li> <li> <p>answer :  str \u2014</p> <p>The answer to the question.</p> </li> <li> <p>annotator_id :  int \u2014</p> <p>The annotator ID for the evaluation.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (question, answer), with <code>question</code> being the next question, and <code>answer</code> being an empty string.</p> </li> </ul> <p> source method HumanEvaluator.example_to_markdown(example: dict) \u2192 tuple[str, str] </p> <p>Convert an example to a Markdown string.</p> <p> Parameters </p> <ul> <li> <p>example :  dict \u2014</p> <p>The example to convert.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, str] \u2014 A tuple (task_examples, question) for the example.</p> </li> </ul> <p> source method HumanEvaluator.compute_and_log_scores() \u2192 None </p> <p>Computes and logs the scores for the dataset.</p> <p> source main(annotator_id: int) \u2192 None </p> <p>Start the Gradio app for human evaluation.</p> <p> Raises </p> <ul> <li> <p>NeedsExtraInstalled</p> </li> </ul>"},{"location":"src/euroeval/human_evaluation/","title":"euroeval.human_evaluation","text":"euroeval.human_evaluation<p> docs module euroeval.human_evaluation </p> <pre><code>\"\"\"Gradio app for conducting human evaluation of the tasks.\"\"\"\n\nimport importlib.util\nimport json\nimport logging\nfrom collections import defaultdict\nfrom functools import partial\nfrom pathlib import Path\n\nimport click\nfrom datasets import Dataset\n\nfrom .benchmark_config_factory import build_benchmark_config\nfrom .data_loading import load_data\nfrom .data_models import BenchmarkResult, GenerativeModelOutput\nfrom .dataset_configs import SPEED_CONFIG, get_all_dataset_configs\nfrom .enums import GenerativeType, TaskGroup\nfrom .exceptions import NeedsExtraInstalled\nfrom .scores import aggregate_scores\nfrom .task_utils import (\n    question_answering,\n    sequence_classification,\n    text_to_text,\n    token_classification,\n)\nfrom .tasks import NER\nfrom .types import ComputeMetricsFunction, ExtractLabelsFunction, ScoreDict\nfrom .utils import enforce_reproducibility\n\nif importlib.util.find_spec(\"gradio\") is not None:\n    import gradio as gr\n    from gradio.components import HTML, Button, Dropdown, Markdown, Textbox\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass HumanEvaluator:docs\n    \"\"\"An app for evaluating human performance on the EuroEval benchmark.\"\"\"\n\n    def __init__(\n        self,\n        annotator_id: int,\n        title: str,\n        description: str,\n        dummy_model_id: str = \"mistralai/Mistral-7B-v0.1\",\n    ) -&gt; None:\n        \"\"\"Initialize the HumanEvaluator.\n\n        Args:\n            annotator_id:\n                The annotator ID for the evaluation.\n            title:\n                The title of the app.\n            description:\n                The description of the app.\n            dummy_model_id:\n                The model ID to use for generating prompts.\n        \"\"\"\n        self.annotator_id = annotator_id\n        self.title = title\n        self.description = description\n        self.dummy_model_id = dummy_model_id\n\n        self.sample_idx: int\n        self.active_dataset: Dataset\n\n        self.dataset_configs = {\n            name: cfg\n            for name, cfg in get_all_dataset_configs().items()\n            if not cfg.unofficial\n        }\n        self.tasks = sorted(\n            {\n                cfg.task.name.replace(\"-\", \" \").title()\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n            }\n        )\n        self.languages = sorted(\n            {\n                language.name\n                for cfg in self.dataset_configs.values()\n                if cfg != SPEED_CONFIG\n                for language in cfg.languages\n                if language.name not in {\"Norwegian Bokm\u00e5l\", \"Norwegian Nynorsk\"}\n            }\n        )\n\n        self.extract_labels_from_generation: ExtractLabelsFunction\n        self.compute_metrics: ComputeMetricsFunction\n\n    def create_app(self) -&gt; \"gr.Blocks\":docs\n        \"\"\"Create the Gradio app for human evaluation.\n\n        Returns:\n            The Gradio app for human evaluation.\n        \"\"\"\n        with gr.Blocks(title=self.title, theme=gr.themes.Monochrome()) as app:\n            HTML(f\"&lt;center&gt;&lt;h1&gt;{self.title}&lt;/h1&gt;&lt;/center&gt;\")\n            Markdown(self.description)\n            with gr.Row(variant=\"panel\"):\n                language_dropdown = Dropdown(label=\"Language\", choices=self.languages)\n                task_dropdown = Dropdown(label=\"Task\", choices=self.tasks)\n                dataset_dropdown = Dropdown(label=\"Dataset\", choices=[\"\"])\n            with gr.Row(variant=\"panel\"):\n                with gr.Column():\n                    task_examples = Markdown(\"Task Examples\", visible=False)\n                with gr.Column():\n                    question = Markdown(label=\"Question\", visible=False)\n                    with gr.Row():\n                        ner_tag_dropdown = Dropdown(\n                            label=\"Entity type\",\n                            choices=[\"\"],\n                            interactive=True,\n                            visible=False,\n                            scale=0.5,  # type: ignore[arg-type]\n                        )\n                        ner_tag_answer = Textbox(\n                            label=\"Entity\", interactive=True, visible=False, scale=1\n                        )\n                        with gr.Column(scale=0.2):  # type: ignore[arg-type]\n                            ner_tag_add_button = Button(\"Add entity\", visible=False)\n                            ner_tag_reset_button = Button(\n                                \"Reset entities\", visible=False\n                            )\n                    answer = Textbox(label=\"Answer\", visible=False)\n                    submit_button = Button(\"Submit\", visible=False)\n\n            language_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            task_dropdown.change(\n                fn=self.update_dataset_choices,\n                inputs=[language_dropdown, task_dropdown],\n                outputs=dataset_dropdown,\n            )\n            dataset_dropdown.change(\n                fn=partial(self.update_dataset, iteration=self.annotator_id),\n                inputs=dataset_dropdown,\n                outputs=[\n                    task_examples,\n                    question,\n                    ner_tag_dropdown,\n                    ner_tag_answer,\n                    ner_tag_add_button,\n                    ner_tag_reset_button,\n                    answer,\n                    submit_button,\n                ],\n            )\n            ner_tag_add_button.click(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_answer.submit(\n                fn=self.add_entity_to_answer,\n                inputs=[question, ner_tag_dropdown, ner_tag_answer, answer],\n                outputs=[ner_tag_answer, answer],\n            )\n            ner_tag_reset_button.click(fn=self.reset_entities, outputs=answer)\n            submit_button.click(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n            answer.submit(\n                fn=partial(self.submit_answer, annotator_id=self.annotator_id),\n                inputs=[dataset_dropdown, question, answer],\n                outputs=[question, answer],\n            )\n        return app\n\n    def update_dataset_choices(docs\n        self, language: str | None, task: str | None\n    ) -&gt; \"Dropdown\":\n        \"\"\"Update the dataset choices based on the selected language and task.\n\n        Args:\n            language:\n                The language selected by the user.\n            task:\n                The task selected by the user.\n\n        Returns:\n            A list of dataset names that match the selected language and task.\n        \"\"\"\n        if language is None or task is None:\n            return Dropdown(choices=[])\n\n        dataset_configs = [\n            cfg\n            for cfg in get_all_dataset_configs().values()\n            if language in {language.name for language in cfg.languages}\n            and task.lower().replace(\" \", \"-\") == cfg.task.name\n            and not cfg.unofficial\n        ]\n        assert len(dataset_configs) &gt; 0\n\n        choices = sorted([cfg.name for cfg in dataset_configs])\n\n        logger.info(\n            f\"User selected {language} and {task}, which resulted in the datasets \"\n            f\"{choices}, with {choices[0]!r} being chosen by default.\"\n        )\n\n        return Dropdown(choices=choices, value=choices[0])\n\n    def update_dataset(docs\n        self, dataset_name: str, iteration: int\n    ) -&gt; (\n        \"tuple[Markdown, Markdown, Dropdown, Textbox, Button, Button, Textbox, Button]\"\n    ):\n        \"\"\"Update the dataset based on a selected dataset name.\n\n        Args:\n            dataset_name:\n                The dataset name selected by the user.\n            iteration:\n                The iteration index of the datasets to evaluate.\n\n        Returns:\n            A tuple (task_examples, question, entity_type, entity, entity_add_button,\n            entity_reset_button, answer, submit_button) for the selected dataset.\n        \"\"\"\n        blank_answer = (\n            Markdown(\"\", visible=False),\n            Markdown(\"\", visible=False),\n            Dropdown(visible=False),\n            Textbox(visible=False),\n            Button(visible=False),\n            Button(visible=False),\n            Textbox(\"\", visible=False),\n            Button(visible=False),\n        )\n\n        if not dataset_name:\n            return blank_answer\n\n        logger.info(f\"User selected dataset {dataset_name} - loading dataset...\")\n        gr.Info(f\"Loading dataset {dataset_name}...\")\n\n        benchmark_config = build_benchmark_config(\n            progress_bar=False,\n            save_results=True,\n            task=None,\n            dataset=None,\n            language=[\n                language.code\n                for cfg in get_all_dataset_configs().values()\n                for language in cfg.languages\n                if not cfg.unofficial\n            ],\n            model_language=None,\n            dataset_language=None,\n            device=None,\n            batch_size=1,\n            raise_errors=False,\n            cache_dir=\".euroeval_cache\",\n            api_key=None,\n            force=False,\n            verbose=False,\n            trust_remote_code=False,\n            use_flash_attention=None,\n            clear_model_cache=False,\n            evaluate_test_split=False,\n            few_shot=True,\n            num_iterations=iteration + 1,\n            api_base=None,\n            api_version=None,\n            debug=False,\n            run_with_cli=True,\n            only_allow_safetensors=False,\n        )\n        self.dataset_config = get_all_dataset_configs()[dataset_name]\n\n        # TODO: Is this needed?\n        # model_id = f\"human-{iteration}\"\n        # model_config = ModelConfig(\n        #     model_id=model_id,\n        #     revision=\"main\",\n        #     task=\"text-generation\",\n        #     languages=dataset_config.languages,\n        #     model_type=ModelType.HUMAN,\n        #     model_cache_dir=create_model_cache_dir(\n        #         cache_dir=benchmark_config.cache_dir, model_id=model_id\n        #     ),\n        #     adapter_base_model_id=None,\n        # )\n\n        self.sample_idx = 0\n\n        dataset_path = (\n            Path(\".euroeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{iteration}.csv\"\n        )\n        if dataset_path.exists():\n            active_dataset = Dataset.from_csv(str(dataset_path))\n            assert isinstance(active_dataset, Dataset)\n            self.active_dataset = active_dataset\n            try:\n                while self.active_dataset[\"answer\"][self.sample_idx] is not None:\n                    self.sample_idx += 1\n            except IndexError:\n                self.compute_and_log_scores()\n                return blank_answer\n        else:\n            rng = enforce_reproducibility()\n            datasets = load_data(\n                rng=rng,\n                dataset_config=self.dataset_config,\n                benchmark_config=benchmark_config,\n            )\n            # TODO: Prepare data?\n            self.active_dataset = (\n                datasets[iteration][\"test\"]\n                .remove_columns(\n                    column_names=[\"input_ids\", \"attention_mask\"],\n                    new_fingerprint=datasets[iteration][\"test\"]._fingerprint,\n                )\n                .add_column(\n                    name=\"answer\",\n                    column=[None] * len(datasets[iteration][\"test\"]),\n                    new_fingerprint=datasets[iteration][\"test\"]._fingerprint,\n                )\n            )\n            if self.dataset_config.task == NER:\n                labels_in_train: set[str] = {\n                    tag\n                    for tag_list in self.active_dataset[\"labels\"]\n                    for tag in tag_list\n                }\n                self.has_misc_tags = (\n                    \"B-MISC\" in labels_in_train or \"I-MISC\" in labels_in_train\n                )\n\n        match self.dataset_config.task.task_group:\n            case TaskGroup.SEQUENCE_CLASSIFICATION:\n                self.compute_metrics = partial(\n                    sequence_classification.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = partial(\n                    sequence_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.TEXT_TO_TEXT:\n                self.compute_metrics = partial(\n                    text_to_text.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = (\n                    text_to_text.extract_labels_from_generation\n                )\n            case TaskGroup.TOKEN_CLASSIFICATION:\n                self.compute_metrics = partial(\n                    token_classification.compute_metrics,\n                    has_misc_tags=self.has_misc_tags,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = partial(\n                    token_classification.extract_labels_from_generation,\n                    dataset_config=self.dataset_config,\n                )\n            case TaskGroup.QUESTION_ANSWERING:\n                self.compute_metrics = partial(\n                    question_answering.compute_metrics,\n                    dataset_config=self.dataset_config,\n                    benchmark_config=benchmark_config,\n                )\n                self.extract_labels_from_generation = (\n                    question_answering.extract_labels_from_generation\n                )\n            case TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION:\n                raise NotImplementedError\n            case _:\n                raise NotImplementedError(\n                    f\"Task group {self.dataset_config.task.task_group} is not \"\n                    \"supported.\"\n                )\n\n        task_examples, question = self.example_to_markdown(\n            example=self.active_dataset[self.sample_idx]\n        )\n\n        logger.info(\n            f\"Loaded dataset {dataset_name}, with the following task examples:\\n\\n\"\n            f\"{task_examples}\"\n        )\n\n        if self.dataset_config.task == NER:\n            ner_tags = list()\n            for ner_tag in self.dataset_config.prompt_label_mapping.values():\n                if ner_tag not in ner_tags:\n                    ner_tags.append(ner_tag)\n            return (\n                Markdown(task_examples, visible=True),\n                Markdown(question, visible=True),\n                Dropdown(\n                    label=\"Entity type\",\n                    choices=ner_tags,\n                    value=ner_tags[0],\n                    visible=True,\n                ),\n                Textbox(label=\"Entity\", interactive=True, visible=True),\n                Button(\"Add entity\", visible=True),\n                Button(\"Reset entities\", visible=True),\n                Textbox(\n                    json.dumps({ner_tag: [] for ner_tag in ner_tags}),\n                    interactive=False,\n                    visible=True,\n                ),\n                Button(\"Submit\", visible=True),\n            )\n        else:\n            return (\n                Markdown(task_examples, visible=True),\n                Markdown(question, visible=True),\n                Dropdown(label=\"Entity type\", choices=[], visible=False),\n                Textbox(label=\"Entity\", interactive=True, visible=False),\n                Button(\"Add entity\", visible=False),\n                Button(\"Reset entities\", visible=False),\n                Textbox(\"\", interactive=True, visible=True),\n                Button(\"Submit\", visible=True),\n            )\n\n    def add_entity_to_answer(docs\n        self, question: str, entity_type: str, entity: str, answer: str\n    ) -&gt; \"tuple[Textbox, Textbox]\":\n        \"\"\"Add an entity to the answer.\n\n        Args:\n            question:\n                The current question.\n            entity_type:\n                The entity type selected by the user.\n            entity:\n                The entity provided by the user.\n            answer:\n                The current answer.\n\n        Returns:\n            A tuple (entity, answer) with a (blank) entity and answer.\n        \"\"\"\n        if not entity_type or not entity:\n            return Textbox(\"\"), Textbox(\"\")\n\n        if entity not in question:\n            gr.Warning(\n                f\"The entity {entity!r} is not present in the question. Please \"\n                \"write it *exactly* as it appears in the question.\"\n            )\n            return Textbox(entity), Textbox(answer)\n\n        current_answer_obj = json.loads(answer)\n        if entity not in current_answer_obj[entity_type]:\n            current_answer_obj[entity_type].append(entity)\n\n        answer = json.dumps(current_answer_obj)\n        return Textbox(\"\"), Textbox(answer)\n\n    def reset_entities(self) -&gt; \"Textbox\":docs\n        \"\"\"Reset the entities in the answer.\n\n        Returns:\n            A blank answer.\n        \"\"\"\n        ner_tags = list()\n        for ner_tag in self.dataset_config.prompt_label_mapping.values():\n            if ner_tag not in ner_tags:\n                ner_tags.append(ner_tag)\n        return Textbox(json.dumps({ner_tag: [] for ner_tag in ner_tags}))\n\n    def submit_answer(docs\n        self, dataset_name: str, question: str, answer: str, annotator_id: int\n    ) -&gt; tuple[str, str]:\n        \"\"\"Submit an answer to the dataset.\n\n        Args:\n            dataset_name:\n                The name of the dataset.\n            question:\n                The question for the dataset.\n            answer:\n                The answer to the question.\n            annotator_id:\n                The annotator ID for the evaluation.\n\n        Returns:\n            A tuple (question, answer), with `question` being the next question, and\n            `answer` being an empty string.\n        \"\"\"\n        if not answer:\n            gr.Warning(\"Please provide an answer before submitting.\")\n            logger.info(\"User tried to submit without providing an answer.\")\n            return question, answer\n\n        # Custom NER validation\n        if self.dataset_config.task == NER:\n            try:\n                json.loads(answer)\n            except json.JSONDecodeError:\n                gr.Warning(\"Please provide a valid JSON object as an answer.\")\n                logger.info(\"User tried to submit an invalid JSON object as an answer.\")\n                return question, answer\n\n            if not isinstance(json.loads(answer), dict):\n                gr.Warning(\n                    \"Please provide a JSON object with a dictionary as an answer.\"\n                )\n                logger.info(\n                    \"User tried to submit a JSON object without a dictionary as an \"\n                    \"answer.\"\n                )\n                return question, answer\n\n            ner_tags = list(self.dataset_config.prompt_label_mapping.values())\n            for ner_tag in ner_tags:\n                if ner_tag not in json.loads(answer).keys():\n                    gr.Warning(\n                        f\"Please provide a JSON object with the key {ner_tag!r}.\"\n                    )\n                    logger.info(\n                        \"User tried to submit a JSON object without the key \"\n                        f\"{ner_tag!r}.\"\n                    )\n                    return question, answer\n\n        samples_left = len(self.active_dataset) - self.sample_idx - 1\n        if samples_left:\n            gr.Info(f\"Submitted - {samples_left} to go!\")\n\n        # Store the user's answer\n        answers = self.active_dataset[\"answer\"]\n        answers[self.sample_idx] = answer\n        self.active_dataset = self.active_dataset.remove_columns(\n            column_names=[\"answer\"], new_fingerprint=self.active_dataset._fingerprint\n        ).add_column(\n            name=\"answer\",\n            column=answers,\n            new_fingerprint=self.active_dataset._fingerprint,\n        )\n        logger.info(\n            f\"User submitted the answer {answer!r} to the question {question!r}, with \"\n            f\"sample index {self.sample_idx}.\"\n        )\n\n        dataset_path = (\n            Path(\".euroeval_cache\")\n            / \"human-evaluation\"\n            / dataset_name\n            / f\"human-{annotator_id}.csv\"\n        )\n        dataset_path.parent.mkdir(parents=True, exist_ok=True)\n        self.active_dataset.to_csv(dataset_path)\n\n        # Attempt to get the next question\n        try:\n            self.sample_idx += 1\n            _, question = self.example_to_markdown(\n                example=self.active_dataset[self.sample_idx]\n            )\n\n            if self.dataset_config.task == NER:\n                ner_tags = list()\n                for ner_tag in self.dataset_config.prompt_label_mapping.values():\n                    if ner_tag not in ner_tags:\n                        ner_tags.append(ner_tag)\n                answer = json.dumps({ner_tag: [] for ner_tag in ner_tags})\n            else:\n                answer = \"\"\n\n        # If we fail to get the next question it means that the user has finished\n        # annotating the dataset, so we compute and log the scores\n        except IndexError:\n            self.compute_and_log_scores()\n            question = \"\"\n            answer = \"\"\n\n        return question, answer\n\n    def example_to_markdown(self, example: dict) -&gt; tuple[str, str]:docs\n        \"\"\"Convert an example to a Markdown string.\n\n        Args:\n            example:\n                The example to convert.\n\n        Returns:\n            A tuple (task_examples, question) for the example.\n        \"\"\"\n        task_examples: str | list[str] = [\n            sample.replace(\"\\n\", \"\\n\\n\")\n            for sample in example[\"text\"].split(\"\\n\\n\")[:-1]\n        ]\n        task_examples = \"\\n\\n**Example**\\n\\n\".join(task_examples)\n\n        question = \"**Question**\\n\\n\"\n        question += \"\\n\\n\".join(example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[:-1])\n        question += \"\\n\\n\" + example[\"text\"].split(\"\\n\\n\")[-1].split(\"\\n\")[-1]\n\n        return task_examples, question\n\n    def compute_and_log_scores(self) -&gt; None:docs\n        \"\"\"Computes and logs the scores for the dataset.\"\"\"\n        model_output = GenerativeModelOutput(sequences=self.active_dataset[\"answer\"])\n\n        active_dataset_dict = self.active_dataset.to_dict()\n        assert isinstance(active_dataset_dict, dict)\n\n        all_preds = self.extract_labels_from_generation(\n            input_batch=active_dataset_dict, model_output=model_output\n        )\n        ground_truth = self.active_dataset[\"label\"]\n        itr_scores: dict[str, float] = self.compute_metrics(\n            model_outputs_and_labels=(all_preds, ground_truth)\n        )\n\n        # We reverse the order, as the Info messages are printed in reverse order\n        scores = list(itr_scores.items())\n        scores.reverse()\n        gr.Info(\n            \"If you want to evaluate another dataset then please select a new \"\n            \"one from the menus.\"\n        )\n        for metric_name, score in scores:\n            gr.Info(f\"\\n\\n{metric_name}: {score:.2%}\")\n        gr.Info(\"You have completed this dataset! Here are your scores:\")\n        logger.info(\n            f\"User completed the dataset {self.dataset_config.name!r}\"\n            f\", with the following scores: {itr_scores}\"\n        )\n\n        # Load previous human results, if any. We do this since the human evaluation is\n        # only a single iteration, so the results from the current annotation should be\n        # added to the previous results.\n        results_path = Path.cwd() / \"euroeval_benchmark_results.jsonl\"\n        results: ScoreDict = defaultdict(list)\n        if results_path.exists():\n            all_results = [\n                json.loads(line.strip())\n                for line in results_path.read_text().strip().split(\"\\n\")\n                if line.strip()\n            ]\n            human_result_candidates = [\n                result\n                for result in all_results\n                if result[\"model\"] == \"human\"\n                and result[\"dataset\"] == self.dataset_config.name\n            ]\n            if human_result_candidates:\n                results = human_result_candidates[0][\"results\"]\n\n        # Append to results\n        results[\"raw\"].append(  # type: ignore[union-attr]\n            {f\"test_{metric_name}\": score for metric_name, score in itr_scores.items()}\n        )\n\n        # Aggregate scores\n        total_dict: dict[str, float] = dict()\n        for metric_cfg in self.dataset_config.task.metrics:\n            test_score, test_se = aggregate_scores(\n                scores=results[\"raw\"],  # type: ignore[arg-type]\n                metric_config=metric_cfg,\n            )\n            test_score, _ = metric_cfg.postprocessing_fn(test_score)\n            test_se, _ = metric_cfg.postprocessing_fn(test_se)\n            total_dict[f\"test_{metric_cfg.name}\"] = test_score\n            total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n        results[\"total\"] = total_dict\n\n        benchmark_result = BenchmarkResult(\n            dataset=self.dataset_config.name,\n            task=self.dataset_config.task.name,\n            dataset_languages=[\n                language.code for language in self.dataset_config.languages\n            ],\n            model=\"human\",\n            results=results,\n            num_model_parameters=-1,\n            max_sequence_length=-1,\n            vocabulary_size=-1,\n            merge=False,\n            generative=True,\n            generative_type=GenerativeType.INSTRUCTION_TUNED,\n            few_shot=True,\n            validation_split=True,\n        )\n        benchmark_result.append_to_results(results_path=results_path)\n\n\n@click.command()\n@click.option(\n    \"--annotator-id\",\n    \"-id\",\n    type=int,\n    required=True,\n    help=\"\"\"The annotator ID to use for the evaluation. Needs to be between 0 and 10,\n    inclusive.\"\"\",\n)\ndef main(annotator_id: int) -&gt; None:docs\n    \"\"\"Start the Gradio app for human evaluation.\"\"\"\n    if importlib.util.find_spec(\"gradio\") is None:\n        raise NeedsExtraInstalled(extra=\"human_evaluation\")\n\n    evaluator = HumanEvaluator(\n        annotator_id=annotator_id,\n        title=\"EuroEval Human Evaluation\",\n        description=\"\"\"\n        In this app we will evaluate your performance on a variety of tasks, with the\n        goal of comparing human performance to language model performance.\n\n        When you select a language and a task then you will be given a brief\n        description of the task, as well as examples of how to solve it. Please read\n        through these examples before proceeding with the task.\n\n        Please do not use any additional aids (such as search engines) when completing\n        these tasks.\n\n        Note that several examples appear more than once - this is intentional, as it\n        allows us to compare your performance across multiple examples.\n\n        Note that the Enter key will also submit your answer!\n        \"\"\",\n    )\n    evaluator.create_app().queue().launch()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/euroeval/languages/","title":"euroeval.languages","text":"euroeval.languages<p> source module euroeval.languages </p> <p>List of languages and their ISO 639-1 codes.</p> <p>Taken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.</p> <p>Last updated 19 June 2022.</p> <p> Functions </p> <ul> <li> <p>get_all_languages \u2014 Get a list of all the languages.</p> </li> </ul> <p> source get_all_languages() \u2192 dict[str, Language] </p> <p>Get a list of all the languages.</p> <p> Returns </p> <ul> <li> <p>dict[str, Language] \u2014 A mapping between language codes and their configurations.</p> </li> </ul>"},{"location":"src/euroeval/languages/","title":"euroeval.languages","text":"euroeval.languages<p> docs module euroeval.languages </p> <pre><code>\"\"\"List of languages and their ISO 639-1 codes.\n\nTaken from https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes.\n\nLast updated 19 June 2022.\n\"\"\"\n\nfrom .data_models import Language\n\n\ndef get_all_languages() -&gt; dict[str, Language]:docs\n    \"\"\"Get a list of all the languages.\n\n    Returns:\n        A mapping between language codes and their configurations.\n    \"\"\"\n    return {cfg.code: cfg for cfg in globals().values() if isinstance(cfg, Language)}\n\n\nAB = Language(code=\"ab\", name=\"Abkhazian\")\nAA = Language(code=\"aa\", name=\"Afar\")\nAF = Language(code=\"af\", name=\"Afrikaans\")\nSQ = Language(code=\"sq\", name=\"Albanian\")\nAM = Language(code=\"am\", name=\"Amharic\")\nAR = Language(code=\"ar\", name=\"Arabic\")\nAN = Language(code=\"an\", name=\"Aragonese\")\nHY = Language(code=\"hy\", name=\"Armenian\")\nAS = Language(code=\"as\", name=\"Assamese\")\nAV = Language(code=\"av\", name=\"Avaric\")\nAE = Language(code=\"ae\", name=\"Avestan\")\nAY = Language(code=\"ay\", name=\"Aymara\")\nAZ = Language(code=\"az\", name=\"Azerbaijani\")\nBM = Language(code=\"bm\", name=\"Bambara\")\nBA = Language(code=\"ba\", name=\"Bashkir\")\nEU = Language(code=\"eu\", name=\"Basque\")\nBE = Language(code=\"be\", name=\"Belarusian\")\nBN = Language(code=\"bn\", name=\"Bengali\")\nBI = Language(code=\"bi\", name=\"Bislama\")\nBS = Language(code=\"bs\", name=\"Bosnian\")\nBR = Language(code=\"br\", name=\"Breton\")\nBG = Language(code=\"bg\", name=\"Bulgarian\")\nMY = Language(code=\"my\", name=\"Burmese\")\nCA = Language(code=\"ca\", name=\"Catalan\")\nCH = Language(code=\"ch\", name=\"Chamorro\")\nCE = Language(code=\"ce\", name=\"Chechen\")\nNY = Language(code=\"ny\", name=\"Chichewa\")\nZH = Language(code=\"zh\", name=\"Chinese\")\nCU = Language(code=\"cu\", name=\"Church Slavic\")\nCV = Language(code=\"cv\", name=\"Chuvash\")\nKW = Language(code=\"kw\", name=\"Cornish\")\nCO = Language(code=\"co\", name=\"Corsican\")\nCR = Language(code=\"cr\", name=\"Cree\")\nHR = Language(code=\"hr\", name=\"Croatian\")\nCS = Language(code=\"cs\", name=\"Czech\")\nDA = Language(code=\"da\", name=\"Danish\")\nDV = Language(code=\"dv\", name=\"Divehi\")\nNL = Language(code=\"nl\", name=\"Dutch\")\nDZ = Language(code=\"dz\", name=\"Dzongkha\")\nEN = Language(code=\"en\", name=\"English\")\nEO = Language(code=\"eo\", name=\"Esperanto\")\nET = Language(code=\"et\", name=\"Estonian\")\nEE = Language(code=\"ee\", name=\"Ewe\")\nFO = Language(code=\"fo\", name=\"Faroese\")\nFJ = Language(code=\"fj\", name=\"Fijian\")\nFI = Language(code=\"fi\", name=\"Finnish\")\nFR = Language(code=\"fr\", name=\"French\")\nFY = Language(code=\"fy\", name=\"Western Frisian\")\nFF = Language(code=\"ff\", name=\"Fulah\")\nGD = Language(code=\"gd\", name=\"Gaelic\")\nGL = Language(code=\"gl\", name=\"Galician\")\nLG = Language(code=\"lg\", name=\"Ganda\")\nKA = Language(code=\"ka\", name=\"Georgian\")\nDE = Language(code=\"de\", name=\"German\")\nEL = Language(code=\"el\", name=\"Greek\")\nKL = Language(code=\"kl\", name=\"Greenlandic\")\nGN = Language(code=\"gn\", name=\"Guarani\")\nGU = Language(code=\"gu\", name=\"Gujarati\")\nHT = Language(code=\"ht\", name=\"Haitian\")\nHA = Language(code=\"ha\", name=\"Hausa\")\nHE = Language(code=\"he\", name=\"Hebrew\")\nHZ = Language(code=\"hz\", name=\"Herero\")\nHI = Language(code=\"hi\", name=\"Hindi\")\nHO = Language(code=\"ho\", name=\"Hiri Motu\")\nHU = Language(code=\"hu\", name=\"Hungarian\")\nIS = Language(code=\"is\", name=\"Icelandic\")\nIO = Language(code=\"io\", name=\"Ido\")\nIG = Language(code=\"ig\", name=\"Igbo\")\nID = Language(code=\"id\", name=\"Indonesian\")\nIA = Language(code=\"ia\", name=\"Interlingua\")\nIE = Language(code=\"ie\", name=\"Interlingue\")\nIU = Language(code=\"iu\", name=\"Inuktitut\")\nIK = Language(code=\"ik\", name=\"Inupiaq\")\nGA = Language(code=\"ga\", name=\"Irish\")\nIT = Language(code=\"it\", name=\"Italian\")\nJA = Language(code=\"ja\", name=\"Japanese\")\nKN = Language(code=\"kn\", name=\"Kannada\")\nKR = Language(code=\"kr\", name=\"Kanuri\")\nKS = Language(code=\"ks\", name=\"Kashmiri\")\nKK = Language(code=\"kk\", name=\"Kazakh\")\nKM = Language(code=\"km\", name=\"Central Khmer\")\nKI = Language(code=\"ki\", name=\"Kikuyu\")\nRW = Language(code=\"rw\", name=\"Kinyarwanda\")\nKY = Language(code=\"ky\", name=\"Kirghiz\")\nKV = Language(code=\"kv\", name=\"Komi\")\nKG = Language(code=\"kg\", name=\"Kongo\")\nKO = Language(code=\"ko\", name=\"Korean\")\nKJ = Language(code=\"kj\", name=\"Kuanyama\")\nKU = Language(code=\"ku\", name=\"Kurdish\")\nLO = Language(code=\"lo\", name=\"Lao\")\nLA = Language(code=\"la\", name=\"Latin\")\nLV = Language(code=\"lv\", name=\"Latvian\")\nLI = Language(code=\"li\", name=\"Limburgan\")\nLN = Language(code=\"ln\", name=\"Lingala\")\nLT = Language(code=\"lt\", name=\"Lithuanian\")\nLU = Language(code=\"lu\", name=\"Luba-Katanga\")\nLB = Language(code=\"lb\", name=\"Luxembourgish\")\nMK = Language(code=\"mk\", name=\"Macedonian\")\nMG = Language(code=\"mg\", name=\"Malagasy\")\nMS = Language(code=\"ms\", name=\"Malay\")\nML = Language(code=\"ml\", name=\"Malayalam\")\nMT = Language(code=\"mt\", name=\"Maltese\")\nGV = Language(code=\"gv\", name=\"Manx\")\nMI = Language(code=\"mi\", name=\"Maori\")\nMR = Language(code=\"mr\", name=\"Marathi\")\nMH = Language(code=\"mh\", name=\"Marshallese\")\nMN = Language(code=\"mn\", name=\"Mongolian\")\nNA = Language(code=\"na\", name=\"Nauru\")\nNV = Language(code=\"nv\", name=\"Navajo\")\nND = Language(code=\"nd\", name=\"Northern Ndebele\")\nNR = Language(code=\"nr\", name=\"South Ndebele\")\nNG = Language(code=\"ng\", name=\"Ndonga\")\nNE = Language(code=\"ne\", name=\"Nepali\")\nNO = Language(code=\"no\", name=\"Norwegian\")\nNB = Language(code=\"nb\", name=\"Norwegian Bokm\u00e5l\")\nNN = Language(code=\"nn\", name=\"Norwegian Nynorsk\")\nII = Language(code=\"ii\", name=\"Sichuan Yi\")\nOC = Language(code=\"oc\", name=\"Occitan\")\nOJ = Language(code=\"oj\", name=\"Ojibwa\")\nOR = Language(code=\"or\", name=\"Oriya\")\nOM = Language(code=\"om\", name=\"Oromo\")\nOS = Language(code=\"os\", name=\"Ossetian\")\nPI = Language(code=\"pi\", name=\"Pali\")\nPS = Language(code=\"ps\", name=\"Pashto\")\nFA = Language(code=\"fa\", name=\"Persian\")\nPL = Language(code=\"pl\", name=\"Polish\")\nPT = Language(code=\"pt\", name=\"Portuguese\")\nPA = Language(code=\"pa\", name=\"Punjabi\")\nQU = Language(code=\"qu\", name=\"Quechua\")\nRO = Language(code=\"ro\", name=\"Romanian\")\nRM = Language(code=\"rm\", name=\"Romansh\")\nRN = Language(code=\"rn\", name=\"Rundi\")\nRU = Language(code=\"ru\", name=\"Russian\")\nSE = Language(code=\"se\", name=\"Northern Sami\")\nSM = Language(code=\"sm\", name=\"Samoan\")\nSG = Language(code=\"sg\", name=\"Sango\")\nSA = Language(code=\"sa\", name=\"Sanskrit\")\nSC = Language(code=\"sc\", name=\"Sardinian\")\nSR = Language(code=\"sr\", name=\"Serbian\")\nSN = Language(code=\"sn\", name=\"Shona\")\nSD = Language(code=\"sd\", name=\"Sindhi\")\nSI = Language(code=\"si\", name=\"Sinhala\")\nSK = Language(code=\"sk\", name=\"Slovak\")\nSL = Language(code=\"sl\", name=\"Slovenian\")\nSO = Language(code=\"so\", name=\"Somali\")\nST = Language(code=\"st\", name=\"Sotho\")\nES = Language(code=\"es\", name=\"Spanish\")\nSU = Language(code=\"su\", name=\"Sundanese\")\nSW = Language(code=\"sw\", name=\"Swahili\")\nSS = Language(code=\"ss\", name=\"Swati\")\nSV = Language(code=\"sv\", name=\"Swedish\")\nTL = Language(code=\"tl\", name=\"Tagalog\")\nTY = Language(code=\"ty\", name=\"Tahitian\")\nTG = Language(code=\"tg\", name=\"Tajik\")\nTA = Language(code=\"ta\", name=\"Tamil\")\nTT = Language(code=\"tt\", name=\"Tatar\")\nTE = Language(code=\"te\", name=\"Telugu\")\nTH = Language(code=\"th\", name=\"Thai\")\nBO = Language(code=\"bo\", name=\"Tibetan\")\nTI = Language(code=\"ti\", name=\"Tigrinya\")\nTO = Language(code=\"to\", name=\"Tonga\")\nTS = Language(code=\"ts\", name=\"Tsonga\")\nTN = Language(code=\"tn\", name=\"Tswana\")\nTR = Language(code=\"tr\", name=\"Turkish\")\nTK = Language(code=\"tk\", name=\"Turkmen\")\nTW = Language(code=\"tw\", name=\"Twi\")\nUG = Language(code=\"ug\", name=\"Uighur\")\nUK = Language(code=\"uk\", name=\"Ukrainian\")\nUR = Language(code=\"ur\", name=\"Urdu\")\nUZ = Language(code=\"uz\", name=\"Uzbek\")\nVE = Language(code=\"ve\", name=\"Venda\")\nVI = Language(code=\"vi\", name=\"Vietnamese\")\nVO = Language(code=\"vo\", name=\"Volap\u00fck\")\nWA = Language(code=\"wa\", name=\"Walloon\")\nCY = Language(code=\"cy\", name=\"Welsh\")\nWO = Language(code=\"wo\", name=\"Wolof\")\nXH = Language(code=\"xh\", name=\"Xhosa\")\nYI = Language(code=\"yi\", name=\"Yiddish\")\nYO = Language(code=\"yo\", name=\"Yoruba\")\nZA = Language(code=\"za\", name=\"Zhuang\")\nZU = Language(code=\"zu\", name=\"Zulu\")\n</code></pre>"},{"location":"api/euroeval/model_cache/","title":"euroeval.model_cache","text":"euroeval.model_cache<p> source module euroeval.model_cache </p> <p>ModelCache class for caching model outputs.</p> <p> Classes </p> <ul> <li> <p>ModelCache \u2014 A cache for model outputs.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>split_dataset_into_cached_and_non_cached \u2014 Split a dataset into a cached and non-cached part.</p> </li> <li> <p>load_cached_model_outputs \u2014 Load the cached model outputs.</p> </li> </ul> <p> source class ModelCache(model_cache_dir: Path, cache_name: str, max_generated_tokens: int) </p> <p>A cache for model outputs.</p> <p>Initialize the model output cache.</p> <p> Attributes </p> <ul> <li> <p>model_cache_dir \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_path \u2014</p> <p>The path to the cache file.</p> </li> <li> <p>cache \u2014</p> <p>The model output cache.</p> </li> <li> <p>max_generated_tokens \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>model_cache_dir :  Path \u2014</p> <p>The directory to store the cache in.</p> </li> <li> <p>cache_name :  str \u2014</p> <p>The name of the cache file.</p> </li> <li> <p>max_generated_tokens :  int \u2014</p> <p>The maximum number of tokens to generate for each example.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>load \u2014 Load the model output cache.</p> </li> <li> <p>save \u2014 Save the model output cache to disk.</p> </li> <li> <p>remove \u2014 Remove the cache from memory and delete it from disk.</p> </li> <li> <p>add_to_cache \u2014 Add the model input/output to the cache.</p> </li> </ul> <p> source method ModelCache.load() \u2192 None </p> <p>Load the model output cache.</p> <p> source method ModelCache.save() \u2192 None </p> <p>Save the model output cache to disk.</p> <p> source method ModelCache.remove() \u2192 None </p> <p>Remove the cache from memory and delete it from disk.</p> <p> source method ModelCache.add_to_cache(model_inputs: dict, model_output: GenerativeModelOutput) \u2192 None </p> <p>Add the model input/output to the cache.</p> <p> Parameters </p> <ul> <li> <p>model_inputs :  dict \u2014</p> <p>The model inputs.</p> </li> <li> <p>model_output :  GenerativeModelOutput \u2014</p> <p>The model output.</p> </li> </ul> <p> source split_dataset_into_cached_and_non_cached(dataset: Dataset, cache: ModelCache) \u2192 tuple[Dataset, Dataset] </p> <p>Split a dataset into a cached and non-cached part.</p> <p> Parameters </p> <ul> <li> <p>dataset :  Dataset \u2014</p> <p>The dataset to split.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[Dataset, Dataset] \u2014 The cached and non-cached parts of the dataset.</p> </li> </ul> <p> source load_cached_model_outputs(cached_dataset: Dataset, cache: ModelCache) \u2192 GenerativeModelOutput </p> <p>Load the cached model outputs.</p> <p> Parameters </p> <ul> <li> <p>cached_dataset :  Dataset \u2014</p> <p>The dataset containing the cached examples.</p> </li> <li> <p>cache :  ModelCache \u2014</p> <p>The model output cache.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>GenerativeModelOutput \u2014 The model output containing the cached sequences.</p> </li> </ul>"},{"location":"src/euroeval/model_cache/","title":"euroeval.model_cache","text":"euroeval.model_cache<p> docs module euroeval.model_cache </p> <pre><code>\"\"\"ModelCache class for caching model outputs.\"\"\"\n\nimport hashlib\nimport json\nimport logging\nimport sys\nimport typing as t\nfrom collections import defaultdict\nfrom dataclasses import asdict\n\nfrom tqdm.auto import tqdm\n\nfrom .data_models import GenerativeModelOutput, SingleGenerativeModelOutput\n\nif t.TYPE_CHECKING:\n    from pathlib import Path\n\n    from datasets import Dataset\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\nclass ModelCache:docs\n    \"\"\"A cache for model outputs.\n\n    Attributes:\n        model_cache_dir:\n            The directory to store the cache in.\n        cache_path:\n            The path to the cache file.\n        cache:\n            The model output cache.\n        max_generated_tokens:\n            The maximum number of tokens to generate for each example.\n    \"\"\"\n\n    def __init__(\n        self, model_cache_dir: \"Path\", cache_name: str, max_generated_tokens: int\n    ) -&gt; None:\n        \"\"\"Initialize the model output cache.\n\n        Args:\n            model_cache_dir:\n                The directory to store the cache in.\n            cache_name:\n                The name of the cache file.\n            max_generated_tokens:\n                The maximum number of tokens to generate for each example.\n        \"\"\"\n        self.model_cache_dir = model_cache_dir\n        self.model_cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_path = self.model_cache_dir / cache_name.replace(\"/\", \"--\")\n        self.max_generated_tokens = max_generated_tokens\n\n    def load(self) -&gt; None:docs\n        \"\"\"Load the model output cache.\"\"\"\n        if not self.cache_path.exists():\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dict(), f)\n\n        try:\n            with self.cache_path.open() as f:\n                json_cache = json.load(f)\n        except json.JSONDecodeError:\n            logger.warning(\n                f\"Failed to load the cache from {self.cache_path}. The cache will be \"\n                f\"re-initialised.\"\n            )\n            json_cache = dict()\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dict(), f)\n\n        cache: dict[str, SingleGenerativeModelOutput] = dict()\n        for key in json_cache:\n            cache[key] = SingleGenerativeModelOutput(**json_cache[key])\n\n        self.cache = cache\n\n    def save(self) -&gt; None:docs\n        \"\"\"Save the model output cache to disk.\"\"\"\n        dumpable_cache: dict[str, dict] = defaultdict(dict)\n        for key, value in self.cache.items():\n            dumpable_cache[key] = asdict(value)\n\n        try:\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dumpable_cache, f)\n        except KeyError:\n            logger.warning(\n                f\"Failed to load the cache from {self.cache_path}. The cache will be \"\n                f\"re-initialised.\"\n            )\n            self.cache = dict()\n            with self.cache_path.open(\"w\") as f:\n                json.dump(dict(), f)\n\n    def _hash_key(self, key: str | list[dict[str, str]]) -&gt; str:\n        \"\"\"Hash the key to use as an index in the cache.\n\n        Args:\n            key:\n                The key to hash.\n\n        Returns:\n            The hashed key.\n        \"\"\"\n        return hashlib.md5(string=str(key).encode()).hexdigest()\n\n    def __getitem__(\n        self, key: str | list[dict[str, str]]\n    ) -&gt; SingleGenerativeModelOutput:\n        \"\"\"Get an item from the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n\n        Returns:\n            The model output.\n        \"\"\"\n        hashed_key = self._hash_key(key=key)\n        return self.cache[hashed_key]\n\n    def __setitem__(\n        self, key: str | list[dict[str, str]], value: SingleGenerativeModelOutput\n    ) -&gt; None:\n        \"\"\"Set an item in the cache.\n\n        Args:\n            key:\n                The key to use to index the cache.\n            value:\n                The value to set in the cache.\n        \"\"\"\n        hashed_key = self._hash_key(key=key)\n        self.cache[hashed_key] = value\n\n    def remove(self) -&gt; None:docs\n        \"\"\"Remove the cache from memory and delete it from disk.\"\"\"\n        self.cache_path.unlink()\n        del self.cache\n\n    def __contains__(self, key: str | list[dict[str, str]]) -&gt; bool:\n        \"\"\"Check if a key is in the cache.\n\n        Args:\n            key:\n                The key to check.\n\n        Returns:\n            Whether the key is in the cache.\n        \"\"\"\n        hashed_key = self._hash_key(key=key)\n        return hashed_key in self.cache\n\n    def add_to_cache(docs\n        self, model_inputs: dict, model_output: GenerativeModelOutput\n    ) -&gt; None:\n        \"\"\"Add the model input/output to the cache.\n\n        Args:\n            model_inputs:\n                The model inputs.\n            model_output:\n                The model output.\n        \"\"\"\n        input_column = \"messages\" if \"messages\" in model_inputs else \"text\"\n        model_inputs = model_inputs[input_column]\n\n        # Store the generated sequences in the cache, one by one\n        with tqdm(\n            iterable=model_inputs,\n            desc=\"Caching model outputs\",\n            leave=False,\n            disable=hasattr(sys, \"_called_from_test\"),\n        ) as pbar:\n            for sample_idx, model_input in enumerate(pbar):\n                # Extract the scores from the model output, to be cached. We only store\n                # the indices of the top scores, to save space. Further, we only store\n                # the scores if the generated sequence is shorter than the maximum\n                # length\n                if model_output.scores is not None and self.max_generated_tokens &lt; 8:\n                    assert model_output.scores is not None\n                    scores = model_output.scores[sample_idx]\n                else:\n                    scores = None\n                self[model_input] = SingleGenerativeModelOutput(\n                    sequence=model_output.sequences[sample_idx], scores=scores\n                )\n\n\ndef split_dataset_into_cached_and_non_cached(docs\n    dataset: \"Dataset\", cache: ModelCache\n) -&gt; tuple[\"Dataset\", \"Dataset\"]:\n    \"\"\"Split a dataset into a cached and non-cached part.\n\n    Args:\n        dataset:\n            The dataset to split.\n        cache:\n            The model output cache.\n\n    Returns:\n        The cached and non-cached parts of the dataset.\n    \"\"\"\n    # Get the sample indices of the non-cached examples, which are unique with respect\n    # to the \"text\" column.\n    input_column = \"messages\" if \"messages\" in dataset.column_names else \"text\"\n    dataset_texts = dataset[input_column]\n    unique_non_cached_ids = set()\n    unique_texts = list()\n    for idx, dataset_text in enumerate(dataset_texts):\n        if dataset_text not in cache and dataset_text not in unique_texts:\n            unique_non_cached_ids.add(idx)\n            unique_texts.append(dataset_text)\n\n    # The cached examples are the ones that are not in the non-cached examples. This\n    # means that if the dataset has duplicates, only a single copy of the duplicate\n    # will be put in the non-cached part, and the rest in the cached part.\n    cached_ids = set(range(len(dataset))) - unique_non_cached_ids\n\n    cached = dataset.select(cached_ids)\n    non_cached = dataset.select(unique_non_cached_ids)\n    return cached, non_cached\n\n\ndef load_cached_model_outputs(docs\n    cached_dataset: \"Dataset\", cache: ModelCache\n) -&gt; GenerativeModelOutput:\n    \"\"\"Load the cached model outputs.\n\n    Args:\n        cached_dataset:\n            The dataset containing the cached examples.\n        cache:\n            The model output cache.\n\n    Returns:\n        The model output containing the cached sequences.\n    \"\"\"\n    input_column = \"messages\" if \"messages\" in cached_dataset.column_names else \"text\"\n    cached_model_outputs: list[SingleGenerativeModelOutput] = [\n        cache[prompt] for prompt in cached_dataset[input_column]\n    ]\n\n    cached_sequences = [model_output.sequence for model_output in cached_model_outputs]\n\n    if cached_model_outputs[0].scores is None:\n        return GenerativeModelOutput(sequences=cached_sequences)\n\n    cached_scores = [model_output.scores or [] for model_output in cached_model_outputs]\n    return GenerativeModelOutput(sequences=cached_sequences, scores=cached_scores)\n</code></pre>"},{"location":"api/euroeval/model_config/","title":"euroeval.model_config","text":"euroeval.model_config<p> source module euroeval.model_config </p> <p>Functions related to getting the model configuration.</p> <p> Functions </p> <ul> <li> <p>get_model_config \u2014 Fetches configuration for a model.</p> </li> </ul> <p> source get_model_config(model_id: str, benchmark_config: BenchmarkConfig) \u2192 ModelConfig </p> <p>Fetches configuration for a model.</p> <p> Parameters </p> <ul> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The configuration of the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ModelConfig \u2014 The model configuration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel \u2014</p> <p>If all model setups can handle the model, but the model does not exist.</p> </li> </ul>"},{"location":"src/euroeval/model_config/","title":"euroeval.model_config","text":"euroeval.model_config<p> docs module euroeval.model_config </p> <pre><code>\"\"\"Functions related to getting the model configuration.\"\"\"\n\nimport logging\nimport typing as t\n\nfrom . import benchmark_modules\nfrom .exceptions import InvalidModel, NeedsEnvironmentVariable, NeedsExtraInstalled\n\nif t.TYPE_CHECKING:\n    from .data_models import BenchmarkConfig, ModelConfig\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef get_model_config(docs\n    model_id: str, benchmark_config: \"BenchmarkConfig\"\n) -&gt; \"ModelConfig\":\n    \"\"\"Fetches configuration for a model.\n\n    Args:\n        model_id:\n            The model ID.\n        benchmark_config:\n            The configuration of the benchmark.\n\n    Returns:\n        The model configuration.\n\n    Raises:\n        InvalidModel:\n            If all model setups can handle the model, but the model does not exist.\n    \"\"\"\n    all_benchmark_modules = [\n        cls\n        for cls in benchmark_modules.__dict__.values()\n        if isinstance(cls, type)\n        and issubclass(cls, benchmark_modules.BenchmarkModule)\n        and cls is not benchmark_modules.BenchmarkModule\n    ]\n    all_benchmark_modules.sort(key=lambda cls: cls.high_priority, reverse=True)\n\n    needs_extras: list[str] = list()\n    needs_env_vars: list[str] = list()\n    for benchmark_module in all_benchmark_modules:\n        exists_or_err = benchmark_module.model_exists(\n            model_id=model_id, benchmark_config=benchmark_config\n        )\n        if isinstance(exists_or_err, NeedsExtraInstalled):\n            needs_extras.append(exists_or_err.extra)\n        elif isinstance(exists_or_err, NeedsEnvironmentVariable):\n            needs_env_vars.append(exists_or_err.env_var)\n        elif exists_or_err is True:\n            logger.debug(\n                f\"The model {model_id!r} was identified by the \"\n                f\"{benchmark_module.__name__} benchmark module.\"\n            )\n            model_config = benchmark_module.get_model_config(\n                model_id=model_id, benchmark_config=benchmark_config\n            )\n            return model_config\n    else:\n        msg = f\"Model {model_id} not found.\"\n        if needs_extras:\n            msg += (\n                \" However, it is possible that the model exists, but a package \"\n                \"needs to be installed to check if it exists. Please try running \"\n                f\"`pip install euroeval[{','.join(needs_extras)}]` or `pip install \"\n                \"euroeval[all]`, and try again.\"\n            )\n        elif needs_env_vars:\n            msg += (\n                \" However, it is possible that the model exists, but an environment \"\n                \"variable needs to be set to check if it exists. Please set the \"\n                f\"environment variables {','.join(needs_env_vars)} and try again.\"\n            )\n        raise InvalidModel(msg)\n</code></pre>"},{"location":"api/euroeval/model_loading/","title":"euroeval.model_loading","text":"euroeval.model_loading<p> source module euroeval.model_loading </p> <p>Functions related to the loading of models.</p> <p> Functions </p> <ul> <li> <p>load_model \u2014 Load a model.</p> </li> </ul> <p> source load_model(model_config: ModelConfig, dataset_config: DatasetConfig, benchmark_config: BenchmarkConfig) \u2192 BenchmarkModule </p> <p>Load a model.</p> <p> Parameters </p> <ul> <li> <p>model_config :  ModelConfig \u2014</p> <p>The model configuration.</p> </li> <li> <p>dataset_config :  DatasetConfig \u2014</p> <p>The dataset configuration.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>The benchmark configuration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BenchmarkModule \u2014 The model.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>InvalidModel</p> </li> </ul>"},{"location":"src/euroeval/model_loading/","title":"euroeval.model_loading","text":"euroeval.model_loading<p> docs module euroeval.model_loading </p> <pre><code>\"\"\"Functions related to the loading of models.\"\"\"\n\nimport typing as t\n\nfrom .benchmark_modules import (\n    FreshEncoderModel,\n    HuggingFaceEncoderModel,\n    LiteLLMModel,\n    VLLMModel,\n)\nfrom .constants import GENERATIVE_DATASET_TASK_GROUPS\nfrom .enums import InferenceBackend, ModelType\nfrom .exceptions import InvalidBenchmark, InvalidModel\n\nif t.TYPE_CHECKING:\n    from .benchmark_modules import BenchmarkModule\n    from .data_models import BenchmarkConfig, DatasetConfig, ModelConfig\n\n\ndef load_model(docs\n    model_config: \"ModelConfig\",\n    dataset_config: \"DatasetConfig\",\n    benchmark_config: \"BenchmarkConfig\",\n) -&gt; \"BenchmarkModule\":\n    \"\"\"Load a model.\n\n    Args:\n        model_config:\n            The model configuration.\n        dataset_config:\n            The dataset configuration.\n        benchmark_config:\n            The benchmark configuration.\n\n    Returns:\n        The model.\n    \"\"\"\n    # The order matters; the first model type that matches will be used. For this\n    # reason, they have been ordered in terms of the most common model types.\n    model_class: t.Type[BenchmarkModule]\n    match (model_config.model_type, model_config.inference_backend, model_config.fresh):\n        case (ModelType.GENERATIVE, InferenceBackend.VLLM, False):\n            model_class = VLLMModel\n        case (ModelType.ENCODER, InferenceBackend.TRANSFORMERS, False):\n            model_class = HuggingFaceEncoderModel\n        case (ModelType.GENERATIVE, InferenceBackend.LITELLM, False):\n            model_class = LiteLLMModel\n        case (ModelType.ENCODER, InferenceBackend.TRANSFORMERS, True):\n            model_class = FreshEncoderModel\n        case (_, _, True):\n            raise InvalidModel(\n                \"Cannot load a freshly initialised model with the model type \"\n                f\"{model_config.model_type!r} and inference backend \"\n                f\"{model_config.inference_backend!r}.\"\n            )\n        case _:\n            raise InvalidModel(\n                f\"Cannot load model with model type {model_config.model_type!r} and \"\n                f\"inference backend {model_config.inference_backend!r}.\"\n            )\n\n    # Refuse to benchmark non-generative models on generative tasks\n    if (\n        dataset_config.task.task_group in GENERATIVE_DATASET_TASK_GROUPS\n        and not model_config.model_type == ModelType.GENERATIVE\n    ):\n        raise InvalidBenchmark(\n            f\"Cannot benchmark non-generative model {model_config.model_id!r} on \"\n            f\"generative task {dataset_config.task.name!r}.\"\n        )\n\n    model = model_class(\n        model_config=model_config,\n        dataset_config=dataset_config,\n        benchmark_config=benchmark_config,\n    )\n\n    return model\n</code></pre>"},{"location":"api/euroeval/scores/","title":"euroeval.scores","text":"euroeval.scores<p> source module euroeval.scores </p> <p>Aggregation of raw scores into the mean and a confidence interval.</p> <p> Functions </p> <ul> <li> <p>log_scores \u2014 Log the scores.</p> </li> <li> <p>aggregate_scores \u2014 Helper function to compute the mean with confidence intervals.</p> </li> </ul> <p> source log_scores(dataset_name: str, metric_configs: list[MetricConfig], scores: list[dict[str, float]], model_id: str) \u2192 ScoreDict </p> <p>Log the scores.</p> <p> Parameters </p> <ul> <li> <p>dataset_name :  str \u2014</p> <p>Name of the dataset.</p> </li> <li> <p>metric_configs :  list[MetricConfig] \u2014</p> <p>List of metrics to log.</p> </li> <li> <p>scores :  list[dict[str, float]] \u2014</p> <p>The scores that are to be logged. This is a list of dictionaries full of scores.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The full Hugging Face Hub path to the pretrained transformer model.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>ScoreDict \u2014 A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being identical to <code>scores</code> and 'total' being a dictionary with the aggregated scores (means and standard errors).</p> </li> </ul> <p> source aggregate_scores(scores: list[dict[str, float]], metric_config: MetricConfig) \u2192 tuple[float, float] </p> <p>Helper function to compute the mean with confidence intervals.</p> <p> Parameters </p> <ul> <li> <p>scores :  list[dict[str, float]] \u2014</p> <p>Dictionary with the names of the metrics as keys, of the form \"_\", such as \"val_f1\", and values the metric values. <li> <p>metric_config :  MetricConfig \u2014</p> <p>The configuration of the metric, which is used to collect the correct metric from <code>scores</code>.</p> </li> <p> Returns </p> <ul> <li> <p>tuple[float, float] \u2014 A pair of floats, containing the score and the radius of its 95% confidence interval.</p> </li> </ul>"},{"location":"src/euroeval/scores/","title":"euroeval.scores","text":"euroeval.scores<p> docs module euroeval.scores </p> <pre><code>\"\"\"Aggregation of raw scores into the mean and a confidence interval.\"\"\"\n\nimport logging\nimport typing as t\nimport warnings\n\nimport numpy as np\n\nif t.TYPE_CHECKING:\n    from .data_models import MetricConfig\n    from .types import ScoreDict\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef log_scores(docs\n    dataset_name: str,\n    metric_configs: list[\"MetricConfig\"],\n    scores: list[dict[str, float]],\n    model_id: str,\n) -&gt; \"ScoreDict\":\n    \"\"\"Log the scores.\n\n    Args:\n        dataset_name:\n            Name of the dataset.\n        metric_configs:\n            List of metrics to log.\n        scores:\n            The scores that are to be logged. This is a list of dictionaries full of\n            scores.\n        model_id:\n            The full Hugging Face Hub path to the pretrained transformer model.\n\n    Returns:\n        A dictionary with keys 'raw_scores' and 'total', with 'raw_scores' being\n        identical to `scores` and 'total' being a dictionary with the aggregated scores\n        (means and standard errors).\n    \"\"\"\n    logger.info(f\"Finished evaluation of {model_id} on {dataset_name}.\")\n\n    total_dict: dict[str, float] = dict()\n    for metric_cfg in metric_configs:\n        test_score, test_se = aggregate_scores(scores=scores, metric_config=metric_cfg)\n        test_score, test_score_str = metric_cfg.postprocessing_fn(test_score)\n        test_se, test_se_str = metric_cfg.postprocessing_fn(test_se)\n        total_dict[f\"test_{metric_cfg.name}\"] = test_score\n        total_dict[f\"test_{metric_cfg.name}_se\"] = test_se\n        logger.info(f\"{metric_cfg.pretty_name}: {test_score_str} \u00b1 {test_se_str}\")\n\n    return dict(raw=scores, total=total_dict)\n\n\ndef aggregate_scores(docs\n    scores: list[dict[str, float]], metric_config: \"MetricConfig\"\n) -&gt; tuple[float, float]:\n    \"\"\"Helper function to compute the mean with confidence intervals.\n\n    Args:\n        scores:\n            Dictionary with the names of the metrics as keys, of the form\n            \"&lt;split&gt;_&lt;metric_name&gt;\", such as \"val_f1\", and values the metric values.\n        metric_config:\n            The configuration of the metric, which is used to collect the correct\n            metric from `scores`.\n\n    Returns:\n        A pair of floats, containing the score and the radius of its 95% confidence\n        interval.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        test_scores = [\n            (\n                dct[metric_config.name]\n                if metric_config.name in dct\n                else dct[f\"test_{metric_config.name}\"]\n            )\n            for dct in scores\n        ]\n        test_score = np.mean(test_scores).item()\n\n        if len(test_scores) &gt; 1:\n            sample_std = np.std(test_scores, ddof=1)\n            test_se = sample_std / np.sqrt(len(test_scores))\n        else:\n            test_se = np.nan\n\n        return (test_score, 1.96 * test_se)\n</code></pre>"},{"location":"api/euroeval/speed_benchmark/","title":"euroeval.speed_benchmark","text":"euroeval.speed_benchmark<p> source module euroeval.speed_benchmark </p> <p>Benchmarking model inference speed.</p> <p> Functions </p> <ul> <li> <p>benchmark_speed \u2014 Benchmark model inference speed.</p> </li> <li> <p>benchmark_speed_single_iteration \u2014 Run a single iteration of the speed benchmark.</p> </li> </ul> <p> source benchmark_speed(model: BenchmarkModule, benchmark_config: BenchmarkConfig) \u2192 list[dict[str, float]] </p> <p>Benchmark model inference speed.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>Model to use.</p> </li> <li> <p>benchmark_config :  BenchmarkConfig \u2014</p> <p>Configuration for the benchmark.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[dict[str, float]] \u2014 Dictionary of scores.</p> </li> </ul> <p> source benchmark_speed_single_iteration(model: BenchmarkModule, itr_idx: int) \u2192 dict[str, float] </p> <p>Run a single iteration of the speed benchmark.</p> <p> Parameters </p> <ul> <li> <p>model :  BenchmarkModule \u2014</p> <p>The model to use in the benchmark.</p> </li> <li> <p>itr_idx :  int \u2014</p> <p>The index of the iteration.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict[str, float] \u2014 A dictionary containing the scores for the current iteration.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidBenchmark</p> </li> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/euroeval/speed_benchmark/","title":"euroeval.speed_benchmark","text":"euroeval.speed_benchmark<p> docs module euroeval.speed_benchmark </p> <pre><code>\"\"\"Benchmarking model inference speed.\"\"\"\n\nimport logging\n\nimport pyinfer\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer\n\nfrom .benchmark_modules import (\n    BenchmarkModule,\n    HuggingFaceEncoderModel,\n    LiteLLMModel,\n    VLLMModel,\n)\nfrom .data_models import BenchmarkConfig\nfrom .exceptions import InvalidBenchmark\nfrom .utils import clear_memory\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef benchmark_speed(docs\n    model: \"BenchmarkModule\", benchmark_config: \"BenchmarkConfig\"\n) -&gt; list[dict[str, float]]:\n    \"\"\"Benchmark model inference speed.\n\n    Args:\n        model:\n            Model to use.\n        benchmark_config:\n            Configuration for the benchmark.\n\n    Returns:\n        Dictionary of scores.\n    \"\"\"\n    scores: list[dict[str, float]] = list()\n    for idx in tqdm(\n        iterable=range(benchmark_config.num_iterations),\n        desc=\"Benchmarking\",\n        disable=not benchmark_config.progress_bar,\n    ):\n        itr_scores = benchmark_speed_single_iteration(model=model, itr_idx=idx)\n        clear_memory()\n        scores.append(itr_scores)\n        logger.debug(f\"Scores for iteration {idx}: {itr_scores}\")\n    return scores\n\n\ndef benchmark_speed_single_iteration(docs\n    model: \"BenchmarkModule\", itr_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Run a single iteration of the speed benchmark.\n\n    Args:\n        model:\n            The model to use in the benchmark.\n        itr_idx:\n            The index of the iteration.\n\n    Returns:\n        A dictionary containing the scores for the current iteration.\n    \"\"\"\n    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n\n    base_doc = \"Document which contains roughly 10 tokens. \"\n    multiplier = 10 * (1 + itr_idx)\n    doc = base_doc * multiplier\n    short_multiplier = 1.25 * (1 + itr_idx)\n    short_doc = base_doc * round(short_multiplier)\n\n    def generate_messages_predict(doc: str) -&gt; None:\n        model.generate(inputs=dict(messages=[[dict(role=\"user\", content=doc)]]))\n\n    def generate_prompt_predict(doc: str) -&gt; None:\n        model.generate(inputs=dict(text=[doc]))\n\n    def encoder_predict(doc: str) -&gt; None:\n        tokenizer = model.get_tokenizer()\n        pytorch_model = model.get_pytorch_module()\n        inputs = {\n            key: tensor.to(pytorch_model.device)\n            for key, tensor in tokenizer(\n                text=[doc], truncation=True, return_tensors=\"pt\"\n            ).items()\n        }\n        pytorch_model(**inputs)\n\n    if isinstance(model, VLLMModel):\n        predict = generate_prompt_predict\n    elif isinstance(model, LiteLLMModel):\n        predict = generate_messages_predict\n    elif isinstance(model, HuggingFaceEncoderModel):\n        predict = encoder_predict\n    else:\n        raise ValueError(f\"Model type {model} not supported for speed benchmark\")\n\n    try:\n        # Do a warmup run, as the first run is always slower\n        pyinfer.InferenceReport(model=predict, inputs=base_doc, n_seconds=1).run(\n            print_report=False\n        )\n\n        speed_scores = pyinfer.InferenceReport(\n            model=predict, inputs=doc, n_seconds=3\n        ).run(print_report=False)\n        num_gpt2_tokens = len(gpt2_tokenizer([doc], truncation=True)[\"input_ids\"][0])\n        gpt2_tokens_per_second = speed_scores[\"Infer(p/sec)\"] * num_gpt2_tokens\n\n        speed_scores_short = pyinfer.InferenceReport(\n            model=predict, inputs=short_doc, n_seconds=3\n        ).run(print_report=False)\n        num_gpt2_tokens_short = len(\n            gpt2_tokenizer([short_doc], truncation=True)[\"input_ids\"][0]\n        )\n        gpt2_tokens_per_second_short = (\n            speed_scores_short[\"Infer(p/sec)\"] * num_gpt2_tokens_short\n        )\n\n    except (RuntimeError, ValueError, IndexError) as e:\n        raise InvalidBenchmark(f\"Speed benchmark failed with error: {e!r}\")\n\n    return dict(\n        test_speed=gpt2_tokens_per_second, test_speed_short=gpt2_tokens_per_second_short\n    )\n</code></pre>"},{"location":"api/euroeval/tasks/","title":"euroeval.tasks","text":"euroeval.tasks<p> source module euroeval.tasks </p> <p>All benchmarks tasks used in EuroEval.</p> <p> Functions </p> <ul> <li> <p>get_all_tasks \u2014 Get a list of all the dataset tasks.</p> </li> </ul> <p> source get_all_tasks() \u2192 dict[str, Task] </p> <p>Get a list of all the dataset tasks.</p> <p> Returns </p> <ul> <li> <p>dict[str, Task] \u2014 A mapping between names of dataset tasks and their configurations.</p> </li> </ul>"},{"location":"src/euroeval/tasks/","title":"euroeval.tasks","text":"euroeval.tasks<p> docs module euroeval.tasks </p> <pre><code>\"\"\"All benchmarks tasks used in EuroEval.\"\"\"\n\nfrom .data_models import MetricConfig, Task\nfrom .enums import TaskGroup\n\n\ndef get_all_tasks() -&gt; dict[str, Task]:docs\n    \"\"\"Get a list of all the dataset tasks.\n\n    Returns:\n        A mapping between names of dataset tasks and their configurations.\n    \"\"\"\n    return {cfg.name: cfg for cfg in globals().values() if isinstance(cfg, Task)}\n\n\nLA = Task(\n    name=\"linguistic-acceptability\",\n    task_group=TaskGroup.SEQUENCE_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n)\n\n\nNER = Task(\n    name=\"named-entity-recognition\",\n    task_group=TaskGroup.TOKEN_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"micro_f1_no_misc\",\n            pretty_name=\"Micro-average F1-score without MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n        MetricConfig(\n            name=\"micro_f1\",\n            pretty_name=\"Micro-average F1-score with MISC tags\",\n            huggingface_id=\"seqeval\",\n            results_key=\"overall_f1\",\n        ),\n    ],\n)\n\n\nRC = Task(\n    name=\"reading-comprehension\",\n    task_group=TaskGroup.QUESTION_ANSWERING,\n    metrics=[\n        MetricConfig(\n            name=\"f1\",\n            pretty_name=\"F1-score\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"f1\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n        MetricConfig(\n            name=\"em\",\n            pretty_name=\"Exact Match\",\n            huggingface_id=\"squad_v2\",\n            results_key=\"exact\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:.2f}%\"),\n        ),\n    ],\n)\n\n\nSENT = Task(\n    name=\"sentiment-classification\",\n    task_group=TaskGroup.SEQUENCE_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"macro_f1\",\n            pretty_name=\"Macro-average F1-score\",\n            huggingface_id=\"f1\",\n            results_key=\"f1\",\n            compute_kwargs=dict(average=\"macro\"),\n        ),\n    ],\n)\n\n\nSUMM = Task(\n    name=\"summarization\",\n    task_group=TaskGroup.TEXT_TO_TEXT,\n    metrics=[\n        MetricConfig(\n            name=\"bertscore\",\n            pretty_name=\"BERTScore\",\n            huggingface_id=\"bertscore\",\n            results_key=\"f1\",\n            compute_kwargs=dict(\n                model_type=\"microsoft/mdeberta-v3-base\", device=\"auto\", batch_size=32\n            ),\n        ),\n        MetricConfig(\n            name=\"rouge_l\",\n            pretty_name=\"ROUGE-L\",\n            huggingface_id=\"rouge\",\n            results_key=\"rougeL\",\n        ),\n    ],\n)\n\n\nKNOW = Task(\n    name=\"knowledge\",\n    task_group=TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n)\n\n\nMCRC = Task(\n    name=\"multiple-choice-reading-comprehension\",\n    task_group=TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n)\n\n\nCOMMON_SENSE = Task(\n    name=\"common-sense-reasoning\",\n    task_group=TaskGroup.MULTIPLE_CHOICE_CLASSIFICATION,\n    metrics=[\n        MetricConfig(\n            name=\"mcc\",\n            pretty_name=\"Matthew's Correlation Coefficient\",\n            huggingface_id=\"matthews_correlation\",\n            results_key=\"matthews_correlation\",\n        ),\n        MetricConfig(\n            name=\"accuracy\",\n            pretty_name=\"Accuracy\",\n            huggingface_id=\"accuracy\",\n            results_key=\"accuracy\",\n        ),\n    ],\n)\n\n\nSPEED = Task(\n    name=\"speed\",\n    task_group=TaskGroup.SPEED,\n    metrics=[\n        MetricConfig(\n            name=\"speed\",\n            pretty_name=\"Tokens per second\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n        MetricConfig(\n            name=\"speed_short\",\n            pretty_name=\"Tokens per second on short documents\",\n            huggingface_id=\"\",\n            results_key=\"speed\",\n            postprocessing_fn=lambda raw_score: (raw_score, f\"{raw_score:,.0f}\"),\n        ),\n    ],\n)\n</code></pre>"},{"location":"api/euroeval/types/","title":"euroeval.types","text":"euroeval.types<p> source module euroeval.types </p> <p>Types used throughout the project.</p> <p> Classes </p> <ul> <li> <p>ComputeMetricsFunction \u2014 A function used to compute the metrics.</p> </li> <li> <p>ExtractLabelsFunction \u2014 A function used to extract the labels from the generated output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>is_list_of_int \u2014 Check if an object is a list of integers.</p> </li> <li> <p>is_list_of_list_of_int \u2014 Check if an object is a list of list of integers.</p> </li> <li> <p>is_list_of_str \u2014 Check if an object is a list of integers.</p> </li> </ul> <p> source class ComputeMetricsFunction() </p> <p><p>Bases : t.Protocol</p></p> <p>A function used to compute the metrics.</p> <p> source class ExtractLabelsFunction() </p> <p><p>Bases : t.Protocol</p></p> <p>A function used to extract the labels from the generated output.</p> <p> source is_list_of_int(x: object) \u2192 t.TypeGuard[list[int]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[int]] \u2014 Whether the object is a list of integers.</p> </li> </ul> <p> source is_list_of_list_of_int(x: object) \u2192 t.TypeGuard[list[list[int]]] </p> <p>Check if an object is a list of list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[list[int]]] \u2014 Whether the object is a list of list of integers.</p> </li> </ul> <p> source is_list_of_str(x: object) \u2192 t.TypeGuard[list[str]] </p> <p>Check if an object is a list of integers.</p> <p> Parameters </p> <ul> <li> <p>x :  object \u2014</p> <p>The object to check.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.TypeGuard[list[str]] \u2014 Whether the object is a list of strings.</p> </li> </ul>"},{"location":"src/euroeval/types/","title":"euroeval.types","text":"euroeval.types<p> docs module euroeval.types </p> <pre><code>\"\"\"Types used throughout the project.\"\"\"\n\nimport typing as t\n\nfrom numpy.typing import NDArray\n\nif t.TYPE_CHECKING:\n    from .data_models import GenerativeModelOutput\n\n\nScoreDict = dict[str, dict[str, float] | list[dict[str, float]]]\nPredictions = NDArray | list[str] | list[list[str]]\nLabels = NDArray | list[str] | list[list[str]]\n\n\nclass ComputeMetricsFunction(t.Protocol):docs\n    \"\"\"A function used to compute the metrics.\"\"\"\n\n    def __call__(\n        self,\n        model_outputs_and_labels: tuple[\n            NDArray | list[str] | list[list[str]], NDArray | list[str] | list[list[str]]\n        ],\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics.\n\n        Args:\n            model_outputs_and_labels:\n                The model outputs and labels.\n\n        Returns:\n            The computed metrics.\n        \"\"\"\n        ...\n\n\nclass ExtractLabelsFunction(t.Protocol):docs\n    \"\"\"A function used to extract the labels from the generated output.\"\"\"\n\n    def __call__(\n        self, input_batch: dict[str, list], model_output: \"GenerativeModelOutput\"\n    ) -&gt; list[str]:\n        \"\"\"Extract the labels from the generated output.\n\n        Args:\n            input_batch:\n                The input batch.\n            model_output:\n                The model output.\n\n        Returns:\n            The extracted labels.\n        \"\"\"\n        ...\n\n\ndef is_list_of_int(x: object) -&gt; t.TypeGuard[list[int]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of integers.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, int) for i in x)\n\n\ndef is_list_of_list_of_int(x: object) -&gt; t.TypeGuard[list[list[int]]]:docs\n    \"\"\"Check if an object is a list of list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of list of integers.\n    \"\"\"\n    return (\n        isinstance(x, list)\n        and all(isinstance(i, list) for i in x)\n        and all(isinstance(j, int) for i in x for j in i)\n    )\n\n\ndef is_list_of_str(x: object) -&gt; t.TypeGuard[list[str]]:docs\n    \"\"\"Check if an object is a list of integers.\n\n    Args:\n        x:\n            The object to check.\n\n    Returns:\n        Whether the object is a list of strings.\n    \"\"\"\n    return isinstance(x, list) and all(isinstance(i, str) for i in x)\n</code></pre>"},{"location":"api/euroeval/utils/","title":"euroeval.utils","text":"euroeval.utils<p> source module euroeval.utils </p> <p>Utility functions to be used in other scripts.</p> <p> Classes </p> <ul> <li> <p>HiddenPrints \u2014 Context manager which removes all terminal output.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>create_model_cache_dir \u2014 Create cache directory for a model.</p> </li> <li> <p>clear_memory \u2014 Clears the memory of unused items.</p> </li> <li> <p>enforce_reproducibility \u2014 Ensures reproducibility of experiments.</p> </li> <li> <p>is_module_installed \u2014 Check if a module is installed.</p> </li> <li> <p>block_terminal_output \u2014 Blocks libraries from writing output to the terminal.</p> </li> <li> <p>get_class_by_name \u2014 Get a class by its name.</p> </li> <li> <p>kebab_to_pascal \u2014 Converts a kebab-case string to PascalCase.</p> </li> <li> <p>internet_connection_available \u2014 Checks if internet connection is available by pinging google.com.</p> </li> <li> <p>get_special_token_metadata \u2014 Get the special token metadata for a tokenizer.</p> </li> <li> <p>raise_if_model_output_contains_nan_values \u2014 Raise an exception if the model output contains NaN values.</p> </li> <li> <p>should_prompts_be_stripped \u2014 Determine if we should strip the prompts for few-shot evaluation.</p> </li> <li> <p>should_prefix_space_be_added_to_labels \u2014 Determine if we should add a prefix space to the labels.</p> </li> <li> <p>get_bos_token \u2014 Get the beginning-of-sequence token from a tokenizer.</p> </li> <li> <p>get_eos_token \u2014 Get the end-of-sequence token from a tokenizer.</p> </li> <li> <p>get_end_of_chat_token_ids \u2014 Get the end token ID for chat models.</p> </li> <li> <p>scramble \u2014 Scramble a string in a bijective manner.</p> </li> <li> <p>unscramble \u2014 Unscramble a string in a bijective manner.</p> </li> <li> <p>log_once \u2014 Log a message once.</p> </li> </ul> <p> source create_model_cache_dir(cache_dir: str, model_id: str) \u2192 str </p> <p>Create cache directory for a model.</p> <p> Parameters </p> <ul> <li> <p>cache_dir :  str \u2014</p> <p>The cache directory.</p> </li> <li> <p>model_id :  str \u2014</p> <p>The model ID.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The path to the cache directory.</p> </li> </ul> <p> source clear_memory() \u2192 None </p> <p>Clears the memory of unused items.</p> <p> source enforce_reproducibility(seed: int = 4242) \u2192 np.random.Generator </p> <p>Ensures reproducibility of experiments.</p> <p> Parameters </p> <ul> <li> <p>seed :  int \u2014</p> <p>Seed for the random number generator.</p> </li> </ul> <p> source is_module_installed(module: str) \u2192 bool </p> <p>Check if a module is installed.</p> <p>This is used when dealing with spaCy models, as these are installed as separate Python packages.</p> <p> Parameters </p> <ul> <li> <p>module :  str \u2014</p> <p>The name of the module.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether the module is installed or not.</p> </li> </ul> <p> source block_terminal_output() \u2192 None </p> <p>Blocks libraries from writing output to the terminal.</p> <p>This filters warnings from some libraries, sets the logging level to ERROR for some libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and disables most of the logging from the <code>transformers</code> library.</p> <p> source get_class_by_name(class_name: str | list[str], module_name: str) \u2192 t.Type | None </p> <p>Get a class by its name.</p> <p> Parameters </p> <ul> <li> <p>class_name :  str | list[str] \u2014</p> <p>The name of the class, written in kebab-case. The corresponding class name must be the same, but written in PascalCase, and lying in a module with the same name, but written in snake_case. If a list of strings is passed, the first class that is found is returned.</p> </li> <li> <p>module_name :  str \u2014</p> <p>The name of the module where the class is located.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>t.Type | None \u2014 The class. If the class is not found, None is returned.</p> </li> </ul> <p> source kebab_to_pascal(kebab_string: str) \u2192 str </p> <p>Converts a kebab-case string to PascalCase.</p> <p> Parameters </p> <ul> <li> <p>kebab_string :  str \u2014</p> <p>The kebab-case string.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The PascalCase string.</p> </li> </ul> <p> source internet_connection_available() \u2192 bool </p> <p>Checks if internet connection is available by pinging google.com.</p> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether or not internet connection is available.</p> </li> </ul> <p> source get_special_token_metadata(tokenizer: PreTrainedTokenizer) \u2192 dict </p> <p>Get the special token metadata for a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>dict \u2014 The special token metadata.</p> </li> </ul> <p> source class HiddenPrints() </p> <p>Context manager which removes all terminal output.</p> <p> source raise_if_model_output_contains_nan_values(model_output: Predictions) \u2192 None </p> <p>Raise an exception if the model output contains NaN values.</p> <p> Parameters </p> <ul> <li> <p>model_output :  Predictions \u2014</p> <p>The model output to check.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>If the model output contains NaN values.</p> </li> <li> <p>NaNValueInModelOutput</p> </li> </ul> <p> source should_prompts_be_stripped(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should strip the prompts for few-shot evaluation.</p> <p>This is the case if the tokenizer needs to include the space as part of the label token. The strategy is thus to tokenize a label with a preceeding colon (as in the prompts), i.e., \": positive\", and check if the tokenization starts with the tokens of \": \". If this is the case, then we should not strip the prompts, since the tokenizer produces the whitespace token separately.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should strip the prompts.</p> </li> </ul> <p> source should_prefix_space_be_added_to_labels(labels_to_be_generated: list[str], tokenizer: PreTrainedTokenizer) \u2192 bool </p> <p>Determine if we should add a prefix space to the labels.</p> <p>This is the case if the prompts are stripped and the tokenizer doesn't automatically add prefix whitespaces to the labels.</p> <p> Parameters </p> <ul> <li> <p>labels_to_be_generated :  list[str] \u2014</p> <p>The labels that are to be generated.</p> </li> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer used to tokenize the labels.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 Whether we should add a prefix space to the labels.</p> </li> </ul> <p> source get_bos_token(tokenizer: PreTrainedTokenizer) \u2192 tuple[str, int] </p> <p>Get the beginning-of-sequence token from a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, int] \u2014 A pair (token, token_id) representing the beginning-of-sequence token and its token ID.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_eos_token(tokenizer: PreTrainedTokenizer) \u2192 tuple[str, int] </p> <p>Get the end-of-sequence token from a tokenizer.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[str, int] \u2014 A pair (token, token_id) representing the end-of-sequence token and its token ID.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>InvalidModel</p> </li> </ul> <p> source get_end_of_chat_token_ids(tokenizer: PreTrainedTokenizer) \u2192 list[int] | None </p> <p>Get the end token ID for chat models.</p> <p>This is only relevant for tokenizers with a chat template.</p> <p> Parameters </p> <ul> <li> <p>tokenizer :  PreTrainedTokenizer \u2014</p> <p>The tokenizer.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>list[int] | None \u2014 The token IDs used to end chats, or None if the tokenizer does not have a chat template.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError \u2014</p> <p>If the end-of-chat token could not be located.</p> </li> </ul> <p> source scramble(text: str) \u2192 str </p> <p>Scramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>text :  str \u2014</p> <p>The string to scramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The scrambled string.</p> </li> </ul> <p> source unscramble(scrambled_text: str) \u2192 str </p> <p>Unscramble a string in a bijective manner.</p> <p> Parameters </p> <ul> <li> <p>scrambled_text :  str \u2014</p> <p>The scrambled string to unscramble.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>str \u2014 The unscrambled string.</p> </li> </ul> <p> source log_once(message: str, level: int = logging.INFO) \u2192 None </p> <p>Log a message once.</p> <p>This is ensured by caching the input/output pairs of this function, using the <code>functools.cache</code> decorator.</p> <p> Parameters </p> <ul> <li> <p>message :  str \u2014</p> <p>The message to log.</p> </li> <li> <p>level :  int \u2014</p> <p>The logging level. Defaults to logging.INFO.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/euroeval/utils/","title":"euroeval.utils","text":"euroeval.utils<p> docs module euroeval.utils </p> <pre><code>\"\"\"Utility functions to be used in other scripts.\"\"\"\n\nimport gc\nimport importlib\nimport importlib.util\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport typing as t\nimport warnings\nfrom functools import cache\nfrom pathlib import Path\nfrom types import TracebackType\n\nimport litellm\nimport numpy as np\nimport pkg_resources\nimport requests\nimport torch\nfrom datasets.utils import disable_progress_bar\nfrom requests.exceptions import RequestException\nfrom transformers import PreTrainedTokenizer\nfrom transformers import logging as tf_logging\n\nfrom .exceptions import InvalidModel, NaNValueInModelOutput\n\nif importlib.util.find_spec(\"ray\") is not None:\n    import ray\n\nif t.TYPE_CHECKING:\n    from .types import Predictions\n\n\nlogger = logging.getLogger(\"euroeval\")\n\n\ndef create_model_cache_dir(cache_dir: str, model_id: str) -&gt; str:docs\n    \"\"\"Create cache directory for a model.\n\n    Args:\n        cache_dir:\n            The cache directory.\n        model_id:\n            The model ID.\n\n    Returns:\n        The path to the cache directory.\n    \"\"\"\n    # to avoid nesting due to models name containing '/'\n    _model_id = model_id.replace(\"/\", \"--\")\n    cache_dir_path = Path(cache_dir) / \"model_cache\" / _model_id\n    return str(cache_dir_path)\n\n\ndef clear_memory() -&gt; None:docs\n    \"\"\"Clears the memory of unused items.\"\"\"\n    for gc_generation in range(3):\n        gc.collect(generation=gc_generation)\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\n\ndef enforce_reproducibility(seed: int = 4242) -&gt; np.random.Generator:docs\n    \"\"\"Ensures reproducibility of experiments.\n\n    Args:\n        seed:\n            Seed for the random number generator.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    rng = np.random.default_rng(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms(True, warn_only=True)\n    return rng\n\n\ndef is_module_installed(module: str) -&gt; bool:docs\n    \"\"\"Check if a module is installed.\n\n    This is used when dealing with spaCy models, as these are installed as separate\n    Python packages.\n\n    Args:\n        module:\n            The name of the module.\n\n    Returns:\n        Whether the module is installed or not.\n    \"\"\"\n    # Get list of all modules, including their versions\n    installed_modules_with_versions = list(pkg_resources.working_set)\n\n    # Strip the module versions from the list of modules. Also make the modules lower\n    # case and replace dashes with underscores\n    installed_modules = [\n        re.sub(\"[0-9. ]\", \"\", str(module)).lower().replace(\"-\", \"_\")\n        for module in installed_modules_with_versions\n    ]\n\n    # Check if the module is installed by checking if the module name is in the list\n    return module.lower() in installed_modules\n\n\ndef block_terminal_output() -&gt; None:docs\n    \"\"\"Blocks libraries from writing output to the terminal.\n\n    This filters warnings from some libraries, sets the logging level to ERROR for some\n    libraries, disabled tokeniser progress bars when using Hugging Face tokenisers, and\n    disables most of the logging from the `transformers` library.\n    \"\"\"\n    # Ignore miscellaneous warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\n        \"ignore\",\n        module=\"torch.nn.parallel*\",\n        message=\"Was asked to gather along dimension 0, but all input tensors were \"\n        \"scalars; will instead unsqueeze and return a vector.\",\n    )\n    warnings.filterwarnings(\"ignore\", module=\"seqeval*\")\n\n    # Up the logging level, to disable outputs\n    logging.getLogger(\"filelock\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"absl\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"datasets\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.distributed.distributed_c10d\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"torch.nn.parallel.distributed\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.engine.llm_engine\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.transformers_utils.tokenizer\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.core.scheduler\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"vllm.model_executor.weight_utils\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"ray._private.worker\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"LiteLLM\").setLevel(logging.CRITICAL)\n    logging.getLogger(\"huggingface_hub\").setLevel(logging.CRITICAL)\n\n    # This suppresses vLLM logging\n    os.environ[\"LOG_LEVEL\"] = \"CRITICAL\"\n    os.environ[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"\n\n    if importlib.util.find_spec(\"ray\") is not None:\n        ray._private.worker._worker_logs_enabled = False\n\n    # Disable the tokeniser progress bars\n    disable_progress_bar()\n\n    # Disable most of the `transformers` logging\n    tf_logging._default_log_level = logging.CRITICAL\n    tf_logging.set_verbosity(logging.CRITICAL)\n    logging.getLogger(\"transformers.trainer\").setLevel(logging.CRITICAL)\n\n    # Disable logging from `litellm`\n    litellm.suppress_debug_info = True\n\ndocs\ndef get_class_by_name(class_name: str | list[str], module_name: str) -&gt; t.Type | None:\n    \"\"\"Get a class by its name.\n\n    Args:\n        class_name:\n            The name of the class, written in kebab-case. The corresponding class name\n            must be the same, but written in PascalCase, and lying in a module with the\n            same name, but written in snake_case. If a list of strings is passed, the\n            first class that is found is returned.\n        module_name:\n            The name of the module where the class is located.\n\n    Returns:\n        The class. If the class is not found, None is returned.\n    \"\"\"\n    if isinstance(class_name, str):\n        class_name = [class_name]\n\n    error_messages = list()\n    for name in class_name:\n        try:\n            module = importlib.import_module(name=module_name)\n            class_: t.Type = getattr(module, name)\n            return class_\n        except (ModuleNotFoundError, AttributeError) as e:\n            error_messages.append(str(e))\n\n    if error_messages:\n        errors = \"\\n- \" + \"\\n- \".join(error_messages)\n        logger.debug(\n            f\"Could not find the class with the name(s) {', '.join(class_name)}. The \"\n            f\"following error messages were raised: {errors}\"\n        )\n\n    # If the class could not be found, return None\n    return None\n\n\ndef kebab_to_pascal(kebab_string: str) -&gt; str:docs\n    \"\"\"Converts a kebab-case string to PascalCase.\n\n    Args:\n        kebab_string:\n            The kebab-case string.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    return \"\".join(word.title() for word in kebab_string.split(\"-\"))\n\n\ndef internet_connection_available() -&gt; bool:docs\n    \"\"\"Checks if internet connection is available by pinging google.com.\n\n    Returns:\n        Whether or not internet connection is available.\n    \"\"\"\n    try:\n        requests.get(\"https://www.google.com\")\n        return True\n    except RequestException:\n        return False\n\ndocs\ndef get_special_token_metadata(tokenizer: \"PreTrainedTokenizer\") -&gt; dict:\n    \"\"\"Get the special token metadata for a tokenizer.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The special token metadata.\n    \"\"\"\n    # Create some test input IDs, to check if the tokenizer is adding special tokens\n    test_input_ids = tokenizer(\"Test\").input_ids\n\n    # Extract the CLS token IDs from the tokenizer, if it's using them\n    has_cls_token = True\n    if tokenizer.cls_token_id in test_input_ids:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n    elif tokenizer.bos_token_id in test_input_ids:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n    elif tokenizer.cls_token is not None:\n        cls_token_id = tokenizer.cls_token_id\n        cls_token = tokenizer.cls_token\n        has_cls_token = False\n    else:\n        cls_token_id = tokenizer.bos_token_id\n        cls_token = tokenizer.bos_token\n        has_cls_token = False\n\n    # Extract the SEP token IDs from the tokenizer, if it's using them\n    has_sep_token = True\n    if tokenizer.sep_token_id in test_input_ids:\n        sep_token = tokenizer.sep_token\n    elif tokenizer.eos_token_id in test_input_ids:\n        sep_token = tokenizer.eos_token\n    elif tokenizer.sep_token is not None:\n        sep_token = tokenizer.sep_token\n        has_sep_token = False\n    else:\n        sep_token = tokenizer.eos_token\n        has_sep_token = False\n\n    return dict(\n        cls_token_id=cls_token_id,\n        cls_token=cls_token,\n        sep_token=sep_token,\n        has_cls_token=has_cls_token,\n        has_sep_token=has_sep_token,\n    )\n\n\nclass HiddenPrints:docs\n    \"\"\"Context manager which removes all terminal output.\"\"\"\n\n    def __enter__(self) -&gt; None:\n        \"\"\"Enter the context manager.\"\"\"\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n\n    def __exit__(\n        self,\n        exc_type: t.Type[BaseException],\n        exc_val: BaseException,\n        exc_tb: TracebackType,\n    ) -&gt; None:\n        \"\"\"Exit the context manager.\"\"\"\n        sys.stdout.close()\n        sys.stderr.close()\n        sys.stdout = self._original_stdout\n        sys.stderr = self._original_stderr\n\ndocs\ndef raise_if_model_output_contains_nan_values(model_output: \"Predictions\") -&gt; None:\n    \"\"\"Raise an exception if the model output contains NaN values.\n\n    Args:\n        model_output:\n            The model output to check.\n\n    Raises:\n        If the model output contains NaN values.\n    \"\"\"\n    if isinstance(model_output, np.ndarray):\n        if model_output.dtype == np.float32 and np.isnan(model_output).any():\n            raise NaNValueInModelOutput()\n    elif len(model_output) &gt; 0:\n        if isinstance(model_output[0], str):\n            if any(x != x for x in model_output):\n                raise NaNValueInModelOutput()\n        elif len(model_output[0]) &gt; 0:\n            if any(x != x for sublist in model_output for x in sublist):\n                raise NaNValueInModelOutput()\n\n\ndef should_prompts_be_stripped(docs\n    labels_to_be_generated: list[str], tokenizer: \"PreTrainedTokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should strip the prompts for few-shot evaluation.\n\n    This is the case if the tokenizer needs to include the space as part of the label\n    token. The strategy is thus to tokenize a label with a preceeding colon (as in the\n    prompts), i.e., \": positive\", and check if the tokenization starts with the tokens\n    of \": \". If this is the case, then we should not strip the prompts, since the\n    tokenizer produces the whitespace token separately.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should strip the prompts.\n    \"\"\"\n    strip_prompts = True\n    for label in labels_to_be_generated:\n        colon_tokens = tokenizer(\": \", add_special_tokens=False).input_ids\n        label_tokens = tokenizer(\": \" + label, add_special_tokens=False).input_ids\n\n        if isinstance(colon_tokens, torch.Tensor):\n            colon_tokens = list(colon_tokens.squeeze(0))\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n\n        label_tokens_start_with_colon_tokens = (\n            label_tokens[: len(colon_tokens)] == colon_tokens\n        )\n        if label_tokens_start_with_colon_tokens:\n            strip_prompts = False\n\n    return strip_prompts\n\n\n# TODO: This is currently not used - maybe remove.\ndef should_prefix_space_be_added_to_labels(docs\n    labels_to_be_generated: list[str], tokenizer: \"PreTrainedTokenizer\"\n) -&gt; bool:\n    \"\"\"Determine if we should add a prefix space to the labels.\n\n    This is the case if the prompts are stripped and the tokenizer doesn't\n    automatically add prefix whitespaces to the labels.\n\n    Args:\n        labels_to_be_generated:\n            The labels that are to be generated.\n        tokenizer:\n            The tokenizer used to tokenize the labels.\n\n    Returns:\n        Whether we should add a prefix space to the labels.\n    \"\"\"\n    if not should_prompts_be_stripped(\n        labels_to_be_generated=labels_to_be_generated, tokenizer=tokenizer\n    ):\n        return False\n\n    whitespace_token = tokenizer.convert_ids_to_tokens(\n        ids=tokenizer(\" \", add_special_tokens=False).input_ids[0]\n    )[0]\n\n    add_prefix_space = True\n    for label in labels_to_be_generated:\n        label_tokens = tokenizer(label, add_special_tokens=False).input_ids\n        if isinstance(label_tokens, torch.Tensor):\n            label_tokens = list(label_tokens.squeeze(0))\n        first_label_token: int = int(label_tokens[0])\n        first_character_of_label = tokenizer.convert_ids_to_tokens(first_label_token)[0]\n        has_prefix_space = first_character_of_label == whitespace_token\n        if has_prefix_space:\n            add_prefix_space = False\n            break\n\n    return add_prefix_space\n\n\ndef get_bos_token(tokenizer: \"PreTrainedTokenizer\") -&gt; tuple[str, int]:docs\n    \"\"\"Get the beginning-of-sequence token from a tokenizer.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        A pair (token, token_id) representing the beginning-of-sequence token and its\n        token ID.\n    \"\"\"\n    if isinstance(tokenizer.bos_token, str) and isinstance(tokenizer.bos_token_id, int):\n        return tokenizer.bos_token, tokenizer.bos_token_id\n\n    vocab: dict[str, int] = tokenizer.get_vocab()\n\n    candidate_bos_tokens = [\"&lt;s&gt;\", \"&lt;|begin_of_text|&gt;\", \"[CLS]\"]\n    for candidate_bos_token in candidate_bos_tokens:\n        if candidate_bos_token in vocab:\n            bos_token = candidate_bos_token\n            bos_token_id = vocab[bos_token]\n            break\n    else:\n        raise InvalidModel(\n            \"The model does not have a beginning-of-sequence token. Please ensure that \"\n            \"this has been set in the tokenizer's configuration.\"\n        )\n\n    return bos_token, bos_token_id\n\n\ndef get_eos_token(tokenizer: \"PreTrainedTokenizer\") -&gt; tuple[str, int]:docs\n    \"\"\"Get the end-of-sequence token from a tokenizer.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        A pair (token, token_id) representing the end-of-sequence token and its token\n        ID.\n    \"\"\"\n    if isinstance(tokenizer.eos_token, str) and isinstance(tokenizer.eos_token_id, int):\n        return tokenizer.eos_token, tokenizer.eos_token_id\n\n    vocab: dict[str, int] = tokenizer.get_vocab()\n\n    candidate_eos_tokens = [\"&lt;/s&gt;\", \"&lt;|end_of_text|&gt;\", \"[SEP]\"]\n    for candidate_eos_token in candidate_eos_tokens:\n        if candidate_eos_token in vocab:\n            eos_token = candidate_eos_token\n            eos_token_id = vocab[eos_token]\n            break\n    else:\n        raise InvalidModel(\n            \"The model does not have an end-of-sequence token. Please ensure that this \"\n            \"has been set in the tokenizer's configuration.\"\n        )\n\n    return eos_token, eos_token_id\n\ndocs\ndef get_end_of_chat_token_ids(tokenizer: \"PreTrainedTokenizer\") -&gt; list[int] | None:\n    \"\"\"Get the end token ID for chat models.\n\n    This is only relevant for tokenizers with a chat template.\n\n    Args:\n        tokenizer:\n            The tokenizer.\n\n    Returns:\n        The token IDs used to end chats, or None if the tokenizer does not have a chat\n        template.\n\n    Raises:\n        ValueError:\n            If the end-of-chat token could not be located.\n    \"\"\"\n    if tokenizer.chat_template is None:\n        return None\n\n    user_message: dict[t.Literal[\"role\", \"content\"], str] = dict()\n    user_message[\"role\"] = \"user\"\n    user_message[\"content\"] = \"X\"\n    token_ids = tokenizer.apply_chat_template(conversation=[user_message])\n    assert isinstance(token_ids, list)\n\n    for idx, token in enumerate(tokenizer.convert_ids_to_tokens(token_ids)):\n        token_id = tokenizer.convert_tokens_to_ids(token)\n        assert isinstance(token_id, int)\n        token = tokenizer.decode([token_id])\n        if \"X\" in token:\n            x_token_index = idx\n            break\n    else:\n        raise ValueError(\"Could not locate the end-of-chat token for the model.\")\n\n    end_of_chat_tokens = token_ids[x_token_index + 1 :]\n    if len(end_of_chat_tokens) == 0:\n        return None\n    return end_of_chat_tokens\n\n\ndef scramble(text: str) -&gt; str:docs\n    \"\"\"Scramble a string in a bijective manner.\n\n    Args:\n        text:\n            The string to scramble.\n\n    Returns:\n        The scrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(text))\n    scrambled = \"\".join(text[i] for i in permutation)\n    return scrambled\n\n\ndef unscramble(scrambled_text: str) -&gt; str:docs\n    \"\"\"Unscramble a string in a bijective manner.\n\n    Args:\n        scrambled_text:\n            The scrambled string to unscramble.\n\n    Returns:\n        The unscrambled string.\n    \"\"\"\n    rng = np.random.default_rng(seed=4242)\n    permutation = rng.permutation(x=len(scrambled_text))\n    inverse_permutation = np.argsort(permutation)\n    unscrambled = \"\".join(scrambled_text[i] for i in inverse_permutation)\n    return unscrambled\n\n\n@cache\ndef log_once(message: str, level: int = logging.INFO) -&gt; None:docs\n    \"\"\"Log a message once.\n\n    This is ensured by caching the input/output pairs of this function, using the\n    `functools.cache` decorator.\n\n    Args:\n        message:\n            The message to log.\n        level:\n            The logging level. Defaults to logging.INFO.\n    \"\"\"\n    match level:\n        case logging.DEBUG:\n            logger.debug(message)\n        case logging.INFO:\n            logger.info(message)\n        case logging.WARNING:\n            logger.warning(message)\n        case logging.ERROR:\n            logger.error(message)\n        case logging.CRITICAL:\n            logger.critical(message)\n        case _:\n            raise ValueError(f\"Invalid logging level: {level}\")\n</code></pre>"}]}