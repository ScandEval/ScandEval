# 游뾇릖 Danish

This is an overview of all the datasets used in the Danish part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### Angry Tweeets

This dataset was published in [this
paper](https://aclanthology.org/2021.nodalida-main.53/) and was a crowd-sourcing effort
to annotate sentiment of Danish tweets. The original full dataset consists of 3,458
samples, and we are using a split of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). All the samples
in the original test set are included in our test set, but our test set is furthermore
using a subset of the original training set as test samples as well. The original
dataset did not have a validation split, so we have created one by sampling from the
training set.

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F칮lgende er tweets og deres sentiment, som kan v칝re 'positiv', 'neutral'
  eller 'negativ'.
  ```
- Base prompt template:
  ```
  Tweet: {text}
  Sentiment: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tweet: {text}

  Klassificer sentimentet i tweetet. Svar kun med 'positiv', 'neutral' eller 'negativ'.
  ```
- Label mapping:
    - `positive` 俱뫮잺 `positiv`
    - `neutral` 俱뫮잺 `neutral`
    - `negative` 俱뫮잺 `negativ`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset angry-tweeets
```


## Named Entity Recognition

### DANSK

This dataset was published in [this
paper](https://doi.org/10.3384/nejlt.2000-1533.2024.5249) and is a manually annotated
subset of [Danish Gigaword](https://aclanthology.org/2021.nodalida-main.46/) with the 18
different named entities, following the OntoNotes 5.0 scheme. It was annotated by 10
different annotators.

The original full dataset consists of 15,062 samples, and we are using a split of 1,024
/ 256 / 1,024 samples for training, validation and testing, respectively (so 2,304
samples used in total). All samples in the validation and test sets of our version also
belong to the original validation and test set, respectively.

We have furthermore converted the OntoNotes 5.0 labelling scheme to the CoNLL-2003
labelling scheme, which is more common in the NER literature. The mapping is as follows:

- `PERSON` 俱뫮잺 `PER`
- `LOCATION` 俱뫮잺 `LOC`
- `FACILITY` 俱뫮잺 `LOC`
- `GPE` 俱뫮잺 `LOC`
- `ORGANIZATION` 俱뫮잺 `PER`
- `EVENT` 俱뫮잺 `MISC`
- `LANGUAGE` 俱뫮잺 `MISC`
- `PRODUCT` 俱뫮잺 `MISC`
- `WORK OF ART` 俱뫮잺 `MISC`
- `NORP` 俱뫮잺 `MISC`
- `CARDINAL` 俱뫮잺 `O`
- `DATE` 俱뫮잺 `O`
- `LAW` 俱뫮잺 `O`
- `MONEY` 俱뫮잺 `O`
- `ORDINAL` 俱뫮잺 `O`
- `PERCENT` 俱뫮잺 `O`
- `QUANTITY` 俱뫮잺 `O`
- `TIME` 俱뫮잺 `O`

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  F칮lgende er s칝tninger og JSON-ordb칮ger med de navngivne enheder, som
  forekommer i den givne s칝tning.
  ```
- Base prompt template:
  ```
  S칝tning: {text}
  Navngivne enheder: {label}
  ```
- Instruction-tuned prompt template:
  ```
  S칝tning: {text}

  Identific칠r de navngivne enheder i s칝tningen. Du skal outputte dette som en JSON-ordbog med n칮glerne 'person', 'sted', 'organisation' og 'diverse'. V칝rdierne skal v칝re lister over de navngivne enheder af den type, pr칝cis som de forekommer i s칝tningen.
  ```
- Label mapping:
    - `B-PER` 俱뫮잺 `person`
    - `I-PER` 俱뫮잺 `person`
    - `B-LOC` 俱뫮잺 `sted`
    - `I-LOC` 俱뫮잺 `sted`
    - `B-ORG` 俱뫮잺 `organisation`
    - `I-ORG` 俱뫮잺 `organisation`
    - `B-MISC` 俱뫮잺 `diverse`
    - `I-MISC` 俱뫮잺 `diverse`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset dansk
```


### Unofficial: DaNE

Coming soon!


## Linguistic Acceptability

### ScaLA-da

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Danish Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Danish-DDT/tree/master) by
assuming that the documents in the treebank are correct, and corrupting the samples to
create grammatically incorrect samples. The corruptions were done by either removing a
word from a sentence, or by swapping two neighbouring words in a sentence. To ensure
that this does indeed break the grammaticality of the sentence, a set of rules were used
on the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F칮lgende er s칝tninger og om de er grammatisk korrekte.
  ```
- Base prompt template:
  ```
  S칝tning: {text}
  Grammatisk korrekt: {label}
  ```
- Instruction-tuned prompt template:
  ```
  S칝tning: {text}

  Bestem om s칝tningen er grammatisk korrekt eller ej. Svar med 'ja', hvis s칝tningen er korrekt, og 'nej', hvis den ikke er.
  ```
- Label mapping:
    - `correct` 俱뫮잺 `ja`
    - `incorrect` 俱뫮잺 `nej`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scala-da
```


## Reading Comprehension

### ScandiQA-da

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the Danish part of the [MKQA
dataset](https://aclanthology.org/2021.tacl-1.82/). The MKQA dataset is based on the
English [Natural Questions dataset](https://aclanthology.org/Q19-1026/), based on search
queries from the Google search engine. The questions and answers were manually
translated to Danish (and other languages) as part of MKQA, and the contexts were in
ScandiQA-da machine translated using the [DeepL translation
API](https://www.deepl.com/en/pro-api/). A rule-based approach was used to ensure that
the translated contexts still contained the answer to the question, potentially by
changing the answers slightly.

The original full dataset consists of 6,810 / 500 / 500 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively (so 3,328 samples used in total). All validation
samples in our version also belong to the original validation set, and all original test
samples are included in our test set. The remaining 1,548 test samples in our version
was sampled from the original training set.

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:
  ```
  F칮lgende er tekster med tilh칮rende sp칮rgsm친l og svar.
  ```
- Base prompt template:
  ```
  Tekst: {text}
  Sp칮rgsm친l: {question}
  Svar med maks. 3 ord: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tekst: {text}

  Besvar f칮lgende sp칮rgsm친l om teksten ovenfor med maks. 3 ord.

  Sp칮rgsm친l: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scandiqa-da
```


## Knowledge

### Danske Talem친der

This dataset was created by The Danish Language and Literature Society, published
[here](https://sprogteknologi.dk/dataset/1000-talemader-evalueringsdatasaet). The
dataset features Danish idioms along with their official meaning. For each idiom, three
negative samples were created: (a) a random idiom, (b) a concrete made-up idiom, and (c)
an abstract made-up idiom. The dataset was created to evaluate the ability of language
models to understand Danish idioms.

The original full dataset consists of 1,000 samples. We use a 128 / 64 / 808 split for
training, validation and testing, respectively (so 1,000 samples used in total).

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  F칮lgende er multiple choice sp칮rgsm친l (med svar).
  ```
- Base prompt template:
  ```
  Hvad er betydningen af f칮lgende talem친de: {text}
  Svarmuligheder:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Svar: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Hvad er betydningen af f칮lgende talem친de: {text}
  Svarmuligheder:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}

  Besvar ovenst친ende sp칮rgsm친l ved at svare med 'a', 'b', 'c' eller 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset danske-talemaader
```


### Danish Citizen Tests

This dataset was created by the Alexandra Institute by scraping the Danish citizenship
tests (indf칮dsretspr칮ven) and permanent residency tests (medborgerskabspr칮ven) from 2016
to 2023. These are available on the [official website of the Danish Ministry of
International Recruitment and Integration](https://danskogproever.dk/).

The original full dataset consists of 720 samples. We use an 80 / 128 / 512 split for
training, validation and testing, respectively (so 720 samples used in total).

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  F칮lgende er multiple choice sp칮rgsm친l (med svar).
  ```
- Base prompt template:
  ```
  Sp칮rgsm친l: {text}
  Svarmuligheder:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  Svar: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Sp칮rgsm친l: {text}
  Svarmuligheder:
  a. {option_a}
  b. {option_b}
  c. {option_c}

  Besvar ovenst친ende sp칮rgsm친l ved at svare med 'a', 'b' eller 'c'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset danish-citizen-tests
```


### Unofficial: MMLU-da

Coming soon!


### Unofficial: ARC-da

Coming soon!


## Common-sense Reasoning

### HellaSwag-da

Coming soon!


## Summarization

### Nordjylland News

Coming soon!
