# üá≥üá¥ Norwegian

This is an overview of all the datasets used in the Norwegian part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Named Entity Recognition

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Linguistic Acceptability

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Reading Comprehension

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Knowledge

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Common-sense Reasoning

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```


## Summarization

### [dataset]

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: XX
- Prefix prompt:
  ```
  [prefix-prompt]
  ```
- Base prompt template:
  ```
  [base-prompt]
  ```
- Instruction-tuned prompt template:
  ```
  [instruction-tuned-prompt]
  ```
- Label mapping:
    - `X` ‚û°Ô∏è `Y`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset X
```
