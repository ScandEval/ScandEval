# 游젏릖 Swedish

This is an overview of all the datasets used in the Swedish part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### SweReC

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F칬ljande 칛r recensioner och deras sentiment, som kan vara 'positiv', 'neutral' eller 'negativ'.
  ```
- Base prompt template:
  ```
  Recension: {text}
  Sentiment: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Recension: {text}

  Klassificera sentimentet i recensionen. Svara med 'positiv', 'neutral' eller 'negativ'.
  ```
- Label mapping:
    - `positive` 俱뫮잺 `positiv`
    - `neutral` 俱뫮잺 `neutral`
    - `negative` 俱뫮잺 `negativ`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset swerec
```


## Named Entity Recognition

### SUC 3.0

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  F칬ljande 칛r meningar och JSON-ordb칬cker med de namngivna enheter som f칬rekommer i den givna meningen.
  ```
- Base prompt template:
  ```
  Mening: {text}
  Namngivna entiteter: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Mening: {text}

  Identifiera de namngivna enheterna i meningen. Du ska outputta detta som en JSON-ordbok med nycklarna 'person', 'plats', 'organisation' och 'diverse'. V칛rdena ska vara listor 칬ver de namngivna enheter av den typen, precis som de f칬rekommer i meningen.
  ```
- Label mapping:
    - `B-PER` 俱뫮잺 `person`
    - `I-PER` 俱뫮잺 `person`
    - `B-LOC` 俱뫮잺 `plats`
    - `I-LOC` 俱뫮잺 `plats`
    - `B-ORG` 俱뫮잺 `organisation`
    - `I-ORG` 俱뫮잺 `organisation`
    - `B-MISC` 俱뫮잺 `diverse`
    - `I-MISC` 俱뫮잺 `diverse`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset suc3
```


## Linguistic Acceptability

### ScaLA-sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  F칬ljande 칛r meningar och huruvida de 칛r grammatiskt korrekta.
  ```
- Base prompt template:
  ```
  Mening: {text}
  Grammatisk korrekt: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Mening: {text}

  Best칛m om meningen 칛r grammatiskt korrekt eller inte. Svara med 'ja' om meningen 칛r korrekt och 'nej' om den inte 칛r.
  ```
- Label mapping:
    - `correct` 俱뫮잺 `ja`
    - `incorrect` 俱뫮잺 `nej`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scala-sv
```


## Reading Comprehension

### ScandiQA-sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:
  ```
  Nedan f칬ljer texter med tillh칬rande fr친gor och svar.
  ```
- Base prompt template:
  ```
  Text: {text}
  Fr친ga: {question}
  Svar p친 max 3 ord: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Text: {text}

  Besvara f칬ljande fr친ga om texten ovan med h칬gst 3 ord.

  Fr친ga: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scandiqa-sv
```


## Knowledge

### MMLU-sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  F칬ljande 칛r flervalsfr친gor (med svar).
  ```
- Base prompt template:
  ```
  Fr친ga: {text}
  Svar: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Fr친ga: {text}

  Besvara f칬ljande fr친ga med 'a', 'b', 'c' eller 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset mmlu-sv
```


### ARC-sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  F칬ljande 칛r flervalsfr친gor (med svar).
  ```
- Base prompt template:
  ```
  Fr친ga: {text}
  Svar: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Fr친ga: {text}

  Besvara f칬ljande fr친ga med 'a', 'b', 'c' eller 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset arc-sv
```


## Common-sense Reasoning

### HellaSwag-sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  F칬ljande 칛r flervalsfr친gor (med svar).
  ```
- Base prompt template:
  ```
  Fr친ga: {text}
  Svar: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Fr친ga: {text}

  Besvara f칬ljande fr친ga med 'a', 'b', 'c' eller 'd'.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset hellaswag-sv
```


## Summarization

### SweDN

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 1
- Prefix prompt:
  ```
  Nedan f칬ljer artiklar med tillh칬rande sammanfattningar.
  ```
- Base prompt template:
  ```
  Artikel: {text}
  Sammanfattning: {target_text}
  ```
- Instruction-tuned prompt template:
  ```
  Artikel: {text}

  Skriv en sammanfattning av artikeln ovan.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset swedn
```


### Unofficial: Schibsted-Sv

[description]

[size-info]

Here are a few examples from the training split:

```
[example-1]
```
```
[example-2]
```
```
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 1
- Prefix prompt:
  ```
  Nedan f칬ljer artiklar med tillh칬rande sammanfattningar.
  ```
- Base prompt template:
  ```
  Artikel: {text}
  Sammanfattning: {target_text}
  ```
- Instruction-tuned prompt template:
  ```
  Artikel: {text}

  Skriv en sammanfattning av artikeln ovan.
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset schibsted-sv
```
