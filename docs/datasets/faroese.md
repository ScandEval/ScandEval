# 游游 Faroese

This is an overview of all the datasets used in the Faroese part of ScandEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.


## Sentiment Classification

### FoSent

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:
  ```
  Her eru nakrir tekstir flokka칧ir eftir lyndi, sum kann vera 'positivt', 'neutralt' ella 'negativt'.
  ```
- Base prompt template:
  ```
  Text: {text}
  Lyndi: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tekstur: {text}

  Flokka lyndi칧 칤 tekstinum. Svara vi칧 'positivt', 'neutralt' ella 'negativt'.
  ```
- Label mapping:
    - `positive` 俱뫮잺 `positivt`
    - `neutral` 俱뫮잺 `neutralt`
    - `negative` 俱뫮잺 `negativt`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset fosent
```


## Named Entity Recognition

### FoNE

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  Her eru nakrir setningar og nakrar JSON or칧ab칮kur vi칧 nevndar eindir, sum eru 칤 setningunum.
  ```
- Base prompt template:
  ```
  Setningur: {text}
  Nevndar eindir: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Setningur: {text}

  Greini칧 nevndu einingarnar 칤 setningunni. 뤢 칝ttir a칧 skila 쬰ssu sem JSON or칧ab칩k me칧 lyklunum 'pers칩nur', 'sta칧ur', 'felagsskapur' og 'ymiskt'. Gildin 칝ttu a칧 vera listi yfir nevndu einingarnar af 쬰irri ger칧, n치kv칝mlega eins og 쮂r koma fram 칤 setningunni.
  ```
- Label mapping:
    - `B-PER` 俱뫮잺 `pers칩nur`
    - `I-PER` 俱뫮잺 `pers칩nur`
    - `B-LOC` 俱뫮잺 `sta칧ur`
    - `I-LOC` 俱뫮잺 `sta칧ur`
    - `B-ORG` 俱뫮잺 `felagsskapur`
    - `I-ORG` 俱뫮잺 `felagsskapur`
    - `B-MISC` 俱뫮잺 `ymiskt`
    - `I-MISC` 俱뫮잺 `ymiskt`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset fone
```


### Unofficial: WikiANN-fo

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:
  ```
  Her eru nakrir setningar og nakrar JSON or칧ab칮kur vi칧 nevndar eindir, sum eru 칤 setningunum.
  ```
- Base prompt template:
  ```
  Setningur: {text}
  Nevndar eindir: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Setningur: {text}

  Greini칧 nevndu einingarnar 칤 setningunni. 뤢 칝ttir a칧 skila 쬰ssu sem JSON or칧ab칩k me칧 lyklunum 'pers칩nur', 'sta칧ur', 'felagsskapur' og 'ymiskt'. Gildin 칝ttu a칧 vera listi yfir nevndu einingarnar af 쬰irri ger칧, n치kv칝mlega eins og 쮂r koma fram 칤 setningunni.
  ```
- Label mapping:
    - `B-PER` 俱뫮잺 `pers칩nur`
    - `I-PER` 俱뫮잺 `pers칩nur`
    - `B-LOC` 俱뫮잺 `sta칧ur`
    - `I-LOC` 俱뫮잺 `sta칧ur`
    - `B-ORG` 俱뫮잺 `felagsskapur`
    - `I-ORG` 俱뫮잺 `felagsskapur`
    - `B-MISC` 俱뫮잺 `ymiskt`
    - `I-MISC` 俱뫮잺 `ymiskt`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset wikiann-fo
```


## Linguistic Acceptability

### ScaLA-fo

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:
  ```
  Hetta eru nakrir setningar og um teir eru m치ll칝ruliga r칝ttir.
  ```
- Base prompt template:
  ```
  Setningur: {text}
  M치ll칝ruliga r칝ttur: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Setningur: {text}

  Greini칧 hvort setningurin er m치ll칝ruliga r칝ttur ella ikki. Svari칧 skal vera 'ja' um setningurin er r칝ttur og 'nei' um hann ikki er.
  ```
- Label mapping:
    - `correct` 俱뫮잺 `ja`
    - `incorrect` 俱뫮잺 `nei`

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset scala-fo
```


## Reading Comprehension

### FoQA

[description]

[size-info]

Here are a few examples from the training split:

```json
[example-1]
```
```json
[example-2]
```
```json
[example-3]
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:
  ```
  Hetta eru tekstir saman vi칧 spurningum og svar.
  ```
- Base prompt template:
  ```
  Tekstur: {text}
  Spurningur: {question}
  Svara vi칧 칤 mesta lagi trimum or칧um: {label}
  ```
- Instruction-tuned prompt template:
  ```
  Tekstur: {text}

  Svara hesum spurninginum um tekstin uppiyvir vi칧 칤 mesta lagi trimum or칧um.

  Spurningur: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
$ scandeval --model <model-id> --dataset foqa
```
